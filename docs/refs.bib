
@article{polson_bayesian_2013,
	title = {Bayesian {Inference} for {Logistic} {Models} {Using} {Pólya}–{Gamma} {Latent} {Variables}},
	volume = {108},
	issn = {0162-1459},
	url = {http://dx.doi.org/10.1080/01621459.2013.829001},
	doi = {10.1080/01621459.2013.829001},
	abstract = {We propose a new data-augmentation strategy for fully Bayesian inference in models with binomial likelihoods. The approach appeals to a new class of Pólya–Gamma distributions, which are constructed in detail. A variety of examples are presented to show the versatility of the method, including logistic regression, negative binomial regression, nonlinear mixed-effect models, and spatial models for count data. In each case, our data-augmentation strategy leads to simple, effective methods for posterior inference that (1) circumvent the need for analytic approximations, numerical integration, or Metropolis–Hastings; and (2) outperform other known data-augmentation strategies, both in ease of use and in computational efficiency. All methods, including an efficient sampler for the Pólya–Gamma distribution, are implemented in the R package BayesLogit. Supplementary materials for this article are available online.},
	number = {504},
	urldate = {2016-09-15},
	journal = {Journal of the American Statistical Association},
	author = {Polson, Nicholas G. and Scott, James G. and Windle, Jesse},
	month = dec,
	year = {2013},
	pages = {1339--1349},
	file = {BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:/Users/magnusmunch/Zotero/storage/VDK8S2H4/BayesianInferenceLogRegModelsPolyaGammaLatentVars.pdf:application/pdf;Full Text PDF:/Users/magnusmunch/Zotero/storage/PDKP9JFH/Polson et al. - 2013 - Bayesian Inference for Logistic Models Using Pólya.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/HWBHFG72/01621459.2013.html:text/html;techsuppR1.pdf:/Users/magnusmunch/Zotero/storage/SVUQ2BMZ/techsuppR1.pdf:application/pdf}
}

@article{bishop_bayesian_1999,
	title = {Bayesian pca},
	url = {http://books.google.com/books?hl=en&lr=&id=bMuzXPzlkG0C&oi=fnd&pg=PA382&dq=%22a+data+set+D+of+observed+d-dimensional+vectors+D+%3D+%7Bt+n+%7D+where+n%22+%22retained+variance+is+a+maximum,+or+equivalently+the+linear+projection+for+which%22+&ots=MwsdzBAGMb&sig=8y5ikc7XyeaXMyg7uCg1VHTmndI},
	urldate = {2016-09-13},
	journal = {Advances in neural information processing systems},
	author = {Bishop, Christopher M.},
	year = {1999},
	pages = {382--388},
	file = {BayesianPCA.pdf:/Users/magnusmunch/Zotero/storage/BNECKZVZ/BayesianPCA.pdf:application/pdf}
}

@article{van_de_wiel_better_2016,
	title = {Better prediction by use of co-data: adaptive group-regularized ridge regression},
	volume = {35},
	issn = {02776715},
	shorttitle = {Better prediction by use of co-data},
	url = {http://doi.wiley.com/10.1002/sim.6732},
	doi = {10.1002/sim.6732},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Statistics in Medicine},
	author = {van de Wiel, Mark A. and Lien, Tonje G. and Verlaat, Wina and van Wieringen, Wessel N. and Wilting, Saskia M.},
	month = feb,
	year = {2016},
	pages = {368--381},
	file = {article.pdf:/Users/magnusmunch/Zotero/storage/N7W75WGI/van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf;sim6732-sup-0001-Supplementary1.pdf:/Users/magnusmunch/Zotero/storage/CNPFHP5A/sim6732-sup-0001-Supplementary1.pdf:application/pdf;van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:/Users/magnusmunch/Zotero/storage/WQFH375F/van de Wiel et al. - 2016 - Better prediction by use of co-data adaptive grou.pdf:application/pdf}
}

@article{tipping_probabilistic_1999,
	title = {Probabilistic {Principal} {Component} {Analysis}},
	volume = {61},
	issn = {1467-9868},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/1467-9868.00196/abstract},
	doi = {10.1111/1467-9868.00196},
	abstract = {Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA.},
	language = {en},
	number = {3},
	urldate = {2016-09-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tipping, Michael E. and Bishop, Christopher M.},
	month = jan,
	year = {1999},
	keywords = {Density estimation, EM algorithm, Gaussian mixtures, Maximum likelihood, Principal component analysis, Probability model},
	pages = {611--622},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/DXK5FWUJ/Tipping and Bishop - 1999 - Probabilistic Principal Component Analysis.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8E9QIIZT/abstract.html:text/html}
}

@article{friedman_regularization_2010,
	title = {Regularization paths for generalized linear models via coordinate descent},
	volume = {33},
	number = {1},
	journal = {Journal of Statistical Software},
	author = {Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert},
	month = jan,
	year = {2010},
	pages = {1--22},
	file = {v33i01.pdf:/Users/magnusmunch/Zotero/storage/IEIJNSEI/v33i01.pdf:application/pdf}
}

@article{casella_empirical_2001,
	title = {Empirical {Bayes} {Gibbs} sampling},
	volume = {2},
	number = {4},
	journal = {Biostatistics},
	author = {Casella, George},
	year = {2001},
	pages = {485--500},
	file = {Biostat-2001-Casella-485-500.pdf:/Users/magnusmunch/Zotero/storage/38IRWHCP/Biostat-2001-Casella-485-500.pdf:application/pdf}
}

@article{kyung_penalized_2010,
	title = {Penalized regression, standard errors, and {Bayesian} lassos},
	volume = {5},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1340218343},
	doi = {10.1214/10-BA607},
	abstract = {Penalized regression methods for simultaneous variable selection and coefficient estimation, especially those based on the lasso of Tibshirani (1996), have received a great deal of attention in recent years, mostly through frequentist models. Properties such as consistency have been studied, and are achieved by different lasso variations. Here we look at a fully Bayesian formulation of the problem, which is flexible enough to encompass most versions of the lasso that have been previously considered. The advantages of the hierarchical Bayesian formulations are many. In addition to the usual ease-of-interpretation of hierarchical models, the Bayesian formulation produces valid standard errors (which can be problematic for the frequentist lasso), and is based on a geometrically ergodic Markov chain. We compare the performance of the Bayesian lassos to their frequentist counterparts using simulations, data sets that previous lasso papers have used, and a difficult modeling problem for predicting the collapse of governments around the world. In terms of prediction mean squared error, the Bayesian lasso performance is similar to and, in some cases, better than, the frequentist lasso.},
	language = {EN},
	number = {2},
	urldate = {2017-10-18},
	journal = {Bayesian Analysis},
	author = {Kyung, Minjung and Gill, Jeff and Ghosh, Malay and Casella, George},
	month = jun,
	year = {2010},
	mrnumber = {MR2719657},
	zmnumber = {1330.62289},
	keywords = {Variable selection, Gibbs sampling, Hierarchical Models, Geometric Ergodicity},
	pages = {369--411},
	file = {euclid.ba.1340218343.pdf:/Users/magnusmunch/Zotero/storage/UZ7R4H27/euclid.ba.1340218343.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8TGZTMSB/1340218343.html:text/html}
}

@article{cowen_network_2017,
	title = {Network propagation: a universal amplifier of genetic associations},
	volume = {18},
	copyright = {2017 Nature Publishing Group},
	issn = {1471-0064},
	shorttitle = {Network propagation},
	url = {https://www.nature.com/articles/nrg.2017.38},
	doi = {10.1038/nrg.2017.38},
	abstract = {{\textless}p{\textgreater}Network propagation is based on the principle that genes underlying similar phenotypes are more likely to interact with each other. It is proving to be a powerful approach for extracting biological information from molecular networks that is relevant to human disease.{\textless}/p{\textgreater}},
	language = {en},
	number = {9},
	urldate = {2017-11-21},
	journal = {Nature Reviews Genetics},
	author = {Cowen, Lenore and Ideker, Trey and Raphael, Benjamin J. and Sharan, Roded},
	month = jun,
	year = {2017},
	pages = {nrg.2017.38},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/GT4QBV8Z/Cowen et al. - 2017 - Network propagation a universal amplifier of gene.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/8R5XU3DR/nrg.2017.html:text/html}
}

@article{van_wieringen_ridge_2016,
	title = {Ridge estimation of inverse covariance matrices from high-dimensional data},
	volume = {103},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947316301141},
	doi = {10.1016/j.csda.2016.05.012},
	abstract = {The ridge estimation of the precision matrix is investigated in the setting where the number of variables is large relative to the sample size. First, two archetypal ridge estimators are reviewed and it is noted that their penalties do not coincide with common quadratic ridge penalties. Subsequently, starting from a proper ℓ2-penalty, analytic expressions are derived for two alternative ridge estimators of the precision matrix. The alternative estimators are compared to the archetypes with regard to eigenvalue shrinkage and risk. The alternatives are also compared to the graphical lasso within the context of graphical modeling. The comparisons may give reason to prefer the proposed alternative estimators.},
	number = {Supplement C},
	urldate = {2017-11-21},
	journal = {Computational Statistics \& Data Analysis},
	author = {van Wieringen, Wessel N. and Peeters, Carel F. W.},
	month = nov,
	year = {2016},
	keywords = {Graphical modeling, High-dimensional precision matrix estimation, Multivariate normal, -penalization, Precision matrix},
	pages = {284--303},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/8SFEHKF6/van Wieringen and Peeters - 2016 - Ridge estimation of inverse covariance matrices fr.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/683EC9SW/S0167947316301141.html:text/html}
}

@article{van_wieringen_mean_2017,
	title = {On the mean squared error of the ridge estimator of the covariance and precision matrix},
	volume = {123},
	issn = {0167-7152},
	url = {http://www.sciencedirect.com/science/article/pii/S0167715216302863},
	doi = {10.1016/j.spl.2016.12.002},
	abstract = {For a suitably chosen ridge penalty parameter, the ridge regression estimator uniformly dominates the maximum likelihood regression estimator in terms of the mean squared error. Analogous results for the ridge maximum likelihood estimators of covariance and precision matrix are presented.},
	number = {Supplement C},
	urldate = {2017-11-21},
	journal = {Statistics \& Probability Letters},
	author = {van Wieringen, Wessel N.},
	month = apr,
	year = {2017},
	keywords = {Multivariate normal, -penalization, Inverse covariance matrix},
	pages = {88--92},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/AWVH8CGE/van Wieringen - 2017 - On the mean squared error of the ridge estimator o.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/DC9TJ73X/S0167715216302863.html:text/html}
}

@article{peeters_spectral_2016,
	title = {The {Spectral} {Condition} {Number} {Plot} for {Regularization} {Parameter} {Determination}},
	url = {http://arxiv.org/abs/1608.04123},
	abstract = {Many modern statistical applications ask for the estimation of a covariance (or precision) matrix in settings where the number of variables is larger than the number of observations. There exists a broad class of ridge-type estimators that employs regularization to cope with the subsequent singularity of the sample covariance matrix. These estimators depend on a penalty parameter and choosing its value can be hard, in terms of being computationally unfeasible or tenable only for a restricted set of ridge-type estimators. Here we introduce a simple graphical tool, the spectral condition number plot, for informed heuristic penalty parameter selection. The proposed tool is computationally friendly and can be employed for the full class of ridge-type covariance (precision) estimators.},
	urldate = {2017-11-21},
	journal = {arXiv:1608.04123 [stat]},
	author = {Peeters, Carel F. W. and van de Wiel, Mark A. and van Wieringen, Wessel N.},
	month = aug,
	year = {2016},
	note = {arXiv: 1608.04123},
	keywords = {Statistics - Computation, Statistics - Machine Learning},
	file = {arXiv\:1608.04123 PDF:/Users/magnusmunch/Zotero/storage/GSP4R94B/Peeters et al. - 2016 - The Spectral Condition Number Plot for Regularizat.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/6JHIS8RD/1608.html:text/html}
}

@article{belilovsky_testing_2015,
	title = {Testing for {Differences} in {Gaussian} {Graphical} {Models}: {Applications} to {Brain} {Connectivity}},
	shorttitle = {Testing for {Differences} in {Gaussian} {Graphical} {Models}},
	url = {http://arxiv.org/abs/1512.08643},
	abstract = {Functional brain networks are well described and estimated from data with Gaussian Graphical Models (GGMs), e.g. using sparse inverse covariance estimators. Comparing functional connectivity of subjects in two populations calls for comparing these estimated GGMs. Our goal is to identify differences in GGMs known to have similar structure. We characterize the uncertainty of differences with confidence intervals obtained using a parametric distribution on parameters of a sparse estimator. Sparse penalties enable statistical guarantees and interpretable models even in high-dimensional and low-sample settings. Characterizing the distributions of sparse models is inherently challenging as the penalties produce a biased estimator. Recent work invokes the sparsity assumptions to effectively remove the bias from a sparse estimator such as the lasso. These distributions can be used to give confidence intervals on edges in GGMs, and by extension their differences. However, in the case of comparing GGMs, these estimators do not make use of any assumed joint structure among the GGMs. Inspired by priors from brain functional connectivity we derive the distribution of parameter differences under a joint penalty when parameters are known to be sparse in the difference. This leads us to introduce the debiased multi-task fused lasso, whose distribution can be characterized in an efficient manner. We then show how the debiased lasso and multi-task fused lasso can be used to obtain confidence intervals on edge differences in GGMs. We validate the techniques proposed on a set of synthetic examples as well as neuro-imaging dataset created for the study of autism.},
	urldate = {2017-11-21},
	journal = {arXiv:1512.08643 [stat]},
	author = {Belilovsky, Eugene and Varoquaux, Gaël and Blaschko, Matthew B.},
	month = dec,
	year = {2015},
	note = {arXiv: 1512.08643},
	keywords = {Statistics - Machine Learning},
	file = {arXiv\:1512.08643 PDF:/Users/magnusmunch/Zotero/storage/W5AMXAEC/Belilovsky et al. - 2015 - Testing for Differences in Gaussian Graphical Mode.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/6CSA7F5Z/1512.html:text/html}
}

@article{van_penalized_2014,
	title = {Penalized differential pathway analysis of integrative oncogenomics studies},
	volume = {13},
	issn = {1544-6115},
	url = {https://www.degruyter.com/view/j/sagmb.2014.13.issue-2/sagmb-2013-0020/sagmb-2013-0020.xml},
	doi = {10.1515/sagmb-2013-0020},
	abstract = {Through integration of genomic data from multiple sources, we may obtain a more accurate and complete picture of the molecular mechanisms underlying tumorigenesis. We discuss the integration of DNA copy number and mRNA gene expression data from an observational integrative genomics study involving cancer patients. The two molecular levels involved are linked through the central dogma of molecular biology. DNA copy number aberrations abound in the cancer cell. Here we investigate how these aberrations affect gene expression levels within a pathway using observational integrative genomics data of cancer patients. In particular, we aim to identify differential edges between regulatory networks of two groups involving these molecular levels. Motivated by the rate equations, the regulatory mechanism between DNA copy number aberrations and gene expression levels within a pathway is modeled by a simultaneous-equations model, for the one- and two-group case. The latter facilitates the identification of differential interactions between the two groups. Model parameters are estimated by penalized least squares using the lasso (L1) penalty to obtain a sparse pathway topology. Simulations show that the inclusion of DNA copy number data benefits the discovery of gene-gene interactions. In addition, the simulations reveal that cis-effects tend to be over-estimated in a univariate (single gene) analysis. In the application to real data from integrative oncogenomic studies we show that inclusion of prior information on the regulatory network architecture benefits the reproducibility of all edges. Furthermore, analyses of the TP53 and TGFb signaling pathways between ER+ and ER- samples from an integrative genomics breast cancer study identify reproducible differential regulatory patterns that corroborate with existing literature.},
	number = {2},
	urldate = {2017-11-21},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {van, Wieringen Wessel N. and van, de Wiel Mark A.},
	year = {2014},
	keywords = {cancer, data integration, penalized estimation, simultaneous-equations model (SEM)},
	pages = {141--158},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/8X4T9P49/van and van - 2014 - Penalized differential pathway analysis of integra.pdf:application/pdf}
}

@article{ma_estimation_2015,
	title = {Estimation and {Inference} for {High}-{Dimensional} {Gaussian} {Graphical} {Models} with {Structural} {Constraints}.},
	author = {Ma, Jing},
	year = {2015},
	file = {mjing_1.pdf:/Users/magnusmunch/Zotero/storage/64JFADKA/mjing_1.pdf:application/pdf}
}

@article{latouche_goodness_2017,
	title = {Goodness of fit of logistic regression models for random graphs},
	issn = {1061-8600},
	url = {http://amstat.tandfonline.com/doi/abs/10.1080/10618600.2017.1349663},
	doi = {10.1080/10618600.2017.1349663},
	abstract = {The logistic regression model constitutes a natural and simple tool to understand how covariates (when available) contribute to explain the topology of a binary network. After estimating the logistic parameters, one of the main questions which arises in practice is to assess the goodness of fit of the corresponding model. To address this problem, we add a general term, related to the graphon function of W-graph models, to the logistic function. Such an extra term aims at characterizing the residual structure of the network, that is not explained by the covariates. We approximate this new generic logistic model using a class of models with blockwise constant residual structure. This framework allows to derive a Bayesian procedure from a model based selection context using goodness-of-fit criteria. All these criteria depend on marginal likelihood terms for which we do provide estimates relying on two series of variational approximations. Experiments on toy data are carried out to assess the inference procedure. Finally, six networks from social sciences and ecology are studied to illustrate the proposed methodology. Logistic regression is a natural and simple tool to understand how covariates contribute to explain the topology of a binary network. Once the model is fitted, the practitioner is interested in the goodness-of-fit of the regression in order to check if the covariates are sufficient to explain the whole topology of the network and, if they are not, to analyze the residual structure. To address this problem, we introduce a generic model that combines logistic regression with a network-oriented residual term. This residual term takes the form of the graphon function of a W-graph. Using a variational Bayes framework, we infer the residual graphon by averaging over a series of blockwise constant functions. This approach allows us to define a generic goodness-of-fit criterion, which corresponds to the posterior probability for the residual graphon to be constant. Experiments on toy data are carried out to assess the accuracy of the procedure. Several networks from social sciences and ecology are studied to illustrate the proposed methodology.},
	urldate = {2017-11-21},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Latouche, Pierre and Robin, Stéphane and Ouadah, Sarah},
	month = jul,
	year = {2017},
	pages = {0--0},
	file = {Goodness of fit of logistic regression models for random graphs.pdf:/Users/magnusmunch/Zotero/storage/PH3CEXI6/Goodness of fit of logistic regression models for random graphs.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/RR3FDGTI/Latouche et al. - 2017 - Goodness of fit of logistic regression models for .html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/98XVX9IS/Latouche et al. - 2017 - Goodness of fit of logistic regression models for .html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/EV3PAWRK/Latouche et al. - 2017 - Goodness of fit of logistic regression models for .html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/JTU4Z6M2/10618600.2017.html:text/html}
}

@article{bilgrau_targeted_2015,
	title = {Targeted {Fused} {Ridge} {Estimation} of {Inverse} {Covariance} {Matrices} from {Multiple} {High}-{Dimensional} {Data} {Classes}},
	url = {http://arxiv.org/abs/1509.07982},
	abstract = {We consider the problem of jointly estimating multiple precision matrices from (aggregated) high-dimensional data consisting of distinct classes. An \${\textbackslash}ell\_2\$-penalized maximum-likelihood approach is employed. The suggested approach is flexible and generic, incorporating several other \${\textbackslash}ell\_2\$-penalized estimators as special cases. In addition, the approach allows for the specification of target matrices through which prior knowledge may be incorporated and which can stabilize the estimation procedure in high-dimensional settings. The result is a targeted fused ridge estimator that is of use when the precision matrices of the constituent classes are believed to chiefly share the same structure while potentially differing in a number of locations of interest. It has many applications in (multi)factorial study designs. We focus on the graphical interpretation of precision matrices with the proposed estimator then serving as a basis for integrative or meta-analytic Gaussian graphical modeling. Situations are considered in which the classes are defined by data sets and/or (subtypes of) diseases. The performance of the proposed estimator in the graphical modeling setting is assessed through extensive simulation experiments. Its practical usability is illustrated by the differential network modeling of 11 large-scale diffuse large B-cell lymphoma gene expression data sets. The estimator and its related procedures are incorporated into the R-package rags2ridges.},
	urldate = {2017-11-22},
	journal = {arXiv:1509.07982 [q-bio, stat]},
	author = {Bilgrau, Anders Ellern and Peeters, Carel F. W. and Eriksen, Poul Svante and Bøgsted, Martin and van Wieringen, Wessel N.},
	month = sep,
	year = {2015},
	note = {arXiv: 1509.07982},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Quantitative Biology - Molecular Networks},
	file = {arXiv\:1509.07982 PDF:/Users/magnusmunch/Zotero/storage/P792TQ9P/Bilgrau et al. - 2015 - Targeted Fused Ridge Estimation of Inverse Covaria.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/URINWK3M/1509.html:text/html}
}

@article{ollerer_robust_2015,
	title = {Robust high-dimensional precision matrix estimation},
	url = {http://arxiv.org/abs/1501.01219},
	abstract = {The dependency structure of multivariate data can be analyzed using the covariance matrix \${\textbackslash}Sigma\$. In many fields the precision matrix \${\textbackslash}Sigma{\textasciicircum}\{-1\}\$ is even more informative. As the sample covariance estimator is singular in high-dimensions, it cannot be used to obtain a precision matrix estimator. A popular high-dimensional estimator is the graphical lasso, but it lacks robustness. We consider the high-dimensional independent contamination model. Here, even a small percentage of contaminated cells in the data matrix may lead to a high percentage of contaminated rows. Downweighting entire observations, which is done by traditional robust procedures, would then results in a loss of information. In this paper, we formally prove that replacing the sample covariance matrix in the graphical lasso with an elementwise robust covariance matrix leads to an elementwise robust, sparse precision matrix estimator computable in high-dimensions. Examples of such elementwise robust covariance estimators are given. The final precision matrix estimator is positive definite, has a high breakdown point under elementwise contamination and can be computed fast.},
	urldate = {2017-11-29},
	journal = {arXiv:1501.01219 [stat]},
	author = {Öllerer, Viktoria and Croux, Christophe},
	month = jan,
	year = {2015},
	note = {arXiv: 1501.01219},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1501.01219 PDF:/Users/magnusmunch/Zotero/storage/UPPB6BKZ/Öllerer and Croux - 2015 - Robust high-dimensional precision matrix estimatio.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/PTD9N8M2/1501.html:text/html}
}

@article{christley_incorporating_2009,
	title = {Incorporating {Existing} {Network} {Information} into {Gene} {Network} {Inference}},
	volume = {4},
	issn = {1932-6203},
	url = {http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0006799},
	doi = {10.1371/journal.pone.0006799},
	abstract = {One methodology that has met success to infer gene networks from gene expression data is based upon ordinary differential equations (ODE). However new types of data continue to be produced, so it is worthwhile to investigate how to integrate these new data types into the inference procedure. One such data is physical interactions between transcription factors and the genes they regulate as measured by ChIP-chip or ChIP-seq experiments. These interactions can be incorporated into the gene network inference procedure as a priori network information. In this article, we extend the ODE methodology into a general optimization framework that incorporates existing network information in combination with regularization parameters that encourage network sparsity. We provide theoretical results proving convergence of the estimator for our method and show the corresponding probabilistic interpretation also converges. We demonstrate our method on simulated network data and show that existing network information improves performance, overcomes the lack of observations, and performs well even when some of the existing network information is incorrect. We further apply our method to the core regulatory network of embryonic stem cells utilizing predicted interactions from two studies as existing network information. We show that including the prior network information constructs a more closely representative regulatory network versus when no information is provided.},
	language = {en},
	number = {8},
	urldate = {2018-03-14},
	journal = {PLOS ONE},
	author = {Christley, Scott and Nie, Qing and Xie, Xiaohui},
	month = aug,
	year = {2009},
	keywords = {Algorithms, Genetic networks, Gene regulatory networks, Gene expression, Network analysis, Transcription factors, Gene regulation, Optimization},
	pages = {e6799},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/6RUGVGEC/Christley et al. - 2009 - Incorporating Existing Network Information into Ge.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/KMDJ5NJ2/article.pdf:application/pdf}
}

@article{blei_variational_2017,
	title = {Variational {Inference}: {A} {Review} for {Statisticians}},
	volume = {112},
	issn = {0162-1459},
	shorttitle = {Variational {Inference}},
	url = {https://doi.org/10.1080/01621459.2017.1285773},
	doi = {10.1080/01621459.2017.1285773},
	abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this article, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find a member of that family which is close to the target density. Closeness is measured by Kullback–Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this article is to catalyze statistical research on this class of algorithms. Supplementary materials for this article are available online.},
	number = {518},
	urldate = {2018-03-23},
	journal = {Journal of the American Statistical Association},
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	month = apr,
	year = {2017},
	keywords = {Computer Science - Learning, Statistics - Computation, Statistics - Machine Learning, Algorithms, Computationally intensive methods, Statistical computing},
	pages = {859--877},
	file = {arXiv\:1601.00670 PDF:/Users/magnusmunch/Zotero/storage/JPZ2BADG/Blei et al. - 2016 - Variational Inference A Review for Statisticians.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/Z9PF9AIP/1601.html:text/html;Full Text PDF:/Users/magnusmunch/Zotero/storage/7XISFUAE/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/ZR4DPV3P/01621459.2017.html:text/html}
}

@article{binder_incorporating_2009,
	title = {Incorporating pathway information into boosting estimation of high-dimensional risk prediction models},
	volume = {10},
	issn = {1471-2105},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2647532/},
	doi = {10.1186/1471-2105-10-18},
	abstract = {Background
There are several techniques for fitting risk prediction models to high-dimensional data, arising from microarrays. However, the biological knowledge about relations between genes is only rarely taken into account. One recent approach incorporates pathway information, available, e.g., from the KEGG database, by augmenting the penalty term in Lasso estimation for continuous response models.

Results
As an alternative, we extend componentwise likelihood-based boosting techniques for incorporating pathway information into a larger number of model classes, such as generalized linear models and the Cox proportional hazards model for time-to-event data. In contrast to Lasso-like approaches, no further assumptions for explicitly specifying the penalty structure are needed, as pathway information is incorporated by adapting the penalties for single microarray features in the course of the boosting steps. This is shown to result in improved prediction performance when the coefficients of connected genes have opposite sign. The properties of the fitted models resulting from this approach are then investigated in two application examples with microarray survival data.

Conclusion
The proposed approach results not only in improved prediction performance but also in structurally different model fits. Incorporating pathway information in the suggested way is therefore seen to be beneficial in several ways.},
	urldate = {2018-04-09},
	journal = {BMC Bioinformatics},
	author = {Binder, Harald and Schumacher, Martin},
	month = jan,
	year = {2009},
	pmid = {19144132},
	pmcid = {PMC2647532},
	pages = {18},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/86PFZP78/Binder and Schumacher - 2009 - Incorporating pathway information into boosting es.pdf:application/pdf}
}

@article{li_network-constrained_2008,
	title = {Network-constrained regularization and variable selection for analysis of genomic data},
	volume = {24},
	issn = {1367-4803},
	url = {https://academic.oup.com/bioinformatics/article/24/9/1175/206444},
	doi = {10.1093/bioinformatics/btn081},
	abstract = {Motivation: Graphs or networks are common ways of depicting information. In biology in particular, many different biological processes are represented by graphs, such as regulatory networks or metabolic pathways. This kind of a priori information gathered over many years of biomedical research is a useful supplement to the standard numerical genomic data such as microarray gene-expression data. How to incorporate information encoded by the known biological networks or graphs into analysis of numerical data raises interesting statistical challenges. In this article, we introduce a network-constrained regularization procedure for linear regression analysis in order to incorporate the information from these graphs into an analysis of the numerical data, where the network is represented as a graph and its corresponding Laplacian matrix. We define a network-constrained penalty function that penalizes the L1-norm of the coefficients but encourages smoothness of the coefficients on the network.Results: Simulation studies indicated that the method is quite effective in identifying genes and subnetworks that are related to disease and has higher sensitivity than the commonly used procedures that do not use the pathway structure information. Application to one glioblastoma microarray gene-expression dataset identified several subnetworks on several of the Kyoto Encyclopedia of Genes and Genomes (KEGG) transcriptional pathways that are related to survival from glioblastoma, many of which were supported by published literatures.Conclusions: The proposed network-constrained regularization procedure efficiently utilizes the known pathway structures in identifying the relevant genes and the subnetworks that might be related to phenotype in a general regression framework. As more biological networks are identified and documented in databases, the proposed method should find more applications in identifying the subnetworks that are related to diseases and other biological processes.Contact:hongzhe@mail.med.upenn.edu},
	language = {en},
	number = {9},
	urldate = {2018-04-09},
	journal = {Bioinformatics},
	author = {Li, Caiyan and Li, Hongzhe},
	month = may,
	year = {2008},
	pages = {1175--1182},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/Z9796N48/Li and Li - 2008 - Network-constrained regularization and variable se.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/NG2MJ8P8/206444.html:text/html}
}

@article{wei_nonparametric_2007,
	title = {Nonparametric pathway-based regression models for analysis of genomic data},
	volume = {8},
	issn = {1465-4644},
	url = {https://academic.oup.com/biostatistics/article/8/2/265/230606},
	doi = {10.1093/biostatistics/kxl007},
	abstract = {High-throughout genomic data provide an opportunity for identifying pathways and genes that are related to various clinical phenotypes. Besides these genomic data, another valuable source of data is the biological knowledge about genes and pathways that might be related to the phenotypes of many complex diseases. Databases of such knowledge are often called the metadata. In microarray data analysis, such metadata are currently explored in post hoc ways by gene set enrichment analysis but have hardly been utilized in the modeling step. We propose to develop and evaluate a pathway-based gradient descent boosting procedure for nonparametric pathways-based regression (NPR) analysis to efficiently integrate genomic data and metadata. Such NPR models consider multiple pathways simultaneously and allow complex interactions among genes within the pathways and can be applied to identify pathways and genes that are related to variations of the phenotypes. These methods also provide an alternative to mediating the problem of a large number of potential interactions by limiting analysis to biologically plausible interactions between genes in related pathways. Our simulation studies indicate that the proposed boosting procedure can indeed identify relevant pathways. Application to a gene expression data set on breast cancer distant metastasis identified that Wnt, apoptosis, and cell cycle-regulated pathways are more likely related to the risk of distant metastasis among lymph-node-negative breast cancer patients. Results from analysis of other two breast cancer gene expression data sets indicate that the pathways of Metalloendopeptidases (MMPs) and MMP inhibitors, as well as cell proliferation, cell growth, and maintenance are important to breast cancer relapse and survival. We also observed that by incorporating the pathway information, we achieved better prediction for cancer recurrence.},
	language = {en},
	number = {2},
	urldate = {2018-04-16},
	journal = {Biostatistics},
	author = {Wei, Zhi and Li, Hongzhe},
	month = apr,
	year = {2007},
	pages = {265--284},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/B264N3X3/Wei and Li - 2007 - Nonparametric pathway-based regression models for .pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/82RXSEMG/230606.html:text/html}
}

@article{donnet_empirical_2010,
	title = {An empirical {Bayes} procedure for the selection of {Gaussian} graphical models},
	url = {http://arxiv.org/abs/1003.5851},
	abstract = {A new methodology for model determination in decomposable graphical Gaussian models is developed. The Bayesian paradigm is used and, for each given graph, a hyper inverse Wishart prior distribution on the covariance matrix is considered. This prior distribution depends on hyper-parameters. It is well-known that the models's posterior distribution is sensitive to the specification of these hyper-parameters and no completely satisfactory method is registered. In order to avoid this problem, we suggest adopting an empirical Bayes strategy, that is a strategy for which the values of the hyper-parameters are determined using the data. Typically, the hyper-parameters are fixed to their maximum likelihood estimations. In order to calculate these maximum likelihood estimations, we suggest a Markov chain Monte Carlo version of the Stochastic Approximation EM algorithm. Moreover, we introduce a new sampling scheme in the space of graphs that improves the add and delete proposal of Armstrong et al. (2009). We illustrate the efficiency of this new scheme on simulated and real datasets.},
	urldate = {2018-04-17},
	journal = {arXiv:1003.5851 [stat]},
	author = {Donnet, Sophie and Marin, Jean-Michel},
	month = mar,
	year = {2010},
	note = {arXiv: 1003.5851},
	keywords = {Statistics - Computation},
	file = {arXiv\:1003.5851 PDF:/Users/magnusmunch/Zotero/storage/7V4BVDQZ/Donnet and Marin - 2010 - An empirical Bayes procedure for the selection of .pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/2KH325H8/1003.html:text/html}
}

@article{marlin_group_2012,
	title = {Group {Sparse} {Priors} for {Covariance} {Estimation}},
	url = {http://arxiv.org/abs/1205.2626},
	abstract = {Recently it has become popular to learn sparse Gaussian graphical models (GGMs) by imposing l1 or group l1,2 penalties on the elements of the precision matrix. Thispenalized likelihood approach results in a tractable convex optimization problem. In this paper, we reinterpret these results as performing MAP estimation under a novel prior which we call the group l1 and l1,2 positivedefinite matrix distributions. This enables us to build a hierarchical model in which the l1 regularization terms vary depending on which group the entries are assigned to, which in turn allows us to learn block structured sparse GGMs with unknown group assignments. Exact inference in this hierarchical model is intractable, due to the need to compute the normalization constant of these matrix distributions. However, we derive upper bounds on the partition functions, which lets us use fast variational inference (optimizing a lower bound on the joint posterior). We show that on two real world data sets (motion capture and financial data), our method which infers the block structure outperforms a method that uses a fixed block structure, which in turn outperforms baseline methods that ignore block structure.},
	urldate = {2018-04-17},
	journal = {arXiv:1205.2626 [cs, stat]},
	author = {Marlin, Benjamin and Schmidt, Mark and Murphy, Kevin},
	month = may,
	year = {2012},
	note = {arXiv: 1205.2626},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1205.2626 PDF:/Users/magnusmunch/Zotero/storage/GGTFADIB/Marlin et al. - 2012 - Group Sparse Priors for Covariance Estimation.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/QEFA2VEK/1205.html:text/html}
}

@book{marlin_sparse_nodate,
	title = {Sparse {Gaussian} {Graphical} {Models} with {Unknown} {Block} {Structure}},
	abstract = {Recent work has shown that one can learn the structure of Gaussian Graphical Models by imposing an L1 penalty on the precision matrix, and then using efficient convex optimization methods to find the penalized maximum likelihood estimate. This is similar to performing MAP estimation with a prior that prefers sparse graphs. In this paper, we use the stochastic block model as a prior. This prefer graphs that are blockwise sparse, but unlike previous work, it does not require that the blocks or groups be specified a priori. The resulting problem is no longer convex, but we devise an efficient variational Bayes algorithm to solve it. We show that our method has better test set likelihood on two different datasets (motion capture and gene expression) compared to independent L1, and can match the performance of group L1 using manually created groups. 1.},
	author = {Marlin, Benjamin M. and Murphy, Kevin P.},
	file = {Citeseer - Full Text PDF:/Users/magnusmunch/Zotero/storage/EQATAPKD/Marlin and Murphy - Sparse Gaussian Graphical Models with Unknown Bloc.pdf:application/pdf;Citeseer - Snapshot:/Users/magnusmunch/Zotero/storage/86R6XGCP/summary.html:text/html}
}

@article{wang_bayesian_2012,
	title = {Bayesian {Graphical} {Lasso} {Models} and {Efficient} {Posterior} {Computation}},
	volume = {7},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1354024465},
	doi = {10.1214/12-BA729},
	abstract = {Recently, the graphical lasso procedure has become popular in estimating Gaussian graphical models. In this paper, we introduce a fully Bayesian treatment of graphical lasso models. We first investigate the graphical lasso prior that has been relatively unexplored. Using data augmentation, we develop a simple but highly efficient block Gibbs sampler for simulating covariance matrices. We then generalize the Bayesian graphical lasso to the Bayesian adaptive graphical lasso. Finally, we illustrate and compare the results from our approach to those obtained using the standard graphical lasso procedures for real and simulated data. In terms of both covariance matrix estimation and graphical structure learning, the Bayesian adaptive graphical lasso appears to be the top overall performer among a range of frequentist and Bayesian methods.},
	language = {EN},
	number = {4},
	urldate = {2018-04-17},
	journal = {Bayesian Analysis},
	author = {Wang, Hao},
	month = dec,
	year = {2012},
	mrnumber = {MR3000017},
	zmnumber = {1330.62041},
	keywords = {Adaptive graphical lasso, Block Gibbs sampler, Constrained parameter spaces, Covariance matrix estimation, Double-exponential distribution, Graphical lasso},
	pages = {867--886},
	file = {euclid.ba.1354024465.pdf:/Users/magnusmunch/Zotero/storage/6U8G8UVN/euclid.ba.1354024465.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/WTTR7ZZ3/1354024465.html:text/html}
}

@article{sun_bayesian_2010,
	title = {A {Bayesian} {Approach} for {Graph}-constrained {Estimation} for {High}-dimensional {Regression}},
	volume = {1},
	issn = {0976-6774},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4373540/},
	abstract = {Many different biological processes are represented by network graphs such as regulatory networks, metabolic pathways, and protein-protein interaction networks. Since genes that are linked on the networks usually have biologically similar functions, the linked genes form molecular modules to affect the clinical phenotypes/outcomes. Similarly, in large-scale genetic association studies, many SNPs are in high linkage disequilibrium (LD), which can also be summarized as a LD graph. In order to incorporate the graph information into regression analysis with high dimensional genomic data as predictors, we introduce a Bayesian approach for graph-constrained estimation (Bayesian GRACE) and regularization, which controls the amount of regularization for sparsity and smoothness of the regression coefficients. The Bayesian estimation with their posterior distributions can provide credible intervals for the estimates of the regression coefficients along with standard errors. The deviance information criterion (DIC) is applied for model assessment and tuning parameter selection. The performance of the proposed Bayesian approach is evaluated through simulation studies and is compared with Bayesian Lasso and Bayesian Elastic-net procedures. We demonstrate our method in an analysis of data from a case-control genome-wide association study of neuroblastoma using a weighted LD graph.},
	number = {2},
	urldate = {2018-04-19},
	journal = {International journal of systems and synthetic biology},
	author = {Sun, Hokeun and Li, Hongzhe},
	year = {2010},
	pmid = {25821387},
	pmcid = {PMC4373540},
	pages = {255--272},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/HFW494KV/Sun and Li - 2010 - A Bayesian Approach for Graph-constrained Estimati.pdf:application/pdf}
}

@article{li_variable_2010,
	title = {Variable selection and regression analysis for graph-structured covariates with an application to genomics},
	volume = {4},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1287409383},
	doi = {10.1214/10-AOAS332},
	abstract = {Graphs and networks are common ways of depicting biological information. In biology, many different biological processes are represented by graphs, such as regulatory networks, metabolic pathways and protein–protein interaction networks. This kind of a priori use of graphs is a useful supplement to the standard numerical data such as microarray gene expression data. In this paper we consider the problem of regression analysis and variable selection when the covariates are linked on a graph. We study a graph-constrained regularization procedure and its theoretical properties for regression analysis to take into account the neighborhood information of the variables measured on a graph. This procedure involves a smoothness penalty on the coefficients that is defined as a quadratic form of the Laplacian matrix associated with the graph. We establish estimation and model selection consistency results and provide estimation bounds for both fixed and diverging numbers of parameters in regression models. We demonstrate by simulations and a real data set that the proposed procedure can lead to better variable selection and prediction than existing methods that ignore the graph information associated with the covariates.},
	language = {EN},
	number = {3},
	urldate = {2018-04-19},
	journal = {The Annals of Applied Statistics},
	author = {Li, Caiyan and Li, Hongzhe},
	month = sep,
	year = {2010},
	mrnumber = {MR2758338},
	zmnumber = {1202.62157},
	keywords = {Regularization, High-dimensional data, sign consistency, network, Laplacian matrix},
	pages = {1498--1516},
	file = {euclid.aoas.1287409383.pdf:/Users/magnusmunch/Zotero/storage/BZ76QZX8/euclid.aoas.1287409383.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/CQIW7Z3B/1287409383.html:text/html}
}

@article{zhou_bayesian_2013,
	title = {Bayesian hierarchical graph-structured model for pathway analysis using gene expression data},
	volume = {12},
	issn = {1544-6115},
	url = {https://www.degruyter.com/view/j/sagmb.2013.12.issue-3/sagmb-2013-0011/sagmb-2013-0011.xml},
	doi = {10.1515/sagmb-2013-0011},
	abstract = {In genomic analysis, there is growing interest in network structures that represent biochemistry interactions. Graph structured or constrained inference takes advantage of a known relational structure among variables to introduce smoothness and reduce complexity in modeling, especially for high-dimensional genomic data. There has been a lot of interest in its application in model regularization and selection. However, prior knowledge on the graphical structure among the variables can be limited and partial. Empirical data may suggest variations and modifications to such a graph, which could lead to new and interesting biological findings. In this paper, we propose a Bayesian random graph-constrained model, rGrace, an extension from the Grace model, to combine a priori network information with empirical evidence, for applications such as pathway analysis. Using both simulations and real data examples, we show that the new method, while leading to improved predictive performance, can identify discrepancy between data and a prior known graph structure and suggest modifications and updates.},
	number = {3},
	urldate = {2018-04-25},
	journal = {Statistical Applications in Genetics and Molecular Biology},
	author = {Zhou, Hui and Zheng, Tian},
	year = {2013},
	keywords = {Gene expression, Network analysis, Bayesian anslysis},
	pages = {393--412},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/X28D4PPN/Zhou and Zheng - 2013 - Bayesian hierarchical graph-structured model for p.pdf:application/pdf}
}

@article{peterson_joint_2016,
	title = {Joint {Bayesian} variable and graph selection for regression models with network-structured predictors},
	volume = {35},
	copyright = {Copyright © 2015 John Wiley \& Sons, Ltd.},
	issn = {1097-0258},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.6792},
	doi = {10.1002/sim.6792},
	abstract = {In this work, we develop a Bayesian approach to perform selection of predictors that are linked within a network. We achieve this by combining a sparse regression model relating the predictors to a response variable with a graphical model describing conditional dependencies among the predictors. The proposed method is well-suited for genomic applications because it allows the identification of pathways of functionally related genes or proteins that impact an outcome of interest. In contrast to previous approaches for network-guided variable selection, we infer the network among predictors using a Gaussian graphical model and do not assume that network information is available a priori. We demonstrate that our method outperforms existing methods in identifying network-structured predictors in simulation settings and illustrate our proposed model with an application to inference of proteins relevant to glioblastoma survival. Copyright © 2015 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {7},
	urldate = {2018-04-30},
	journal = {Statistics in Medicine},
	author = {Peterson, Christine B. and Stingo, Francesco C. and Vannucci, Marina},
	month = mar,
	year = {2016},
	keywords = {Bayesian variable selection, Gaussian graphical model, protein network, linear model},
	pages = {1017--1031},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/7I7ADV6M/Peterson et al. - 2016 - Joint Bayesian variable and graph selection for re.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/XBQTMZXW/sim.html:text/html}
}

@article{pan_incorporating_2010,
	title = {Incorporating {Predictor} {Network} in {Penalized} {Regression} with {Application} to {Microarray} {Data}},
	volume = {66},
	issn = {0006-341X},
	url = {http://www.jstor.org/stable/40663240},
	abstract = {We consider penalized linear regression, especially for "large p, small n" problems, for which the relationships among predictors are described a priori by a network. A class of motivating examples includes modeling a phenotype through gene expression profiles while accounting for coordinated functioning of genes in the form of biological pathways or networks. To incorporate the prior knowledge of the similar effect sizes of neighboring predictors in a network, we propose a grouped penalty based on the L $_{\textrm{γ}}$ -norm that smoothes the regression coefficients of the predictors over the network. The main feature of the proposed method is its ability to automatically realize grouped variable selection and exploit grouping effects. We also discuss effects of the choices of the γ and some weights inside the L $_{\textrm{γ}}$ -norm. Simulation studies demonstrate the superior finite-sample performance of the proposed method as compared to Lasso, elastic net, and a recently proposed network-based method. The new method performs best in variable selection across all simulation set-ups considered. For illustration, the method is applied to a microarray dataset to predict survival times for some glioblastoma patients using a gene expression dataset and a gene network compiled from some Kyoto Encyclopedia of Genes and Genomes (KEGG) pathways.},
	number = {2},
	urldate = {2018-04-30},
	journal = {Biometrics},
	author = {Pan, Wei and Xie, Benhuai and Shen, Xiaotong},
	year = {2010},
	pages = {474--484},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/AAV9D5ME/Pan et al. - 2010 - Incorporating Predictor Network in Penalized Regre.pdf:application/pdf}
}

@article{huang_sparse_2011,
	title = {{THE} {SPARSE} {LAPLACIAN} {SHRINKAGE} {ESTIMATOR} {FOR} {HIGH}-{DIMENSIONAL} {REGRESSION}},
	volume = {39},
	issn = {0090-5364},
	url = {http://www.jstor.org/stable/23033591},
	abstract = {We propose a new penalized method for variable selection and estimation that explicitly incorporates the correlation patterns among predictors. This method is based on a combination of the minimax concave penalty and Laplacian quadratic associated with a graph as the penalty function. We call in the sparse Laplacian shrinkage (SLS) method. The SLS uses the minimax concave penalty for encouraging sparsity and Laplacian quadratic penalty for promoting smoothness among coefficients associated with the correlated predictors. The SLS has a generalized grouping property with respect to the graph represented by the Laplacian quadratic. We show that the SLS possesses an oracle property in the sense that it is selection consistent and equal to the oracle Laplacian shrinkage estimator with high probability. This result holds in sparse, high-dimensional settings with p » n under reasonable conditions. We derive a coordinate descent algorithm for computing the SLS estimates. Simulation studies are conducted to evaluate the performance of the SLS method and a real data example is used to illustrate its application.},
	number = {4},
	urldate = {2018-04-30},
	journal = {The Annals of Statistics},
	author = {Huang, Jian and Ma, Shuangge and Li, Hongzhe and Zhang, Cun-Hui},
	year = {2011},
	pages = {2021--2046},
	file = {JSTOR Full Text PDF:/Users/magnusmunch/Zotero/storage/R6R75N3Z/Huang et al. - 2011 - THE SPARSE LAPLACIAN SHRINKAGE ESTIMATOR FOR HIGH-.pdf:application/pdf}
}

@article{kim_network-based_2013,
	title = {Network-{Based} {Penalized} {Regression} {With} {Application} to {Genomic} {Data}},
	volume = {69},
	copyright = {© 2013, The International Biometric Society},
	issn = {1541-0420},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/biom.12035},
	doi = {10.1111/biom.12035},
	abstract = {Penalized regression approaches are attractive in dealing with high-dimensional data such as arising in high-throughput genomic studies. New methods have been introduced to utilize the network structure of predictors, for example, gene networks, to improve parameter estimation and variable selection. All the existing network-based penalized methods are based on an assumption that parameters, for example, regression coefficients, of neighboring nodes in a network are close in magnitude, which however may not hold. Here we propose a novel penalized regression method based on a weaker prior assumption that the parameters of neighboring nodes in a network are likely to be zero (or non-zero) at the same time, regardless of their specific magnitudes. We propose a novel non-convex penalty function to incorporate this prior, and an algorithm based on difference convex programming. We use simulated data and two breast cancer gene expression datasets to demonstrate the advantages of the proposed methods over some existing methods. Our proposed methods can be applied to more general problems for group variable selection.},
	language = {en},
	number = {3},
	urldate = {2018-04-30},
	journal = {Biometrics},
	author = {Kim, Sunkyung and Pan, Wei and Shen, Xiaotong},
	month = sep,
	year = {2013},
	keywords = {Gene expression, Networks analysis, Nonconvex minimization, Penalty, Truncated Lasso penalty},
	pages = {582--593},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/78DE9B9C/Kim et al. - 2013 - Network-Based Penalized Regression With Applicatio.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/2RNGHU39/biom.html:text/html}
}

@article{li_bayesian_2010,
	title = {Bayesian {Variable} {Selection} in {Structured} {High}-{Dimensional} {Covariate} {Spaces} {With} {Applications} in {Genomics}},
	volume = {105},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/jasa.2010.tm08177},
	doi = {10.1198/jasa.2010.tm08177},
	abstract = {We consider the problem of variable selection in regression modeling in high-dimensional spaces where there is known structure among the covariates. This is an unconventional variable selection problem for two reasons: (1) The dimension of the covariate space is comparable, and often much larger, than the number of subjects in the study, and (2) the covariate space is highly structured, and in some cases it is desirable to incorporate this structural information in to the model building process. We approach this problem through the Bayesian variable selection framework, where we assume that the covariates lie on an undirected graph and formulate an Ising prior on the model space for incorporating structural information. Certain computational and statistical problems arise that are unique to such high-dimensional, structured settings, the most interesting being the phenomenon of phase transitions. We propose theoretical and computational schemes to mitigate these problems. We illustrate our methods on two different graph structures: the linear chain and the regular graph of degree k. Finally, we use our methods to study a specific application in genomics: the modeling of transcription factor binding sites in DNA sequences.},
	number = {491},
	urldate = {2018-04-30},
	journal = {Journal of the American Statistical Association},
	author = {Li, Fan and Zhang, Nancy R.},
	month = sep,
	year = {2010},
	keywords = {Markov chain Monte Carlo, Ising model, Motif analysis, Phase transition, Undirected graph},
	pages = {1202--1214},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/DNRXD2FF/Li and Zhang - 2010 - Bayesian Variable Selection in Structured High-Dim.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/6KUF9DPN/jasa.2010.html:text/html}
}

@article{stingo_incorporating_2011,
	title = {Incorporating biological information into linear models: {A} {Bayesian} approach to the selection of pathways and genes},
	volume = {5},
	issn = {1932-6157, 1941-7330},
	shorttitle = {Incorporating biological information into linear models},
	url = {https://projecteuclid.org/euclid.aoas/1318514292},
	doi = {10.1214/11-AOAS463},
	abstract = {The vast amount of biological knowledge accumulated over the years has allowed researchers to identify various biochemical interactions and define different families of pathways. There is an increased interest in identifying pathways and pathway elements involved in particular biological processes. Drug discovery efforts, for example, are focused on identifying biomarkers as well as pathways related to a disease. We propose a Bayesian model that addresses this question by incorporating information on pathways and gene networks in the analysis of DNA microarray data. Such information is used to define pathway summaries, specify prior distributions, and structure the MCMC moves to fit the model. We illustrate the method with an application to gene expression data with censored survival outcomes. In addition to identifying markers that would have been missed otherwise and improving prediction accuracy, the integration of existing biological knowledge into the analysis provides a better understanding of underlying molecular processes.},
	language = {EN},
	number = {3},
	urldate = {2018-04-30},
	journal = {The Annals of Applied Statistics},
	author = {Stingo, Francesco C. and Chen, Yian A. and Tadesse, Mahlet G. and Vannucci, Marina},
	month = sep,
	year = {2011},
	mrnumber = {MR2884929},
	zmnumber = {1228.62150},
	keywords = {Markov chain Monte Carlo, Gene expression, Bayesian variable selection, Markov random field prior, pathway selection},
	pages = {1978--2002},
	file = {euclid.aoas.1318514292.pdf:/Users/magnusmunch/Zotero/storage/6NSE4HQZ/euclid.aoas.1318514292.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/A5DP9TVM/1318514292.html:text/html}
}

@article{hill_integrating_2012,
	title = {Integrating biological knowledge into variable selection: an empirical {Bayes} approach with an application in cancer biology},
	volume = {13},
	issn = {1471-2105},
	shorttitle = {Integrating biological knowledge into variable selection},
	url = {https://doi.org/10.1186/1471-2105-13-94},
	doi = {10.1186/1471-2105-13-94},
	abstract = {An important question in the analysis of biochemical data is that of identifying subsets of molecular variables that may jointly influence a biological response. Statistical variable selection methods have been widely used for this purpose. In many settings, it may be important to incorporate ancillary biological information concerning the variables of interest. Pathway and network maps are one example of a source of such information. However, although ancillary information is increasingly available, it is not always clear how it should be used nor how it should be weighted in relation to primary data.},
	urldate = {2018-04-30},
	journal = {BMC Bioinformatics},
	author = {Hill, Steven M. and Neve, Richard M. and Bayani, Nora and Kuo, Wen-Lin and Ziyad, Safiyyah and Spellman, Paul T. and Gray, Joe W. and Mukherjee, Sach},
	month = may,
	year = {2012},
	keywords = {Lasso, Bayesian variable selection, Markov Random Field, Marginal Likelihood, Inclusion Probability},
	pages = {94},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/KFZZXJ8Q/Hill et al. - 2012 - Integrating biological knowledge into variable sel.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/EJ9N4TKV/1471-2105-13-94.html:text/html}
}

@article{liu_bayesian_2014,
	title = {Bayesian {Regularization} via {Graph} {Laplacian}},
	volume = {9},
	issn = {1936-0975, 1931-6690},
	url = {https://projecteuclid.org/euclid.ba/1401148316},
	doi = {10.1214/14-BA860},
	abstract = {Regularization plays a critical role in modern statistical research, especially in high-dimensional variable selection problems. Existing Bayesian methods usually assume independence between variables a priori. In this article, we propose a novel Bayesian approach, which explicitly models the dependence structure through a graph Laplacian matrix. We also generalize the graph Laplacian to allow both positively and negatively correlated variables. A prior distribution for the graph Laplacian is then proposed, which allows conjugacy and thereby greatly simplifies the computation. We show that the proposed Bayesian model leads to proper posterior distribution. Connection is made between our method and some existing regularization methods, such as Elastic Net, Lasso, Octagonal Shrinkage and Clustering Algorithm for Regression (OSCAR) and Ridge regression. An efficient Markov Chain Monte Carlo method based on parameter augmentation is developed for posterior computation. Finally, we demonstrate the method through several simulation studies and an application on a real data set involving key performance indicators of electronics companies.},
	language = {EN},
	number = {2},
	urldate = {2018-04-30},
	journal = {Bayesian Analysis},
	author = {Liu, Fei and Chakraborty, Sounak and Li, Fan and Liu, Yan and Lozano, Aurelie C.},
	month = jun,
	year = {2014},
	mrnumber = {MR3217003},
	zmnumber = {1327.62152},
	keywords = {Lasso, Regularization, Variable selection, Ridge regression, Elastic net, Bayesian analysis, Grouping, OSCAR},
	pages = {449--474},
	file = {euclid.ba.1401148316.pdf:/Users/magnusmunch/Zotero/storage/AJJAWAGA/euclid.ba.1401148316.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/6ZX5D5E6/1401148316.html:text/html}
}

@article{dobra_variable_2009,
	title = {Variable selection and dependency networks for genomewide data},
	volume = {10},
	issn = {1465-4644},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2742495/},
	doi = {10.1093/biostatistics/kxp018},
	abstract = {We describe a new stochastic search algorithm for linear regression models called the bounded mode stochastic search (BMSS). We make use of BMSS to perform variable selection and classification as well as to construct sparse dependency networks. Furthermore, we show how to determine genetic networks from genomewide data that involve any combination of continuous and discrete variables. We illustrate our methodology with several real-world data sets.},
	number = {4},
	urldate = {2018-04-30},
	journal = {Biostatistics (Oxford, England)},
	author = {Dobra, Adrian},
	month = oct,
	year = {2009},
	pmid = {19520789},
	pmcid = {PMC2742495},
	pages = {621--639},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/DGPEK3CQ/Dobra - 2009 - Variable selection and dependency networks for gen.pdf:application/pdf}
}

@article{gelfand_proper_2003,
	title = {Proper multivariate conditional autoregressive models for spatial data analysis},
	volume = {4},
	issn = {1465-4644},
	url = {https://academic.oup.com/biostatistics/article/4/1/11/246085},
	doi = {10.1093/biostatistics/4.1.11},
	abstract = {In the past decade conditional autoregressive modelling specifications have found considerable application for the analysis of spatial data. Nearly all of this work is done in the univariate case and employs an improper specification. Our contribution here is to move to multivariate conditional autoregressive models and to provide rich, flexible classes which yield proper distributions. Our approach is to introduce spatial autoregression parameters. We first clarify what classes can be developed from the family of Mardia (1988) and contrast with recent work of Kim et al. (2000). We then present a novel parametric linear transformation which provides an extension with attractive interpretation. We propose to employ these models as specifications for second‐stage spatial effects in hierarchical models. Two applications are discussed; one for the two‐dimensional case modelling spatial patterns of child growth, the other for a four‐dimensional situation modelling spatial variation in HLA‐B allele frequencies. In each case, full Bayesian inference is carried out using Markov chain Monte Carlo simulation.},
	language = {en},
	number = {1},
	urldate = {2018-05-01},
	journal = {Biostatistics},
	author = {Gelfand, Alan E. and Vounatsou, Penelope},
	month = jan,
	year = {2003},
	pages = {11--15},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/H4UZPKAB/Gelfand and Vounatsou - 2003 - Proper multivariate conditional autoregressive mod.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/A36KJWST/246085.html:text/html}
}

@inproceedings{xin_efficient_2014,
	title = {Efficient {Generalized} {Fused} {Lasso} and its {Application} to the {Diagnosis} of {Alzheimer}'s {Disease}.},
	booktitle = {{AAAI}},
	author = {Xin, Bo and Kawahara, Yoshinobu and Wang, Yizhou and Gao, Wen},
	year = {2014},
	pages = {2163--2169},
	file = {8261-38583-1-PB.pdf:/Users/magnusmunch/Zotero/storage/3ZUDHT45/8261-38583-1-PB.pdf:application/pdf}
}

@article{she_sparse_2010,
	title = {Sparse regression with exact clustering},
	volume = {4},
	issn = {1935-7524},
	url = {https://projecteuclid.org/euclid.ejs/1286889184},
	doi = {10.1214/10-EJS578},
	abstract = {This paper studies a generic sparse regression problem with a customizable sparsity pattern matrix, motivated by, but not limited to, a supervised gene clustering problem in microarray data analysis. The clustered lasso method is proposed with the l1-type penalties imposed on both the coefficients and their pairwise differences. Somewhat surprisingly, it behaves differently than the lasso or the fused lasso – the exact clustering effect expected from the l1 penalization is rarely seen in applications. An asymptotic study is performed to investigate the power and limitations of the l1-penalty in sparse regression. We propose to combine data-augmentation and weights to improve the l1 technique. To address the computational issues in high dimensions, we successfully generalize a popular iterative algorithm both in practice and in theory and propose an ‘annealing’ algorithm applicable to generic sparse regressions (including the fused/clustered lasso). Some effective accelerating techniques are further investigated to boost the convergence. The accelerated annealing (AA) algorithm, involving only matrix multiplications and thresholdings, can handle a large design matrix as well as a large sparsity pattern matrix.},
	language = {EN},
	urldate = {2018-06-25},
	journal = {Electronic Journal of Statistics},
	author = {She, Yiyuan},
	year = {2010},
	mrnumber = {MR2727453},
	zmnumber = {1329.62327},
	keywords = {Lasso, Sparsity, Thresholding, clustering},
	pages = {1055--1096},
	file = {euclid.ejs.1286889184.pdf:/Users/magnusmunch/Zotero/storage/9B37A5DI/euclid.ejs.1286889184.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/XN59836Q/1286889184.html:text/html}
}

@article{kanehisa_kegg:_2000,
	title = {{KEGG}: {Kyoto} {Encyclopedia} of {Genes} and {Genomes}},
	volume = {28},
	issn = {0305-1048},
	shorttitle = {{KEGG}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102409/},
	abstract = {KEGG (Kyoto Encyclopedia of Genes and Genomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. The genomic information is stored in the GENES database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. The higher order functional information is stored in the PATHWAY database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. The PATHWAY database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. A third database in KEGG is LIGAND for the information about chemical compounds, enzyme molecules and enzymatic reactions. KEGG provides Java graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. The KEGG databases are daily updated and made freely available (http://www.genome.ad.jp/kegg/ ).},
	number = {1},
	urldate = {2018-08-08},
	journal = {Nucleic Acids Research},
	author = {Kanehisa, Minoru and Goto, Susumu},
	month = jan,
	year = {2000},
	pmid = {10592173},
	pmcid = {PMC102409},
	pages = {27--30},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/G8KKWH3R/Kanehisa and Goto - 2000 - KEGG Kyoto Encyclopedia of Genes and Genomes.pdf:application/pdf}
}

@article{tibshirani_sparsity_2005,
	title = {Sparsity and smoothness via the fused lasso},
	volume = {67},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2005.00490.x},
	doi = {10.1111/j.1467-9868.2005.00490.x},
	abstract = {Summary. The lasso penalizes a least squares regression by the sum of the absolute values (L1-norm) of the coefficients. The form of this penalty encourages sparse solutions (with many coefficients equal to 0). We propose the ‘fused lasso’, a generalization that is designed for problems with features that can be ordered in some meaningful way. The fused lasso penalizes the L1-norm of both the coefficients and their successive differences. Thus it encourages sparsity of the coefficients and also sparsity of their differences—i.e. local constancy of the coefficient profile. The fused lasso is especially useful when the number of features p is much greater than N, the sample size. The technique is also extended to the ‘hinge’ loss function that underlies the support vector classifier. We illustrate the methods on examples from protein mass spectroscopy and gene expression data.},
	language = {en},
	number = {1},
	urldate = {2018-08-08},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Tibshirani, Robert and Saunders, Michael and Rosset, Saharon and Zhu, Ji and Knight, Keith},
	month = feb,
	year = {2005},
	keywords = {Lasso, Gene expression, Fused lasso, Least squares regression, Protein mass spectroscopy, Sparse solutions, Support vector classifier},
	pages = {91--108},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/JSKDZDH5/Tibshirani et al. - 2005 - Sparsity and smoothness via the fused lasso.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/V9ZA54Q2/j.1467-9868.2005.00490.html:text/html}
}

@article{tibshirani_solution_2011,
	title = {The solution path of the generalized lasso},
	volume = {39},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1304514656},
	doi = {10.1214/11-AOS878},
	abstract = {We present a path algorithm for the generalized lasso problem. This problem penalizes the ℓ1 norm of a matrix D times the coefficient vector, and has a wide range of applications, dictated by the choice of D. Our algorithm is based on solving the dual of the generalized lasso, which greatly facilitates computation of the path. For D = I (the usual lasso), we draw a connection between our approach and the well-known LARS algorithm. For an arbitrary D, we derive an unbiased estimate of the degrees of freedom of the generalized lasso fit. This estimate turns out to be quite intuitive in many applications.},
	language = {EN},
	number = {3},
	urldate = {2018-08-16},
	journal = {The Annals of Statistics},
	author = {Tibshirani, Ryan J. and Taylor, Jonathan},
	month = jun,
	year = {2011},
	mrnumber = {MR2850205},
	zmnumber = {1234.62107},
	keywords = {Lasso, Path algorithm, Lagrange dual, LARS, degrees of freedom},
	pages = {1335--1371},
	file = {euclid.aos.1304514656.pdf:/Users/magnusmunch/Zotero/storage/62UAN8TI/euclid.aos.1304514656.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/BPIIQV9S/1304514656.html:text/html}
}

@article{arnold_efficient_2016,
	title = {Efficient {Implementations} of the {Generalized} {Lasso} {Dual} {Path} {Algorithm}},
	volume = {25},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2015.1008638},
	doi = {10.1080/10618600.2015.1008638},
	abstract = {We consider efficient implementations of the generalized lasso dual path algorithm given by Tibshirani and Taylor in 2011. We first describe a generic approach that covers any penalty matrix D and any (full column rank) matrix X of predictor variables. We then describe fast implementations for the special cases of trend filtering problems, fused lasso problems, and sparse fused lasso problems, both with X = I and a general matrix X. These specialized implementations offer a considerable improvement over the generic implementation, both in terms of numerical stability and efficiency of the solution path computation. These algorithms are all available for use in the genlasso R package, which can be found in the CRAN repository.},
	number = {1},
	urldate = {2018-08-16},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Arnold, Taylor B. and Tibshirani, Ryan J.},
	month = jan,
	year = {2016},
	keywords = {Fused lasso, Laplacian linear systems, QR decomposition, Trend filtering},
	pages = {1--27},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/R28BZHZQ/Arnold and Tibshirani - 2016 - Efficient Implementations of the Generalized Lasso.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/CW9GVTHJ/10618600.2015.html:text/html}
}

@article{hofling_coordinate-wise_2010,
	title = {A coordinate-wise optimization algorithm for the {Fused} {Lasso}},
	url = {http://arxiv.org/abs/1011.6409},
	abstract = {L1 -penalized regression methods such as the Lasso (Tibshirani 1996) that achieve both variable selection and shrinkage have been very popular. An extension of this method is the Fused Lasso (Tibshirani and Wang 2007), which allows for the incorporation of external information into the model. In this article, we develop new and fast algorithms for solving the Fused Lasso which are based on coordinate-wise optimization. This class of algorithms has recently been applied very successfully to solve L1 -penalized problems very quickly (Friedman et al. 2007). As a straightforward coordinate-wise procedure does not converge to the global optimum in general, we adapt it in two ways, using maximum-flow algorithms and a Huber penalty based approximation to the loss function. In a simulation study, we evaluate the speed of these algorithms and compare them to other standard methods. As the Huber-penalty based method is only approximate, we also evaluate its accuracy. Apart from this, we also extend the Fused Lasso to logistic as well as proportional hazards models and allow for a more flexible penalty structure.},
	urldate = {2018-08-20},
	journal = {arXiv:1011.6409 [stat]},
	author = {Höfling, Holger and Binder, Harald and Schumacher, Martin},
	month = nov,
	year = {2010},
	note = {arXiv: 1011.6409},
	keywords = {Statistics - Computation},
	file = {arXiv\:1011.6409 PDF:/Users/magnusmunch/Zotero/storage/ZNDZN9NH/Höfling et al. - 2010 - A coordinate-wise optimization algorithm for the F.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/CPKIQZV6/1011.html:text/html}
}

@unpublished{viallon_adaptive_2013,
	title = {Adaptive {Generalized} {Fused}-{Lasso}: {Asymptotic} {Properties} and {Applications}},
	shorttitle = {Adaptive {Generalized} {Fused}-{Lasso}},
	url = {https://hal.archives-ouvertes.fr/hal-00813281},
	abstract = {The Lasso has been widely studied and used in many applications over the last decade. It has also been extended in various directions in particular to ensure asymptotic oracle properties through adaptive weights (Zou, 2006). Another direction has been to incorporate additional knowledge within the penalty to account for some structure among features. Among such strategies the Fused-Lasso (Tibshirani et al., 2005) has recently been extended to penalize differences of coefficients corresponding to features organized along a network, through the Generalized Fused-Lasso. In this work we investigate the theoretical and empirical properties of the Adaptive Generalized Fused-Lasso in the context of Generalized Linear Models, with emphasis on Logistic Regression. More precisely, we establish its asymptotic oracle properties and propose an extensive simulation study to explore its empirical properties. We especially show that it compares favorably with other strategies. We also propose an adaptation of the Relaxed Lasso (Meinshausen, 2007). Finally we present an original application of the Generalized Fused-Lasso to the Joint Modeling framework where the design itself suggests the graph to be used in the penalty; an illustration is provided on road safety data.},
	urldate = {2018-08-20},
	author = {Viallon, Vivian and Lambert-Lacroix, Sophie and Höfling, Holger and Picard, Franck},
	month = apr,
	year = {2013},
	keywords = {Penalized regression, Fused lasso, joint modeling},
	file = {HAL PDF Full Text:/Users/magnusmunch/Zotero/storage/FX6GQ4BD/Viallon et al. - 2013 - Adaptive Generalized Fused-Lasso Asymptotic Prope.pdf:application/pdf}
}

@article{viallon_robustness_2016,
	title = {On the robustness of the generalized fused lasso to prior specifications},
	volume = {26},
	issn = {0960-3174, 1573-1375},
	url = {http://link.springer.com/10.1007/s11222-014-9497-6},
	doi = {10.1007/s11222-014-9497-6},
	language = {en},
	number = {1-2},
	urldate = {2018-08-20},
	journal = {Statistics and Computing},
	author = {Viallon, Vivian and Lambert-Lacroix, Sophie and Hoefling, Hölger and Picard, Franck},
	month = jan,
	year = {2016},
	pages = {285--301},
	file = {Viallon2016_Article_OnTheRobustnessOfTheGeneralize.pdf:/Users/magnusmunch/Zotero/storage/ZAFS4CVH/Viallon2016_Article_OnTheRobustnessOfTheGeneralize.pdf:application/pdf}
}

@article{sokolov_pathway-based_2016,
	title = {Pathway-{Based} {Genomics} {Prediction} using {Generalized} {Elastic} {Net}},
	volume = {12},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004790},
	doi = {10.1371/journal.pcbi.1004790},
	abstract = {We present a novel regularization scheme called The Generalized Elastic Net (GELnet) that incorporates gene pathway information into feature selection. The proposed formulation is applicable to a wide variety of problems in which the interpretation of predictive features using known molecular interactions is desired. The method naturally steers solutions toward sets of mechanistically interlinked genes. Using experiments on synthetic data, we demonstrate that pathway-guided results maintain, and often improve, the accuracy of predictors even in cases where the full gene network is unknown. We apply the method to predict the drug response of breast cancer cell lines. GELnet is able to reveal genetic determinants of sensitivity and resistance for several compounds. In particular, for an EGFR/HER2 inhibitor, it finds a possible trans-differentiation resistance mechanism missed by the corresponding pathway agnostic approach.},
	language = {en},
	number = {3},
	urldate = {2018-08-21},
	journal = {PLOS Computational Biology},
	author = {Sokolov, Artem and Carlin, Daniel E. and Paull, Evan O. and Baertsch, Robert and Stuart, Joshua M.},
	month = mar,
	year = {2016},
	keywords = {Genetic networks, Gene regulatory networks, Gene expression, Graphs, Breast cancer, Covariance, Drug synthesis, Signaling networks},
	pages = {e1004790},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/JNQ97K5D/Sokolov et al. - 2016 - Pathway-Based Genomics Prediction using Generalize.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/3QR7C5JT/article.html:text/html}
}

@article{leday_gene_2017,
	title = {Gene network reconstruction using global-local shrinkage priors},
	volume = {11},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1491616871},
	doi = {10.1214/16-AOAS990},
	abstract = {Reconstructing a gene network from high-throughput molecular data is an important but challenging task, as the number of parameters to estimate easily is much larger than the sample size. A conventional remedy is to regularize or penalize the model likelihood. In network models, this is often done locally in the neighborhood of each node or gene. However, estimation of the many regularization parameters is often difficult and can result in large statistical uncertainties. In this paper we propose to combine local regularization with global shrinkage of the regularization parameters to borrow strength between genes and improve inference. We employ a simple Bayesian model with nonsparse, conjugate priors to facilitate the use of fast variational approximations to posteriors. We discuss empirical Bayes estimation of hyperparameters of the priors, and propose a novel approach to rank-based posterior thresholding. Using extensive model- and data-based simulations, we demonstrate that the proposed inference strategy outperforms popular (sparse) methods, yields more stable edges, and is more reproducible. The proposed method, termed ShrinkNet, is then applied to Glioblastoma to investigate the interactions between genes associated with patient survival.},
	language = {EN},
	number = {1},
	urldate = {2018-12-06},
	journal = {The Annals of Applied Statistics},
	author = {Leday, Gwenaël G. R. and Gunst, Mathisca C. M. de and Kpogbezan, Gino B. and Vaart, Aad W. van der and Wieringen, Wessel N. van and van de Wiel, Mark A.},
	month = mar,
	year = {2017},
	mrnumber = {MR3634314},
	zmnumber = {1366.62227},
	keywords = {Bayesian inference, Empirical Bayes, Shrinkage, Variational approximation, Undirected gene network},
	pages = {41--68},
	file = {euclid.aoas.1491616871.pdf:/Users/magnusmunch/Zotero/storage/P6Z8KDIC/euclid.aoas.1491616871.pdf:application/pdf;euclid.aoas.1491616871.pdf:/Users/magnusmunch/Zotero/storage/UV4KWCK9/euclid.aoas.1491616871.pdf:application/pdf;NIHMS70644-supplement-Supplmentary_Information.pdf:/Users/magnusmunch/Zotero/storage/IKP52PJ5/NIHMS70644-supplement-Supplmentary_Information.pdf:application/pdf;PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/K24764M3/Leday et al. - 2017 - Gene Network Reconstruction using Global-Local Shr.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/7KRURCR3/1491616871.html:text/html;Snapshot:/Users/magnusmunch/Zotero/storage/ITKH2TM7/1491616871.html:text/html}
}

@article{liu_graph-based_2018,
	title = {Graph-based sparse linear discriminant analysis for high-dimensional classification},
	issn = {0047-259X},
	url = {http://www.sciencedirect.com/science/article/pii/S0047259X17305729},
	doi = {10.1016/j.jmva.2018.12.007},
	abstract = {Linear discriminant analysis (LDA) is a well-known classification technique that enjoyed great success in practical applications. Despite its effectiveness for traditional low-dimensional problems, extensions of LDA are necessary in order to classify high-dimensional data. Many variants of LDA have been proposed in the literature. However, most of these methods do not fully incorporate the structure information among predictors when such information is available. In this paper, we introduce a new high-dimensional LDA technique, namely graph-based sparse LDA (GSLDA), that utilizes the graph structure among the features. In particular, we use the regularized regression formulation for penalized LDA techniques, and propose to impose a structure-based sparse penalty on the discriminant vector β. The graph structure can be either given or estimated from the training data. Moreover, we explore the relationship between the within-class feature structure and the overall feature structure. Based on this relationship, we further propose a variant of our proposed GSLDA to utilize effectively unlabeled data, which can be abundant in the semi-supervised learning setting. With the new regularization, we can obtain a sparse estimate of β andmore accurate and interpretable classifiers than many existing methods. Both the selection consistency of β estimation and the convergence rate of the classifier are established, and the resulting classifier has an asymptotic Bayes error rate. Finally, we demonstrate the competitive performance of the proposed GSLDA on both simulated and real data studies.},
	urldate = {2019-01-04},
	journal = {Journal of Multivariate Analysis},
	author = {Liu, Jianyu and Yu, Guan and Liu, Yufeng},
	month = dec,
	year = {2018},
	keywords = {Regularization, Undirected graph, Feature structure, Gaussian graphical models},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/2R5SKC5W/Liu et al. - 2018 - Graph-based sparse linear discriminant analysis fo.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/8ZZZUFDV/S0047259X17305729.html:text/html}
}

@article{chakraborty_graph_2019,
	title = {A graph {Laplacian} prior for {Bayesian} variable selection and grouping},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947319300040},
	doi = {10.1016/j.csda.2019.01.003},
	abstract = {Variable selection, or subset selection, plays a fundamental role in modern statistical modeling. In many applications, interactions exist between the selected variables. Statistical modeling of such dependence structure is of great importance. In this paper, the focus is on cases in which some correlated predictors have similar effects on the response, and will be grouped into predictive clusters. Here a graph Laplacian prior (GL-prior) is introduced within the Bayesian framework, the Maximum A Posterior (MAP) estimate which simultaneously allows for variable selection, coefficient estimation and predictive group identification. The connections between the GL-prior (graph Laplacian) and the existing regularized regression methods are established accordingly. For computation, an EM based algorithm is proposed, where an efficient augmented Lagrangian approach is utilized for the maximization step. The performance of the proposed approach is examined through simulation studies, followed by a microarray data analysis concerning the plant Arabidopsis thaliana.},
	urldate = {2019-01-21},
	journal = {Computational Statistics \& Data Analysis},
	author = {Chakraborty, Sounak and Lozano, Aurelie C.},
	month = jan,
	year = {2019},
	keywords = {Variable selection, Bayesian analysis, Graph Laplacian matrix, Predictive cluster, Regularized regression},
	file = {ScienceDirect Full Text PDF:/Users/magnusmunch/Zotero/storage/XACPWXZX/Chakraborty and Lozano - 2019 - A graph Laplacian prior for Bayesian variable sele.pdf:application/pdf;ScienceDirect Snapshot:/Users/magnusmunch/Zotero/storage/HXSSGEB9/S0167947319300040.html:text/html}
}

@article{te_beest_improved_2017,
	title = {Improved high-dimensional prediction with {Random} {Forests} by the use of co-data},
	volume = {18},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/s12859-017-1993-1},
	doi = {10.1186/s12859-017-1993-1},
	abstract = {Prediction in high dimensional settings is difficult due to the large number of variables relative to the sample size. We demonstrate how auxiliary ‘co-data’ can be used to improve the performance of a Random Forest in such a setting.},
	number = {1},
	urldate = {2019-02-12},
	journal = {BMC Bioinformatics},
	author = {te Beest, Dennis E. and Mes, Steven W. and Wilting, Saskia M. and Brakenhoff, Ruud H. and Van De Wiel, Mark A},
	month = dec,
	year = {2017},
	pages = {584},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/XGL53RPW/te Beest et al. - 2017 - Improved high-dimensional prediction with Random F.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/7PEGGSNC/s12859-017-1993-1.html:text/html}
}

@article{the_cancer_genome_atlas_network_comprehensive_2015,
	title = {Comprehensive genomic characterization of head and neck squamous cell carcinomas},
	volume = {517},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14129},
	doi = {10.1038/nature14129},
	abstract = {The Cancer Genome Atlas profiled 279 head and neck squamous cell carcinomas (HNSCCs) to provide a comprehensive landscape of somatic genomic alterations. Here we show that human-papillomavirus-associated tumours are dominated by helical domain mutations of the oncogene PIK3CA, novel alterations involving loss of TRAF3, and amplification of the cell cycle gene E2F1. Smoking-related HNSCCs demonstrate near universal loss-of-function TP53 mutations and CDKN2A inactivation with frequent copy number alterations including amplification of 3q26/28 and 11q13/22. A subgroup of oral cavity tumours with favourable clinical outcomes displayed infrequent copy number alterations in conjunction with activating mutations of HRAS or PIK3CA, coupled with inactivating mutations of CASP8, NOTCH1 and TP53. Other distinct subgroups contained loss-of-function alterations of the chromatin modifier NSD1, WNT pathway genes AJUBA and FAT1, and activation of oxidative stress factor NFE2L2, mainly in laryngeal tumours. Therapeutic candidate alterations were identified in most HNSCCs.},
	language = {en},
	number = {7536},
	urldate = {2019-02-22},
	journal = {Nature},
	author = {{The Cancer Genome Atlas Network}},
	month = jan,
	year = {2015},
	pages = {576--582},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/PCLQTFY3/The Cancer Genome Atlas Network - 2015 - Comprehensive genomic characterization of head and.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/MH7KVSNI/nature14129.html:text/html}
}

@article{mes_prognostic_2017,
	title = {Prognostic modeling of oral cancer by gene profiles and clinicopathological co-variables},
	volume = {8},
	issn = {1949-2553},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5601734/},
	doi = {10.18632/oncotarget.19576},
	abstract = {Accurate staging and outcome prediction is a major problem in clinical management of oral cancer patients, hampering high precision treatment and adjuvant therapy planning. Here, we have built and validated multivariable models that integrate gene signatures with clinical and pathological variables to improve staging and survival prediction of patients with oral squamous cell carcinoma (OSCC). Gene expression profiles from 249 human papillomavirus (HPV)-negative OSCCs were explored to identify a 22-gene lymph node metastasis signature (LNMsig) and a 40-gene overall survival signature (OSsig). To facilitate future clinical implementation and increase performance, these signatures were transferred to quantitative polymerase chain reaction (qPCR) assays and validated in an independent cohort of 125 HPV-negative tumors. When applied in the clinically relevant subgroup of early-stage (cT1-2N0) OSCC, the LNMsig could prevent overtreatment in two-third of the patients. Additionally, the integration of RT-qPCR gene signatures with clinical and pathological variables provided accurate prognostic models for oral cancer, strongly outperforming TNM. Finally, the OSsig gene signature identified a subpopulation of patients, currently considered at low-risk for disease-related survival, who showed an unexpected poor prognosis. These well-validated models will assist in personalizing primary treatment with respect to neck dissection and adjuvant therapies.},
	number = {35},
	urldate = {2019-02-22},
	journal = {Oncotarget},
	author = {Mes, Steven W. and te Beest, Dennis and Poli, Tito and Rossi, Silvia and Scheckenbach, Kathrin and van Wieringen, Wessel N. and Brink, Arjen and Bertani, Nicoletta and Lanfranco, Davide and Silini, Enrico M. and van Diest, Paul J. and Bloemena, Elisabeth and Leemans, C. René and van de Wiel, Mark A. and Brakenhoff, Ruud H.},
	month = jul,
	year = {2017},
	pmid = {28938638},
	pmcid = {PMC5601734},
	pages = {59312--59323},
	file = {PubMed Central Full Text PDF:/Users/magnusmunch/Zotero/storage/S5YYFTWI/Mes et al. - 2017 - Prognostic modeling of oral cancer by gene profile.pdf:application/pdf}
}

@article{masayesva_gene_2004,
	title = {Gene expression alterations over large chromosomal regions in cancers include multiple genes unrelated to malignant progression},
	volume = {101},
	copyright = {Copyright © 2004, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/101/23/8715},
	doi = {10.1073/pnas.0400027101},
	abstract = {In solid tumors, the relationship between DNA copy number and global expression over large chromosomal regions has not been systematically explored. We used a 12,626-gene expression array analysis of head and neck squamous cell carcinoma and normal oral mucosa and annotated gene expression levels to specific chromosomal loci. Expression alterations correlated with reported data using comparative genomic hybridization. When genes with significant differences in expression between normal and malignant lesions, as defined by significance analysis of microarrays (SAM), were compared to nonsignificant genes, similar chromosomal patterns of alteration in expression were noted. Individual tumors underwent microsatellite analysis and χ2 analysis of expression at 3p and 22q. Significant 3p underexpression and 22q overexpression were found in all primary tumors with 3p and 22q allelic imbalance, respectively, whereas no tumor without allelic imbalance on these chromosomal arms demonstrated expression differences. Loss and gain of chromosomal material in solid cancers can alter gene expression over large chromosomal regions, including multiple genes unrelated to malignant progression.},
	language = {en},
	number = {23},
	urldate = {2019-02-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Masayesva, Brett G. and Ha, Patrick and Garrett-Mayer, Elizabeth and Pilkington, Thomas and Mao, Rong and Pevsner, Jonathan and Speed, Traci and Benoit, Nicole and Moon, Chul-So and Sidransky, David and Westra, William H. and Califano, Joseph},
	month = jun,
	year = {2004},
	pmid = {15155901},
	keywords = {allelic imbalance, chromosomal mapping, comparative genomic hybridization, head and neck squamous cell carcinoma, microarrays},
	pages = {8715--8720},
	file = {Full Text PDF:/Users/magnusmunch/Zotero/storage/WBIC65RL/Masayesva et al. - 2004 - Gene expression alterations over large chromosomal.pdf:application/pdf;Snapshot:/Users/magnusmunch/Zotero/storage/H9U98GWF/8715.html:text/html}
}

@article{leday_fast_2018,
	title = {Fast {Bayesian} inference in large {Gaussian} graphical models},
	url = {http://arxiv.org/abs/1803.08155},
	abstract = {Despite major methodological developments, Bayesian inference for Gaussian graphical models remains challenging in high dimension due to the tremendous size of the model space. This article proposes a method to infer the marginal and conditional independence structures between variables by multiple testing of hypotheses. Specifically, we introduce closed-form Bayes factors under the Gaussian conjugate model to evaluate the null hypotheses of marginal and conditional independence between variables. Their computation for all pairs of variables is shown to be extremely efficient, thereby allowing us to address large problems with thousands of nodes. Moreover, we derive exact tail probabilities from the null distributions of the Bayes factors. These allow the use of any multiplicity correction procedure to control error rates for incorrect edge inclusion. We demonstrate the proposed approach to graphical model selection on various simulated examples as well as on a large gene expression data set from The Cancer Genome Atlas.},
	urldate = {2019-04-12},
	journal = {arXiv:1803.08155 [stat]},
	author = {Leday, Gwenaël G. R. and Richardson, Sylvia},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.08155},
	keywords = {Statistics - Methodology, to read},
	file = {arXiv\:1803.08155 PDF:/Users/magnusmunch/Zotero/storage/WL78SMIC/Leday and Richardson - 2018 - Fast Bayesian inference in large Gaussian graphica.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/4AEHTNAF/1803.html:text/html;biom.13064.pdf:/Users/magnusmunch/Zotero/storage/7DY73FUW/biom.13064.pdf:application/pdf;biom13064-sup-0001-supp_to_manuscriptbf_l_and_r.pdf:/Users/magnusmunch/Zotero/storage/E8J7BW5I/biom13064-sup-0001-supp_to_manuscriptbf_l_and_r.pdf:application/pdf}
}

@inproceedings{urda_addition_2019,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Addition of {Pathway}-{Based} {Information} to {Improve} {Predictions} in {Transcriptomics}},
	isbn = {978-3-030-17935-9},
	abstract = {The diagnosis and prognosis of cancer are among the more critical challenges that modern medicine confronts. In this sense, personalized medicine aims to use data from heterogeneous sources to estimate the evolution of the disease for each specific patient in order to fit the more appropriate treatments. In recent years, DNA sequencing data have boosted cancer prediction and treatment by supplying genetic information that has been used to design genetic signatures or biomarkers that led to a better classification of the different subtypes of cancer as well as to a better estimation of the evolution of the disease and the response to diverse treatments. Several machine learning models have been proposed in the literature for cancer prediction. However, the efficacy of these models can be seriously affected by the existing imbalance between the high dimensionality of the gene expression feature sets and the number of samples available, what is known as the curse of dimensionality. Although linear predictive models could give worse performance rates when compared to more sophisticated non-linear models, they have the main advantage of being interpretable. However, the use of domain-specific information has been proved useful to boost the performance of multivariate linear predictors in high dimensional settings. In this work, we design a set of linear predictive models that incorporate domain-specific information from genetic pathways for effective feature selection. By combining these linear model with other classical machine learning models, we get state-of-art performance rates in the prediction of vital status on a public cancer dataset.},
	language = {en},
	booktitle = {Bioinformatics and {Biomedical} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Urda, Daniel and Veredas, Francisco J. and Turias, Ignacio and Franco, Leonardo},
	editor = {Rojas, Ignacio and Valenzuela, Olga and Rojas, Fernando and Ortuño, Francisco},
	year = {2019},
	keywords = {Machine learning, Next-generation sequencing, Predictive modelling, Problem-specific information},
	pages = {200--208},
	file = {Springer Full Text PDF:/Users/magnusmunch/Zotero/storage/HEF27GGA/Urda et al. - 2019 - Addition of Pathway-Based Information to Improve P.pdf:application/pdf}
}

@article{shimamura_bayesian_2019,
	title = {Bayesian generalized fused lasso modeling via {NEG} distribution},
	volume = {48},
	issn = {0361-0926, 1532-415X},
	url = {https://www.tandfonline.com/doi/full/10.1080/03610926.2018.1489056},
	doi = {10.1080/03610926.2018.1489056},
	abstract = {The fused lasso penalizes a loss function by the L1 norm for both the regression coefficients and their successive differences to encourage sparsity of both. In this paper, we propose a Bayesian generalized fused lasso modeling based on a normal-exponential-gamma (NEG) prior distribution. The NEG prior is assumed into the difference of successive regression coefficients. The proposed method enables us to construct a more versatile sparse model than the ordinary fused lasso using a flexible regularization term. Simulation studies and real data analyses show that the proposed method has superior performance to the ordinary fused lasso.},
	language = {en},
	number = {16},
	urldate = {2019-12-11},
	journal = {Communications in Statistics - Theory and Methods},
	author = {Shimamura, Kaito and Ueki, Masao and Kawano, Shuichi and Konishi, Sadanori},
	month = aug,
	year = {2019},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Primary 62F15, 62J07, Secondary 62J05},
	pages = {4132--4153},
	file = {arXiv\:1602.04910 PDF:/Users/magnusmunch/Zotero/storage/A7KPE4VV/Shimamura et al. - 2016 - Bayesian generalized fused lasso modeling via NEG .pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/53FCTAN9/1602.html:text/html;Shimamura et al. - 2019 - Bayesian generalized fused lasso modeling via NEG .pdf:/Users/magnusmunch/Zotero/storage/EX8U93HY/Shimamura et al. - 2019 - Bayesian generalized fused lasso modeling via NEG .pdf:application/pdf}
}

@article{hannart_estimating_2014,
	title = {Estimating high dimensional covariance matrices: {A} new look at the {Gaussian} conjugate framework},
	volume = {131},
	issn = {0047259X},
	shorttitle = {Estimating high dimensional covariance matrices},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X14001316},
	doi = {10.1016/j.jmva.2014.06.001},
	abstract = {In this paper, we describe and study a class of linear shrinkage estimators of the covariance matrix that is well-suited for high dimensional matrices, has a rather wide domain of applicability, and is rooted into the Gaussian conjugate framework of Chen (1979). We propose here a new look at this framework. The linear shrinkage estimator is thereby obtained as the posterior mean of the covariance, using a Bayesian Gaussian model with conjugate inverse Wishart prior, and deriving the shrinkage intensity and target matrix by marginal likelihood maximization. We introduce some extensions to the seminal approach by deriving a closed-form expression of the marginal likelihood as well as computationally light schemes for its maximization. Further, these developments are implemented in a variety of situations and include a simulation-based performance comparison with a recent, widely used class of linear shrinkage estimators. The Gaussian conjugate estimators are found to outperform these estimators in every tested situation where the latter are available and to be more widely and directly applicable.},
	language = {en},
	urldate = {2019-12-12},
	journal = {Journal of Multivariate Analysis},
	author = {Hannart, Alexis and Naveau, Philippe},
	month = oct,
	year = {2014},
	pages = {149--162},
	file = {Hannart and Naveau - 2014 - Estimating high dimensional covariance matrices A.pdf:/Users/magnusmunch/Zotero/storage/W8X28EE7/Hannart and Naveau - 2014 - Estimating high dimensional covariance matrices A.pdf:application/pdf}
}

@article{uhler_exact_2016,
	title = {Exact formulas for the normalizing constants of {Wishart} distributions for graphical models},
	url = {http://arxiv.org/abs/1406.4901},
	abstract = {Gaussian graphical models have received considerable attention during the past four decades from the statistical and machine learning communities. In Bayesian treatments of this model, the G-Wishart distribution serves as the conjugate prior for inverse covariance matrices satisfying graphical constraints. While it is straightforward to posit the unnormalized densities, the normalizing constants of these distributions have been known only for graphs that are chordal, or decomposable. Up until now, it was unknown whether the normalizing constant for a general graph could be represented explicitly, and a considerable body of computational literature emerged that attempted to avoid this apparent intractability. We close this question by providing an explicit representation of the G-Wishart normalizing constant for general graphs.},
	language = {en},
	urldate = {2019-12-12},
	journal = {arXiv:1406.4901 [math, stat]},
	author = {Uhler, Caroline and Lenkoski, Alex and Richards, Donald},
	month = jun,
	year = {2016},
	note = {arXiv: 1406.4901},
	keywords = {Mathematics - Statistics Theory, 62H05, 60E05, 62E15},
	file = {Uhler et al. - 2016 - Exact formulas for the normalizing constants of Wi.pdf:/Users/magnusmunch/Zotero/storage/TD2RC6BC/Uhler et al. - 2016 - Exact formulas for the normalizing constants of Wi.pdf:application/pdf}
}

@article{smith_restricted_2015,
	title = {Restricted {Covariance} {Priors} with {Applications} in {Spatial} {Statistics}},
	volume = {10},
	issn = {1936-0975},
	url = {http://arxiv.org/abs/1402.5655},
	doi = {10.1214/14-BA927},
	abstract = {We present a Bayesian model for area-level count data that uses Gaussian random eﬀects with a novel type of G-Wishart prior on the inverse variance–covariance matrix. Speciﬁcally, we introduce a new distribution called the truncated G-Wishart distribution that has support over precision matrices that lead to positive associations between the random eﬀects of neighboring regions while preserving conditional independence of non-neighboring regions. We describe Markov chain Monte Carlo sampling algorithms for the truncated G-Wishart prior in a disease mapping context and compare our results to Bayesian hierarchical models based on intrinsic autoregression priors. A simulation study illustrates that using the truncated G-Wishart prior improves over the intrinsic autoregressive priors when there are discontinuities in the disease risk surface. The new model is applied to an analysis of cancer incidence data in Washington State.},
	language = {en},
	number = {4},
	urldate = {2019-12-12},
	journal = {Bayesian Analysis},
	author = {Smith, Theresa R. and Wakefield, Jon and Dobra, Adrian},
	month = dec,
	year = {2015},
	note = {arXiv: 1402.5655},
	keywords = {Statistics - Methodology},
	pages = {965--990},
	file = {Smith et al. - 2015 - Restricted Covariance Priors with Applications in .pdf:/Users/magnusmunch/Zotero/storage/5M2GCRIQ/Smith et al. - 2015 - Restricted Covariance Priors with Applications in .pdf:application/pdf}
}

@article{kollo_approximating_1995,
	title = {Approximating by the {Wishart} distribution},
	volume = {47},
	issn = {0020-3157, 1572-9052},
	url = {http://link.springer.com/10.1007/BF01856546},
	doi = {10.1007/BF01856546},
	language = {en},
	number = {4},
	urldate = {2019-12-13},
	journal = {Annals of the Institute of Statistical Mathematics},
	author = {Kollo, Tönu and von Rosen, Dietrich},
	month = dec,
	year = {1995},
	pages = {767--783},
	file = {Kollo and von Rosen - 1995 - Approximating by the Wishart distribution.pdf:/Users/magnusmunch/Zotero/storage/UGAEF3YY/Kollo and von Rosen - 1995 - Approximating by the Wishart distribution.pdf:application/pdf}
}

@article{royen_non-central_2016,
	title = {{NON}-{CENTRAL} {MULTIVARIATE} {CHI}-{SQUARE} {AND} {GAMMA} {DISTRIBUTIONS}},
	volume = {52},
	issn = {09720863},
	url = {http://www.pphmj.com/abstract/10073.htm},
	doi = {10.17654/TS052040289},
	language = {en},
	number = {4},
	urldate = {2019-12-13},
	journal = {Far East Journal of Theoretical Statistics},
	author = {Royen, Thomas},
	month = sep,
	year = {2016},
	pages = {289--315},
	file = {Royen - 2016 - NON-CENTRAL MULTIVARIATE CHI-SQUARE AND GAMMA DIST.pdf:/Users/magnusmunch/Zotero/storage/UF2XK2DD/Royen - 2016 - NON-CENTRAL MULTIVARIATE CHI-SQUARE AND GAMMA DIST.pdf:application/pdf}
}

@article{hinton_using_nodate,
	title = {Using {Deep} {Belief} {Nets} to {Learn} {Covariance} {Kernels} for {Gaussian} {Processes}},
	abstract = {We show how to use unlabeled data and a deep belief net (DBN) to learn a good covariance kernel for a Gaussian process. We ﬁrst learn a deep generative model of the unlabeled data using the fast, greedy algorithm introduced by [7]. If the data is high-dimensional and highly-structured, a Gaussian kernel applied to the top layer of features in the DBN works much better than a similar kernel applied to the raw input. Performance at both regression and classiﬁcation can then be further improved by using backpropagation through the DBN to discriminatively ﬁne-tune the covariance kernel.},
	language = {en},
	author = {Hinton, Geoffrey E and Salakhutdinov, Russ R},
	pages = {8},
	file = {Hinton and Salakhutdinov - Using Deep Belief Nets to Learn Covariance Kernels.pdf:/Users/magnusmunch/Zotero/storage/R58UXT3Q/Hinton and Salakhutdinov - Using Deep Belief Nets to Learn Covariance Kernels.pdf:application/pdf}
}

@article{porzelius_leveraging_2011,
	title = {Leveraging external knowledge on molecular interactions in classification methods for risk prediction of patients},
	volume = {53},
	issn = {03233847},
	url = {http://doi.wiley.com/10.1002/bimj.201000155},
	doi = {10.1002/bimj.201000155},
	language = {en},
	number = {2},
	urldate = {2020-01-02},
	journal = {Biometrical Journal},
	author = {Porzelius, Christine and Johannes, Marc and Binder, Harald and Beißbarth, Tim},
	month = mar,
	year = {2011},
	keywords = {to read},
	pages = {190--201},
	file = {Porzelius et al. - 2011 - Leveraging external knowledge on molecular interac.pdf:/Users/magnusmunch/Zotero/storage/56TBY849/Porzelius et al. - 2011 - Leveraging external knowledge on molecular interac.pdf:application/pdf}
}

@article{munch_adaptive_2019,
	title = {Adaptive group-regularized logistic elastic net regression},
	issn = {1465-4644, 1468-4357},
	url = {https://academic.oup.com/biostatistics/advance-article/doi/10.1093/biostatistics/kxz062/5689687},
	doi = {10.1093/biostatistics/kxz062},
	abstract = {In high-dimensional data settings, additional information on the features is often available. Examples of such external information in omics research are: (i) p-values from a previous study and (ii) omics annotation. The inclusion of this information in the analysis may enhance classiﬁcation performance and feature selection but is not straightforward. We propose a group-regularized (logistic) elastic net regression method, where each penalty parameter corresponds to a group of features based on the external information. The method, termed gren, makes use of the Bayesian formulation of logistic elastic net regression to estimate both the model and penalty parameters in an approximate empirical–variational Bayes framework. Simulations and applications to three cancer genomics studies and one Alzheimer metabolomics study show that, if the partitioning of the features is informative, classiﬁcation performance, and feature selection are indeed enhanced.},
	language = {en},
	urldate = {2020-01-02},
	journal = {Biostatistics},
	author = {Münch, Magnus M and Peeters, Carel F W and Van Der Vaart, Aad W and Van De Wiel, Mark A},
	month = dec,
	year = {2019},
	keywords = {Statistics - Methodology},
	file = {arXiv\:1805.00389 PDF:/Users/magnusmunch/Zotero/storage/4RZ5M98Z/Münch et al. - 2018 - Adaptive group-regularized logistic elastic net re.pdf:application/pdf;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/HWCM9IDB/1805.html:text/html;arXiv.org Snapshot:/Users/magnusmunch/Zotero/storage/5Z9KDUGJ/1805.html:text/html;biosts-19093-File046.pdf:/Users/magnusmunch/Zotero/storage/8UVG5YN9/biosts-19093-File046.pdf:application/pdf;Münch et al. - 2019 - Adaptive group-regularized logistic elastic net re.pdf:/Users/magnusmunch/Zotero/storage/4PNK28YQ/Münch et al. - 2019 - Adaptive group-regularized logistic elastic net re.pdf:application/pdf}
}

@book{zhu_introduction_2009,
	address = {San Rafael, California, USA},
	series = {Synthesis lectures on artificial intelligence and machine learning},
	title = {Introduction to semi-supervised learning},
	isbn = {978-1-59829-547-4 978-1-59829-548-1},
	language = {en},
	number = {6},
	publisher = {Morgan \& Claypool},
	author = {Zhu, Xiaojin and Goldberg, Andrew B.},
	year = {2009},
	note = {OCLC: 731094006},
	file = {Zhu and Goldberg - 2009 - Introduction to semi-supervised learning.pdf:/Users/magnusmunch/Zotero/storage/IHRBN7JP/Zhu and Goldberg - 2009 - Introduction to semi-supervised learning.pdf:application/pdf}
}

@article{zhang_semi-supervised_2019,
	title = {Semi-supervised inference: {General} theory and estimation of means},
	volume = {47},
	issn = {0090-5364},
	shorttitle = {Semi-supervised inference},
	url = {https://projecteuclid.org/euclid.aos/1564797856},
	doi = {10.1214/18-AOS1756},
	language = {en},
	number = {5},
	urldate = {2020-01-17},
	journal = {The Annals of Statistics},
	author = {Zhang, Anru and Brown, Lawrence D. and Cai, T. Tony},
	month = oct,
	year = {2019},
	pages = {2538--2566},
	file = {Zhang et al. - 2019 - Semi-supervised inference General theory and esti.pdf:/Users/magnusmunch/Zotero/storage/8Y62U3GT/Zhang et al. - 2019 - Semi-supervised inference General theory and esti.pdf:application/pdf}
}

@article{azriel_semi-supervised_2018,
	title = {Semi-{Supervised} linear regression},
	url = {http://arxiv.org/abs/1612.02391},
	abstract = {We study a regression problem where for some part of the data we observe both the label variable (Y ) and the predictors (X), while for other part of the data only the predictors are given. Such a problem arises, for example, when observations of the label variable are costly and may require a skilled human agent. If the conditional expectation ErY {\textbar}Xs is exactly linear in X then typically the additional observations of the X’s do not contain useful information, but otherwise the unlabeled data can be informative. In this case, our aim is at constructing the best linear predictor. We suggest improved alternative estimates to the naive standard procedures that depend only on the labeled data. Our estimation method can be easily implemented and has simply described asymptotic properties. The new estimates asymptotically dominate the usual standard procedures under certain non-linearity condition of ErY {\textbar}Xs; otherwise, they are asymptotically equivalent. The performance of the new estimator for small sample size is investigated in an extensive simulation study. A real data example of inferring homeless population is used to illustrate the new methodology.},
	language = {en},
	urldate = {2020-01-17},
	journal = {arXiv:1612.02391 [math, stat]},
	author = {Azriel, David and Brown, Lawrence D. and Sklar, Michael and Berk, Richard and Buja, Andreas and Zhao, Linda},
	month = sep,
	year = {2018},
	note = {arXiv: 1612.02391},
	keywords = {Mathematics - Statistics Theory},
	file = {Azriel et al. - 2018 - Semi-Supervised linear regression.pdf:/Users/magnusmunch/Zotero/storage/IASLF6EQ/Azriel et al. - 2018 - Semi-Supervised linear regression.pdf:application/pdf}
}

@article{culp_semisupervised_2013,
	title = {On the {Semisupervised} {Joint} {Trained} {Elastic} {Net}},
	volume = {22},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1080/10618600.2012.657139},
	doi = {10.1080/10618600.2012.657139},
	language = {en},
	number = {2},
	urldate = {2020-01-17},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Culp, Mark},
	month = apr,
	year = {2013},
	pages = {300--318},
	file = {Culp - 2013 - On the Semisupervised Joint Trained Elastic Net.pdf:/Users/magnusmunch/Zotero/storage/Z7JZ6JNV/Culp - 2013 - On the Semisupervised Joint Trained Elastic Net.pdf:application/pdf}
}

@article{larsen_semisupervised_2020,
	title = {Semi‐supervised covariate shift modelling of spectroscopic data},
	volume = {34},
	issn = {0886-9383, 1099-128X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.3204},
	doi = {10.1002/cem.3204},
	abstract = {Utilizing the full potential of spectroscopic calibrations in changing environments typically requires large amounts of maintenance and/or model updates as the presence of new sources of variation makes the calibration insufficient. In this paper, we propose the use of unlabelled data in order to automize such maintenance. We extend the Linear Joint Trained Framework by Ryan and Culp such that the shifts in mean value and covariance structure are modelled explicitly. The extension yields a more flexible framework, and thus we are able to regularize the final calibration in a more desirable manner. The proposed framework is tested on a simulated dataset where we simulate three different realistic scenarios that are either challenging for classic multivariate calibrations or challenging when adding unlabelled data. Furthermore, we test our framework on two real datasets across multiple data splits. We find that our framework not only achieves the same (and in some instances lower) error level as that of the baseline model (NARE), it also yields better calibration models than the Linear Joint Trained Framework.},
	language = {en},
	number = {3},
	urldate = {2020-05-07},
	journal = {Journal of Chemometrics},
	author = {Larsen, Jacob Søgaard and Clemmensen, Line and Stockmarr, Anders and Skov, Thomas and Larsen, Anders and Ersbøll, Bjarne Kjær},
	month = mar,
	year = {2020},
	file = {Larsen et al. - 2020 - Semi‐supervised covariate shift modelling of spect.pdf:/Users/magnusmunch/Zotero/storage/2H6DGGTK/Larsen et al. - 2020 - Semi‐supervised covariate shift modelling of spect.pdf:application/pdf}
}

@article{ryan_semi-supervised_nodate,
	title = {On {Semi}-{Supervised} {Linear} {Regression} in {Covariate} {Shift} {Problems}},
	abstract = {Semi-supervised learning approaches are trained using the full training (labeled) data and available testing (unlabeled) data. Demonstrations of the value of training with unlabeled data typically depend on a smoothness assumption relating the conditional expectation to high density regions of the marginal distribution and an inherent missing completely at random assumption for the labeling. So-called covariate shift poses a challenge for many existing semi-supervised or supervised learning techniques. Covariate shift models allow the marginal distributions of the labeled and unlabeled feature data to diﬀer, but the conditional distribution of the response given the feature data is the same. An example of this occurs when a complete labeled data sample and then an unlabeled sample are obtained sequentially, as it would likely follow that the distributions of the feature data are quite diﬀerent between samples. The value of using unlabeled data during training for the elastic net is justiﬁed geometrically in such practical covariate shift problems. The approach works by obtaining adjusted coeﬃcients for unlabeled prediction which recalibrate the supervised elastic net to compromise: (i) maintaining elastic net predictions on the labeled data with (ii) shrinking unlabeled predictions to zero. Our approach is shown to dominate linear supervised alternatives on unlabeled response predictions when the unlabeled feature data are concentrated on a low dimensional manifold away from the labeled data and the true coeﬃcient vector emphasizes directions away from this manifold. Large variance of the supervised predictions on the unlabeled set is reduced more than the increase in squared bias when the unlabeled responses are expected to be small, so an improved compromise within the bias-variance tradeoﬀ is the rationale for this performance improvement. Performance is validated on simulated and real data.},
	language = {en},
	author = {Ryan, Kenneth Joseph and Culp, Mark Vere},
	pages = {35},
	file = {Ryan and Culp - On Semi-Supervised Linear Regression in Covariate .pdf:/Users/magnusmunch/Zotero/storage/NF3XBJAB/Ryan and Culp - On Semi-Supervised Linear Regression in Covariate .pdf:application/pdf}
}

@article{ledoit_well-conditioned_2004,
	title = {A well-conditioned estimator for large-dimensional covariance matrices},
	volume = {88},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X03000964},
	doi = {10.1016/S0047-259X(03)00096-4},
	abstract = {Many applied problems require a covariance matrix estimator that is not only invertible, but also well-conditioned (that is, inverting it does not amplify estimation error). For largedimensional covariance matrices, the usual estimator—the sample covariance matrix—is typically not well-conditioned and may not even be invertible. This paper introduces an estimator that is both well-conditioned and more accurate than the sample covariance matrix asymptotically. This estimator is distribution-free and has a simple explicit formula that is easy to compute and interpret. It is the asymptotically optimal convex linear combination of the sample covariance matrix with the identity matrix. Optimality is meant with respect to a quadratic loss function, asymptotically as the number of observations and the number of variables go to inﬁnity together. Extensive Monte Carlo conﬁrm that the asymptotic results tend to hold well in ﬁnite sample.},
	language = {en},
	number = {2},
	urldate = {2020-05-07},
	journal = {Journal of Multivariate Analysis},
	author = {Ledoit, Olivier and Wolf, Michael},
	month = feb,
	year = {2004},
	pages = {365--411},
	file = {Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:/Users/magnusmunch/Zotero/storage/JSIAICRH/Ledoit and Wolf - 2004 - A well-conditioned estimator for large-dimensional.pdf:application/pdf}
}

@article{witten_covariance-regularized_2009,
	title = {Covariance-regularized regression and classification for high dimensional problems},
	volume = {71},
	issn = {13697412, 14679868},
	url = {http://doi.wiley.com/10.1111/j.1467-9868.2009.00699.x},
	doi = {10.1111/j.1467-9868.2009.00699.x},
	abstract = {We propose covariance-regularized regression, a family of methods for prediction in high dimensional settings that uses a shrunken estimate of the inverse covariance matrix of the features to achieve superior prediction. An estimate of the inverse covariance matrix is obtained by maximizing the log-likelihood of the data, under a multivariate normal model, subject to a penalty; it is then used to estimate coefﬁcients for the regression of the response onto the features. We show that ridge regression, the lasso and the elastic net are special cases of covariance-regularized regression, and we demonstrate that certain previously unexplored forms of covariance-regularized regression can outperform existing methods in a range of situations. The covariance-regularized regression framework is extended to generalized linear models and linear discriminant analysis, and is used to analyse gene expression data sets with multiple class and survival outcomes.},
	language = {en},
	number = {3},
	urldate = {2020-05-14},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Witten, Daniela M. and Tibshirani, Robert},
	month = jun,
	year = {2009},
	pages = {615--636},
	file = {Witten and Tibshirani - 2009 - Covariance-regularized regression and classificati.pdf:/Users/magnusmunch/Zotero/storage/L3GEADIM/Witten and Tibshirani - 2009 - Covariance-regularized regression and classificati.pdf:application/pdf}
}

@article{van_nee_flexible_2020,
	title = {Flexible co-data learning for high-dimensional prediction},
	url = {http://arxiv.org/abs/2005.04010},
	abstract = {Clinical research often focuses on complex traits in which many variables play a role in mechanisms driving, or curing, diseases. Clinical prediction is hard when data is highdimensional, but additional information, like domain knowledge and previously published studies, may be helpful to improve predictions. Such complementary data, or co-data, provide information on the covariates, such as genomic location or p-values from external studies. Our method enables exploiting multiple and various co-data sources to improve predictions. We use discrete or continuous co-data to deﬁne possibly overlapping or hierarchically structured groups of covariates. These are then used to estimate adaptive multi-group ridge penalties for generalised linear and Cox models. We combine empirical Bayes estimation of group penalty hyperparameters with an extra level of shrinkage. This renders a uniquely ﬂexible framework as any type of shrinkage can be used on the group level. The hyperparameter shrinkage learns how relevant a speciﬁc co-data source is, counters overﬁtting of hyperparameters for many groups, and accounts for structured co-data. We describe various types of co-data and propose suitable forms of hypershrinkage. The method is very versatile, as it allows for integration and weighting of multiple co-data sets, inclusion of unpenalised covariates and posterior variable selection. We demonstrate it on two cancer genomics applications and show that it may improve the performance of other dense and parsimonious prognostic models substantially, and stabilises variable selection.},
	language = {en},
	urldate = {2020-05-18},
	journal = {arXiv:2005.04010 [stat]},
	author = {van Nee, Mirrelijn M. and Wessels, Lodewyk F. A. and van de Wiel, Mark A.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.04010},
	keywords = {Statistics - Methodology},
	file = {van Nee et al. - 2020 - Flexible co-data learning for high-dimensional pre.pdf:/Users/magnusmunch/Zotero/storage/RPXLAFNS/van Nee et al. - 2020 - Flexible co-data learning for high-dimensional pre.pdf:application/pdf}
}

@article{fan_overview_2015,
	title = {An {Overview} on the {Estimation} of {Large} {Covariance} and {Precision} {Matrices}},
	url = {http://arxiv.org/abs/1504.02995},
	abstract = {Estimating large covariance and precision matrices are fundamental in modern multivariate analysis. The problems arise from statistical analysis of large panel economics and ﬁnance data. The covariance matrix reveals marginal correlations between variables, while the precision matrix encodes conditional correlations between pairs of variables given the remaining variables. In this paper, we provide a selective review of several recent developments on estimating large covariance and precision matrices. We focus on two general approaches: rank based method and factor model based method. Theories and applications of both approaches are presented. These methods are expected to be widely applicable to analysis of economic and ﬁnancial data.},
	language = {en},
	urldate = {2020-05-22},
	journal = {arXiv:1504.02995 [stat]},
	author = {Fan, Jianqing and Liao, Yuan and Liu, Han},
	month = apr,
	year = {2015},
	note = {arXiv: 1504.02995},
	keywords = {Statistics - Methodology},
	file = {Fan et al. - 2015 - An Overview on the Estimation of Large Covariance .pdf:/Users/magnusmunch/Zotero/storage/6H255BBE/Fan et al. - 2015 - An Overview on the Estimation of Large Covariance .pdf:application/pdf}
}

@article{rosen_moments_2020,
	title = {Moments for the {Inverted} {Wishart} {Distribution}},
	abstract = {In order to obtain moments for the inverted Wishart distribution two new approaches are brought forward. One is applied to moments of order up to three whereas another, bearing less information, is applied to moments of arbitrary order. Expressions for the moments of arbitrary order are given in a recursive and non-recursive manner. As an application the growth curve model is considered. Results are given using the third order moments for the inverted Wishart distribution.},
	language = {en},
	author = {Rosen, Dietrich Von},
	year = {2020},
	pages = {14},
	file = {Rosen - 2020 - Moments for the Inverted Wishart Distribution.pdf:/Users/magnusmunch/Zotero/storage/CHEKZMX8/Rosen - 2020 - Moments for the Inverted Wishart Distribution.pdf:application/pdf}
}

@article{christensen_covariance_nodate,
	title = {Covariance of the {Wishart} {Distribution} with {Applications} to {Regression}},
	abstract = {We discuss the covariance matrix of the Wishart and the derivation of the covariance matrix of a regression estimate discussed in Tarpey et al. (2014). We also examine the relationship between inequalities considered in Cook, Forzani, and Rothman (2015) and Tarpey et al.’s response to that comment.},
	language = {en},
	author = {Christensen, Ronald},
	pages = {13},
	file = {Christensen - Covariance of the Wishart Distribution with Applic.pdf:/Users/magnusmunch/Zotero/storage/NETUYMA4/Christensen - Covariance of the Wishart Distribution with Applic.pdf:application/pdf}
}

@article{letac_all_2004,
	title = {All {Invariant} {Moments} of the {Wishart} {Distribution}},
	volume = {31},
	issn = {0303-6898, 1467-9469},
	url = {http://doi.wiley.com/10.1111/j.1467-9469.2004.01-043.x},
	doi = {10.1111/j.1467-9469.2004.01-043.x},
	abstract = {In this paper, we compute moments of a Wishart matrix variate U of the form E(Q(U) where Q(u) is a polynomial with respect to the entries of the symmetric matrix u, invariant in the sense that it depends only on the eigenvalues of the matrix u. This gives us in particular the expected value of any power of the Wishart matrix U or its inverse U-1. For our proofs, we do not rely on traditional combinatorial methods but rather on the interplay between two bases of the space of invariant polynomials in U. This means that all moments can be obtained through the multiplication of three matrices with known entries. Practically, the moments are obtained by computer with an extremely simple Maple program.},
	language = {en},
	number = {2},
	urldate = {2020-08-21},
	journal = {Scandinavian Journal of Statistics},
	author = {Letac, Gerard and Massam, Helene},
	month = jun,
	year = {2004},
	pages = {295--318},
	file = {Letac and Massam - 2004 - All Invariant Moments of the Wishart Distribution.pdf:/Users/magnusmunch/Zotero/storage/SS3SER2Y/Letac and Massam - 2004 - All Invariant Moments of the Wishart Distribution.pdf:application/pdf}
}

@article{williams_multivariate_nodate,
	title = {{MULTIVARIATE} {FINANCIAL} {ECONOMETRICS}: {WITH} {APPLICATIONS} {TO} {VOLATILITY} {MODELLING}, {OPTION} {PRICING} {AND} {ASSET} {ALLOCATION}},
	language = {en},
	author = {Williams, Julian},
	pages = {378},
	file = {Williams - MULTIVARIATE FINANCIAL ECONOMETRICS WITH APPLICAT.pdf:/Users/magnusmunch/Zotero/storage/XQ3GVZFP/Williams - MULTIVARIATE FINANCIAL ECONOMETRICS WITH APPLICAT.pdf:application/pdf}
}

@inproceedings{bernardo_bayesian_2003,
	title = {Bayesian {Factor} {Regression} {Models} in the “{Large} p, {Small} n” {Paradigm}},
	abstract = {I discuss Bayesian factor regression models with many explanatory variables. These models are of particular interest and applicability in problems of prediction, but also for elucidating underlying structure in predictor variables. One key motivating application here is in studies of gene expression in functional genomics. I ﬁrst discuss empirical factor (principal components) regression, and the use of general classes of shrinkage priors, with an example. These models raise foundational questions for Bayesians, and related practical issues, due to the use of design-dependent priors and the need to recover inferences on the eﬀects of the original, high-dimensional predictors. I then discuss latent factor models for high-dimensional variables, and regression approaches in which low-dimensional latent factors are the predictor variables. These models generalise empirical factor regression, provide for more incisive evaluation of factor structure underlying high-dimensional predictors, and resolve the modelling and practical issues in empirical factor models by casting the latter as limiting special cases. Finally, I turn to questions of prior speciﬁcation in these models, and introduce sparse latent factor models to induce sparsity in factor loadings matrices. Embedding such sparse latent factor models in factor regressions provides a novel approach to variable selection with very many predictors. The paper concludes with an example of sparse factor analysis of gene expression data and comments about further research.},
	language = {en},
	booktitle = {Bayesian {Statistics} 7},
	author = {West, M},
	editor = {Bernardo, J M and Bayarri, M J and Berger, J O and Dawid, A P and Heckerman, D and Smith, A F M and West, M},
	year = {2003},
	pages = {11},
	file = {Bernardo et al. - Bayesian Factor Regression Models in the “Large p,.pdf:/Users/magnusmunch/Zotero/storage/X7MQ6TCX/Bernardo et al. - Bayesian Factor Regression Models in the “Large p,.pdf:application/pdf}
}

@article{liang_use_2007,
	title = {The {Use} of {Unlabeled} {Data} in {Predictive} {Modeling}},
	volume = {22},
	issn = {0883-4237},
	url = {http://projecteuclid.org/euclid.ss/1190905518},
	doi = {10.1214/088342307000000032},
	abstract = {The incorporation of unlabeled data in regression and classifi cation analysis is an increasing focus of the applied statistics and machine learning literatures, with a number of recent examples demonstrating the po tential for unlabeled data to contribute to improved predictive accuracy. The statistical basis for this semisupervised analysis does not appear to have been well delineated; as a result, the underlying theory and rationale may be under appreciated, especially by nonstatisticians. There is also room for statisticians to become more fully engaged in the vigorous research in this important area of intersection of the statistical and computer sciences. Much of the theoret ical work in the literature has focused, for example, on geometric and struc tural properties of the unlabeled data in the context of particular algorithms, rather than probabilistic and statistical questions. This paper overviews the fundamental statistical foundations for predictive modeling and the general questions associated with unlabeled data, highlighting the relevance of ven erable concepts of sampling design and prior specification. This theory, illus trated with a series of central illustrative examples and two substantial real data analyses, shows precisely when, why and how unlabeled data matter. Key words and phrases: Bayesian analysis, Bayesian kernel regression, la tent factor models, mixture models, predictive distribution, semisupervised learning, unlabeled data.},
	language = {en},
	number = {2},
	urldate = {2020-08-25},
	journal = {Statistical Science},
	author = {Liang, Feng and Mukherjee, Sayan and West, Mike},
	month = may,
	year = {2007},
	pages = {189--205},
	file = {Liang et al. - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:/Users/magnusmunch/Zotero/storage/5KUSNQMQ/Liang et al. - 2007 - The Use of Unlabeled Data in Predictive Modeling.pdf:application/pdf}
}

@article{bhattacharya_sparse_2011,
	title = {Sparse {Bayesian} infinite factor models},
	volume = {98},
	issn = {0006-3444, 1464-3510},
	url = {https://academic.oup.com/biomet/article-lookup/doi/10.1093/biomet/asr013},
	doi = {10.1093/biomet/asr013},
	abstract = {We focus on sparse modelling of high-dimensional covariance matrices using Bayesian latent factor models. We propose a multiplicative gamma process shrinkage prior on the factor loadings which allows introduction of infinitely many factors, with the loadings increasingly shrunk towards zero as the column index increases. We use our prior on a parameter-expanded loading matrix to avoid the order dependence typical in factor analysis models and develop an efficient Gibbs sampler that scales well as data dimensionality increases. The gain in efficiency is achieved by the joint conjugacy property of the proposed prior, which allows block updating of the loadings matrix. We propose an adaptive Gibbs sampler for automatically truncating the infinite loading matrix through selection of the number of important factors. Theoretical results are provided on the support of the prior and truncation approximation bounds. A fast algorithm is proposed to produce approximate Bayes estimates. Latent factor regression methods are developed for prediction and variable selection in applications with high-dimensional correlated predictors. Operating characteristics are assessed through simulation studies, and the approach is applied to predict survival times from gene expression data.},
	language = {en},
	number = {2},
	urldate = {2020-08-25},
	journal = {Biometrika},
	author = {Bhattacharya, A. and Dunson, D. B.},
	month = jun,
	year = {2011},
	pages = {291--306},
	file = {Bhattacharya and Dunson - 2011 - Sparse Bayesian infinite factor models.pdf:/Users/magnusmunch/Zotero/storage/RPUL7F57/Bhattacharya and Dunson - 2011 - Sparse Bayesian infinite factor models.pdf:application/pdf}
}

@article{ferrari_bayesian_2020,
	title = {Bayesian {Factor} {Analysis} for {Inference} on {Interactions}},
	url = {http://arxiv.org/abs/1904.11603},
	abstract = {This article is motivated by the problem of inference on interactions among chemical exposures impacting human health outcomes. Chemicals often co-occur in the environment or in synthetic mixtures and as a result exposure levels can be highly correlated. We propose a latent factor joint model, which includes shared factors in both the predictor and response components while assuming conditional independence. By including a quadratic regression in the latent variables in the response component, we induce ﬂexible dimension reduction in characterizing main eﬀects and interactions. We propose a Bayesian approach to inference under this Factor analysis for INteractions (FIN) framework. Through appropriate modiﬁcations of the factor modeling structure, FIN can accommodate higher order interactions. We evaluate the performance using a simulation study and data from the National Health and Nutrition Examination Survey (NHANES). Code is available on GitHub.},
	language = {en},
	urldate = {2020-08-25},
	journal = {arXiv:1904.11603 [stat]},
	author = {Ferrari, Federico and Dunson, David B.},
	month = jan,
	year = {2020},
	note = {arXiv: 1904.11603},
	keywords = {Statistics - Methodology, Statistics - Applications},
	file = {Bayesian Factor Analysis for Inference on Interactions.pdf:/Users/magnusmunch/Zotero/storage/L2UEPZSW/Bayesian Factor Analysis for Inference on Interactions.pdf:application/pdf;Ferrari and Dunson - 2020 - Bayesian Factor Analysis for Inference on Interact.pdf:/Users/magnusmunch/Zotero/storage/X6NIQL6C/Ferrari and Dunson - 2020 - Bayesian Factor Analysis for Inference on Interact.pdf:application/pdf}
}

@article{lopes_bayesian_nodate,
	title = {{BAYESIAN} {MODEL} {ASSESSMENT} {IN} {FACTOR} {ANALYSIS}},
	abstract = {Factor analysis has been one of the most powerful and ﬂexible tools for assessment of multivariate dependence and codependence. Loosely speaking, it could be argued that the origin of its success rests in its very exploratory nature, where various kinds of data-relationships amongst the variables at study can be iteratively veriﬁed and/or refuted. Bayesian inference in factor analytic models has received renewed attention in recent years, partly due to computational advances but also partly to applied focuses generating factor structures as exempliﬁed by recent work in ﬁnancial time series modeling. The focus of our current work is on exploring questions of uncertainty about the number of latent factors in a multivariate factor model, combined with methodological and computational issues of model speciﬁcation and model ﬁtting. We explore reversible jump MCMC methods that build on sets of parallel Gibbs sampling-based analyses to generate suitable empirical proposal distributions and that address the challenging problem of ﬁnding eﬃcient proposals in high-dimensional models. Alternative MCMC methods based on bridge sampling are discussed, and these fully Bayesian MCMC approaches are compared with a collection of popular model selection methods in empirical studies. Various additional computational issues are discussed, including situations where prior information is scarce, and the methods are explored in studies of some simulated data sets and an econometric time series example.},
	language = {en},
	author = {Lopes, Hedibert Freitas and West, Mike},
	pages = {27},
	file = {Lopes and West - BAYESIAN MODEL ASSESSMENT IN FACTOR ANALYSIS.pdf:/Users/magnusmunch/Zotero/storage/VFIJSCIL/Lopes and West - BAYESIAN MODEL ASSESSMENT IN FACTOR ANALYSIS.pdf:application/pdf}
}

@article{brave_networks_nodate,
	title = {Networks and {Factor} {Analysis} {Theory} and {Evidence}},
	abstract = {In this paper, we demonstrate that network models commonly admit a factor model representation. However, estimates of the network’s common components by large approximate factor model methods are not always consistent, with the cross-sectional correlations induced by the underlying network structure posing a potential problem for identiﬁcation. We show how the collapsed factor model (CFM) approach can bridge the gap between reducing the proliferation of parameters in large networks and remaining ﬂexible in the permissible cross-sectional correlation structure. Using a multi-sector real business cycle model, with input-output linkages serving as the foundation for the network structure, we apply the CFM methodology to estimate a coincident indicator of the U.S. business cycle, leveraging a large panel of monthly real activity data series. We ﬁnd that this coincident indicator exhibits significant predictive power in identifying business cycle turning points, exceeding the performance of “gold standards” such as the Chicago Fed National Activity Index and the Aruoba-Diebold-Scotti Business Conditions Index.},
	language = {en},
	author = {Brave, Scott A and Butters, R Andrew},
	pages = {43},
	file = {Brave and Butters - Networks and Factor Analysis Theory and Evidence.pdf:/Users/magnusmunch/Zotero/storage/ZSJCSXM9/Brave and Butters - Networks and Factor Analysis Theory and Evidence.pdf:application/pdf}
}

@article{carvalho_high-dimensional_2008,
	title = {High-{Dimensional} {Sparse} {Factor} {Modeling}: {Applications} in {Gene} {Expression} {Genomics}},
	volume = {103},
	issn = {0162-1459, 1537-274X},
	shorttitle = {High-{Dimensional} {Sparse} {Factor} {Modeling}},
	url = {https://www.tandfonline.com/doi/full/10.1198/016214508000000869},
	doi = {10.1198/016214508000000869},
	language = {en},
	number = {484},
	urldate = {2020-08-28},
	journal = {Journal of the American Statistical Association},
	author = {Carvalho, Carlos M. and Chang, Jeffrey and Lucas, Joseph E. and Nevins, Joseph R. and Wang, Quanli and West, Mike},
	month = dec,
	year = {2008},
	pages = {1438--1456},
	file = {Carvalho et al. - 2008 - High-Dimensional Sparse Factor Modeling Applicati.pdf:/Users/magnusmunch/Zotero/storage/7E5JG4Y5/Carvalho et al. - 2008 - High-Dimensional Sparse Factor Modeling Applicati.pdf:application/pdf}
}

@book{peeters_bayesian_2012,
	title = {Bayesian {Exploratory} and {Confirmatory} {Factor} {Analysis}: {Perspectives} on {Constrained}-{Model} {Selection}},
	isbn = {978-90-393-5787-3},
	shorttitle = {Bayesian {Exploratory} and {Confirmatory} {Factor} {Analysis}},
	abstract = {The dissertation revolves around three aims. The first aim is the construction of a conceptually and computationally simple Bayes factor for Type I constrained-model selection (dimensionality determination) that is determinate under usage of improper priors and the subsequent utilization of this Bayes factor in a Bayesian exploratory factor analysis (EFA) concerned with the selection of an optimal dimensionality for the latent factors. Chapter 2 deals with this aim. It connects the candidate estimator for marginal likelihood computation with the usage of training sample priors so that, pending certain conditions, a simulation consistent MCMC implementation of well-known default Bayes factors is obtained. This automated candidate estimator allows for noninformative prior usage in EFA, such that the problem of interference of informative prior information with the determination of the intrinsic dimensionality of the factor solution may be avoided. Subsequently, an appropriate stopping rule for factor analytic data compression is proposed. The second aim of this dissertation is to construct a conceptually and computationally simple Bayes factor for Type II constrained-model selection (the determination of appropriate inequality restrictions on the parameter space) that is geared towards inequalities on regression-type parameters and the subsequent embedding of this Bayes factor within a strategy that allows one to express factor analytic structure using inequality constraints. This aim has been given attention in Chapters 3 and 4. Chapter 3 gives a set of conditions for global rotational identification of the factor model. The condition set enables the formulation of an unrestricted confirmatory factor model (UCFM). An UCFM is a factor analysis model that places only minimal restrictions on the factor model for achieving global rotational uniqueness of the factor solution, with the restrictions chosen such that they convey preconceived theoretical meaning and thus render unnecessary post-hoc rotation of the solution for interpretation purposes. In Chapter 4 a Bayes factor for Type II constrained-model selection is given. Computation of this Bayes factor is relatively simple as its expressions of model fit and model complexity are explicitly connected to, respectively, the posterior and prior probability mass satisfying the constraints defining the constrained model. This implies that one only needs to evaluate the number of times an appropriate MCMC sampler visits the permissable space. This Bayes factor is then used in rendering a take on confirmatory factor analysis (CFA) in which factor structure is expressed using inequalities. The strategy consists of choosing as a base model an UCFM and to subsequently express factor structure using inequalities on and between the free parameters in the loadings matrix. The third and final aim is to let the provisions from Aims 1 and 2 conjoin in order to develop an integrative factor analytic strategy that proposes a bridge crossing the divide between EFA and CFA and to bring this strategy to bear on substantive fields of study outside the direct realm that brought about the FA model. Chapter 5 applies the integrative strategy to epidemiological data on the metabolic syndrome. Chapter 6 utilizes this strategy to analyze political sciences data on decision acceptance and ethical leadership},
	language = {en},
	publisher = {Utrecht University},
	author = {Peeters, C.F.W},
	year = {2012},
	note = {OCLC: 6893592188},
	file = {Peeters - 2012 - Bayesian Exploratory and Confirmatory Factor Analy.pdf:/Users/magnusmunch/Zotero/storage/N9Z9S5PG/Peeters - 2012 - Bayesian Exploratory and Confirmatory Factor Analy.pdf:application/pdf}
}

@article{trendafilov_exploratory_2011,
	title = {Exploratory {Factor} {Analysis} of {Data} {Matrices} {With} {More} {Variables} {Than} {Observations}},
	volume = {20},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.09211},
	doi = {10.1198/jcgs.2011.09211},
	language = {en},
	number = {4},
	urldate = {2020-09-02},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Trendafilov, Nickolay T. and Unkel, Steffen},
	month = jan,
	year = {2011},
	pages = {874--891},
	file = {app.pdf:/Users/magnusmunch/Zotero/storage/HF8GFEVU/app.pdf:application/pdf;gefals.m:/Users/magnusmunch/Zotero/storage/CTN9Z7X3/gefals.m:text/plain;thurstone_26box:/Users/magnusmunch/Zotero/storage/AGGYMZHR/thurstone_26box:text/plain;Trendafilov and Unkel - 2011 - Exploratory Factor Analysis of Data Matrices With .pdf:/Users/magnusmunch/Zotero/storage/WLA5UPFT/Trendafilov and Unkel - 2011 - Exploratory Factor Analysis of Data Matrices With .pdf:application/pdf}
}

@book{mardia_multivariate_1979,
	address = {London [etc.]},
	series = {Probability and mathematical statistics 810776022},
	title = {Multivariate analysis},
	isbn = {0-12-471250-9},
	language = {eng},
	publisher = {Academic Press},
	author = {Mardia, Kanti V.},
	year = {1979},
	keywords = {31.73 mathematical statistics., Multivariate analysis., Multivariate analysis, QA278}
}

@article{banbura_maximum_2014,
	title = {Maximum {Likelihood} {Estimation} of {Factor} {Models} on {Datasets} with arbitrary {Pattern} of {Missing} {Data}: {ML} for {Factor} {Models} with {Missing} {Data}},
	volume = {29},
	issn = {08837252},
	shorttitle = {{MAXIMUM} {LIKELIHOOD} {ESTIMATION} {OF} {FACTOR} {MODELS} {ON} {DATASETS} {WITH} {ARBITRARY} {PATTERN} {OF} {MISSING} {DATA}},
	url = {http://doi.wiley.com/10.1002/jae.2306},
	doi = {10.1002/jae.2306},
	abstract = {In this paper we modify the expectation maximization algorithm in order to estimate the parameters of the dynamic factor model on a dataset with an arbitrary pattern of missing data. We also extend the model to the case with a serially correlated idiosyncratic component. The framework allows us to handle efﬁciently and in an automatic manner sets of indicators characterized by different publication delays, frequencies and sample lengths. This can be relevant, for example, for young economies for which many indicators have been compiled only recently. We evaluate the methodology in a Monte Carlo experiment and we apply it to nowcasting of the euro area gross domestic product. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2020-09-03},
	journal = {Journal of Applied Econometrics},
	author = {Bańbura, Marta and Modugno, Michele},
	month = jan,
	year = {2014},
	pages = {133--160},
	file = {Bańbura and Modugno - 2014 - MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR MODELS ON .pdf:/Users/magnusmunch/Zotero/storage/JGBDACCS/Bańbura and Modugno - 2014 - MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR MODELS ON .pdf:application/pdf}
}

@article{liu_maximum_1998,
	title = {Maximum {Likelihood} {Estimation} of {Factor} {Analysis} using the {ECME} {Algorithm} with {Complete} and {Incomplete} {Data}},
	volume = {8},
	abstract = {Factor analysis is a standard tool in educational testing contexts, which can be ﬁt using the EM algorithm (Dempster, Laird and Rubin (1977)). An extension of EM, called the ECME algorithm (Liu and Rubin (1994)), can be used to obtain ML estimates more eﬃciently in factor analysis models. ECME has an E-step, identical to the E-step of EM, but instead of EM’s M-step, it has a sequence of CM (conditional maximization) steps, each of which maximizes either the constrained expected complete-data log-likelihood, as with the ECM algorithm (Meng and Rubin (1993)), or the constrained actual log-likelihood. For factor analysis, we use two CM steps: the ﬁrst maximizes the expected complete-data log-likelihood over the factor loadings given ﬁxed uniquenesses, and the second maximizes the actual likelihood over the uniquenesses given ﬁxed factor loadings. We also describe EM and ECME for ML estimation of factor analysis from incomplete data, which arise in applications of factor analysis in educational testing contexts. ECME shares with EM its monotone increase in likelihood and stable convergence to an ML estimate, but converges more quickly than EM. This more rapid convergence not only can shorten CPU time, but at least as important, it allows for a substantially easier assessment of convergence, as shown by examples. We believe that the application of ECME to factor analysis illustrates the role that extended EM-type algorithms, such as the even more general AECM algorithm (Meng and van Dyk (1997)) and the PX-EM algorithm (Liu, Rubin and Wu (1997)), can play in ﬁtting complex models that can arise in educational testing contexts.},
	language = {en},
	journal = {Statistica Sinica},
	author = {Liu, Chuanhai and Rubin, Donald B},
	year = {1998},
	pages = {729--747},
	file = {Liu and Rubin - MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR ANALYSIS U.pdf:/Users/magnusmunch/Zotero/storage/DMYQL37S/Liu and Rubin - MAXIMUM LIKELIHOOD ESTIMATION OF FACTOR ANALYSIS U.pdf:application/pdf}
}

@article{jungbacker_maximum_2011,
	title = {Maximum likelihood estimation for dynamic factor models with missing data},
	volume = {35},
	issn = {01651889},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165188911000546},
	doi = {10.1016/j.jedc.2011.03.009},
	abstract = {This paper concerns estimating parameters in a high-dimensional dynamic factor model by the method of maximum likelihood. To accommodate missing data in the analysis, we propose a new model representation for the dynamic factor model. It allows the Kalman ﬁlter and related smoothing methods to evaluate the likelihood function and to produce optimal factor estimates in a computationally efﬁcient way when missing data is present. The implementation details of our methods for signal extraction and maximum likelihood estimation are discussed. The computational gains of the new devices are presented based on simulated data sets with varying numbers of missing entries.},
	language = {en},
	number = {8},
	urldate = {2020-09-03},
	journal = {Journal of Economic Dynamics and Control},
	author = {Jungbacker, B. and Koopman, S.J. and van der Wel, M.},
	month = aug,
	year = {2011},
	pages = {1358--1368},
	file = {Jungbacker et al. - 2011 - Maximum likelihood estimation for dynamic factor m.pdf:/Users/magnusmunch/Zotero/storage/3NVSI54S/Jungbacker et al. - 2011 - Maximum likelihood estimation for dynamic factor m.pdf:application/pdf}
}

@article{lawley_factor_2020,
	title = {Factor {Analysis} as a {Statistical} {Method}},
	language = {en},
	author = {Lawley, D N and Maxwell, A E},
	year = {2020},
	pages = {22},
	file = {Lawley and Maxwell - 2020 - Factor Analysis as a Statistical Method.pdf:/Users/magnusmunch/Zotero/storage/G7GKPTS5/Lawley and Maxwell - 2020 - Factor Analysis as a Statistical Method.pdf:application/pdf}
}

@article{jackman_introduction_nodate,
	title = {An {Introduction} to {Factor} {Analysis}},
	language = {en},
	author = {Jackman, Simon},
	pages = {12},
	file = {Jackman - An Introduction to Factor Analysis.pdf:/Users/magnusmunch/Zotero/storage/M6N6EW53/Jackman - An Introduction to Factor Analysis.pdf:application/pdf}
}

@phdthesis{wang_factor_2016,
	title = {{FACTOR} {ANALYSIS} {FOR} {HIGH}-{DIMENSIONAL} {DATA}},
	language = {en},
	school = {stanford university},
	author = {Wang, Jingshu},
	month = jul,
	year = {2016},
	file = {Wang - FACTOR ANALYSIS FOR HIGH-DIMENSIONAL DATA.pdf:/Users/magnusmunch/Zotero/storage/6YA8TVZI/Wang - FACTOR ANALYSIS FOR HIGH-DIMENSIONAL DATA.pdf:application/pdf}
}

@article{jorseskog_contributions_1967,
	title = {Some contributions to maximum likelihood factor analysis},
	volume = {32},
	language = {en},
	number = {4},
	journal = {Psychometrika},
	author = {Jörseskog, K. G.},
	month = dec,
	year = {1967},
	pages = {40},
	file = {Some contributions to maximum likelihood factor an.pdf:/Users/magnusmunch/Zotero/storage/F28KI47H/Some contributions to maximum likelihood factor an.pdf:application/pdf}
}

@article{khamaru_computation_2018,
	title = {Computation of the {Maximum} {Likelihood} estimator in low-rank {Factor} {Analysis}},
	url = {http://arxiv.org/abs/1801.05935},
	abstract = {Factor analysis, a classical multivariate statistical technique is popularly used as a fundamental tool for dimensionality reduction in statistics, econometrics and data science. Estimation is often carried out via the Maximum Likelihood (ML) principle, which seeks to maximize the likelihood under the assumption that the positive deﬁnite covariance matrix can be decomposed as the sum of a low rank positive semideﬁnite matrix and a diagonal matrix with nonnegative entries. This leads to a challenging rank constrained nonconvex optimization problem. We reformulate the low rank ML Factor Analysis problem as a nonlinear nonsmooth semideﬁnite optimization problem, study various structural properties of this reformulation and propose fast and scalable algorithms based on diﬀerence of convex (DC) optimization. Our approach has computational guarantees, gracefully scales to large problems, is applicable to situations where the sample covariance matrix is rank deﬁcient and adapts to variants of the ML problem with additional constraints on the problem parameters. Our numerical experiments demonstrate the signiﬁcant usefulness of our approach over existing state-of-the-art approaches.},
	language = {en},
	urldate = {2020-09-07},
	journal = {arXiv:1801.05935 [math, stat]},
	author = {Khamaru, Koulik and Mazumder, Rahul},
	month = jan,
	year = {2018},
	note = {arXiv: 1801.05935},
	keywords = {Statistics - Computation, Statistics - Machine Learning, Mathematics - Optimization and Control},
	file = {Khamaru and Mazumder - 2018 - Computation of the Maximum Likelihood estimator in.pdf:/Users/magnusmunch/Zotero/storage/P74IU7XZ/Khamaru and Mazumder - 2018 - Computation of the Maximum Likelihood estimator in.pdf:application/pdf}
}

@article{peeters_stable_2019,
	title = {Stable prediction with radiomics data},
	url = {http://arxiv.org/abs/1903.11696},
	abstract = {Motivation: Radiomics refers to the high-throughput mining of quantitative features from radiographic images. It is a promising ﬁeld in that it may provide a non-invasive solution for screening and classiﬁcation. Standard machine learning classiﬁcation and feature selection techniques, however, tend to display inferior performance in terms of (the stability of) predictive performance. This is due to the heavy multicollinearity present in radiomic data. We set out to provide an easy-to-use approach that deals with this problem.},
	language = {en},
	urldate = {2020-09-08},
	journal = {arXiv:1903.11696 [cs, eess, q-bio, stat]},
	author = {Peeters, Carel F. W. and Übelhör, Caroline and Mes, Steven W. and Martens, Roland and Koopman, Thomas and de Graaf, Pim and van Velden, Floris H. P. and Boellaard, Ronald and Castelijns, Jonas A. and Beest, Dennis E. te and Heymans, Martijn W. and van de Wiel, Mark A.},
	month = mar,
	year = {2019},
	note = {arXiv: 1903.11696},
	keywords = {Statistics - Machine Learning, Statistics - Methodology, Statistics - Applications, Computer Science - Machine Learning, Electrical Engineering and Systems Science - Image and Video Processing, Quantitative Biology - Quantitative Methods},
	file = {Peeters et al. - 2019 - Stable prediction with radiomics data.pdf:/Users/magnusmunch/Zotero/storage/M36JCXR8/Peeters et al. - 2019 - Stable prediction with radiomics data.pdf:application/pdf}
}

@article{niati_inverse_2019,
	title = {Inverse of sum of {Kronecker} products as a sum of {Kronecker} products},
	volume = {23},
	issn = {1080-5370, 1521-1886},
	url = {http://link.springer.com/10.1007/s10291-018-0789-8},
	doi = {10.1007/s10291-018-0789-8},
	abstract = {In the context of processing global navigation satellite system (GNSS) data by least squares adjustment, one may encounter a mathematical problem when inverting a sum of two Kronecker products. As a solution of this problem, we propose to invert this sum in the form of another sum of two Kronecker products. We present and demonstrate two mathematical formulas that enable us to achieve this task. We conclude from the demonstration that there is one condition for each formula to be checked before applying this proposed matrix inversion technique. In fact, these conditions restrict greatly the application of the formulas from being more general to the inversion problems of this kind. However, when applicable, the formulas obviously save computations in general and are very useful for large and fully populated matrices. In addition, this proposed matrix inversion technique shows several benefits when used in the processing of a single baseline with multi-frequency GNSS signals. These benefits are summarized in the following. First, the fully populated variance–covariance matrix of observations is easily inverted. Second, the computation of the normal matrix becomes easier as well since the blocks in both the design and weight matrix are all written in the form of Kronecker products. Third, this proposed matrix inversion technique contributes greatly to the computation of the variance–covariance matrix of estimates.},
	language = {en},
	number = {1},
	urldate = {2020-09-28},
	journal = {GPS Solutions},
	author = {Niati, Abdelhalim},
	month = jan,
	year = {2019},
	file = {Niati - 2019 - Inverse of sum of Kronecker products as a sum of K.pdf:/Users/magnusmunch/Zotero/storage/N5A3GVX8/Niati - 2019 - Inverse of sum of Kronecker products as a sum of K.pdf:application/pdf}
}

@article{van_de_wiel_fast_2020,
	title = {Fast cross-validation for multi-penalty ridge regression},
	url = {http://arxiv.org/abs/2005.09301},
	abstract = {Prediction based on multiple high-dimensional data types needs to account for the potentially strong differences in predictive signal. Ridge regression is a simple, yet versatile and interpretable model for high-dimensional data that has challenged the predictive performance of many more complex models and learners, in particular in dense settings. Moreover, it allows using a specific penalty per data type to account for differences between those. Then, the largest challenge for multi-penalty ridge is to optimize these penalties efficiently in a cross-validation (CV) setting, in particular for GLM and Cox ridge regression, which require an additional loop for fitting the model by iterative weighted least squares (IWLS). Our main contribution is a computationally very efficient formula for the multi-penalty, sample-weighted hat-matrix, as used in the IWLS algorithm. As a result, nearly all computations are in the low-dimensional sample space. We show that our approach is several orders of magnitude faster than more naive ones. We developed a very flexible framework that includes prediction of several types of response, allows for unpenalized covariates, can optimize several performance criteria and implements repeated CV. Moreover, extensions to pair data types and to allow a preferential order of data types are included and illustrated on several cancer genomics survival prediction problems. The corresponding R-package, multiridge, serves as a versatile standalone tool, but also as a fast benchmark for other more complex models and multi-view learners.},
	language = {en},
	urldate = {2020-10-12},
	journal = {arXiv:2005.09301 [stat]},
	author = {van de Wiel, Mark A. and van Nee, Mirrelijn M. and Rauschenberger, Armin},
	month = may,
	year = {2020},
	note = {arXiv: 2005.09301},
	keywords = {Statistics - Computation, Statistics - Methodology},
	file = {van de Wiel et al. - 2020 - Fast cross-validation for multi-penalty ridge regr.pdf:/Users/magnusmunch/Zotero/storage/IY6LR65G/van de Wiel et al. - 2020 - Fast cross-validation for multi-penalty ridge regr.pdf:application/pdf}
}

@article{moran_bayesian_2020,
	title = {Bayesian {Hierarchical} {Factor} {Regression} {Models} to {Infer} {Cause} of {Death} {From} {Verbal} {Autopsy} {Data}},
	url = {http://arxiv.org/abs/1908.07632},
	abstract = {In low-resource settings where vital registration of death is not routine it is often of critical interest to determine and study the cause of death (COD) for individuals and the cause-speciﬁc mortality fraction (CSMF) for populations. Post-mortem autopsies, considered the gold standard for COD assignment, are often diﬃcult or impossible to implement due to deaths occurring outside the hospital, expense, and/or cultural norms. For this reason, Verbal Autopsies (VAs) are commonly conducted, consisting of a questionnaire administered to next of kin recording demographic information, known medical conditions, symptoms, and other factors for the decedent. This article proposes a novel class of hierarchical factor regression models that avoid restrictive assumptions of standard methods, allow both the mean and covariance to vary with COD category, and can include covariate information on the decedent, region, or events surrounding death. Taking a Bayesian approach to inference, this work develops an MCMC algorithm and validates the FActor Regression for Verbal Autopsy (FARVA) model in simulation experiments. An application of FARVA to real VA data shows improved goodness-of-ﬁt and better predictive performance in inferring COD and CSMF over competing methods. Code and a user manual are made available at https://github.com/kelrenmor/farva.},
	language = {en},
	urldate = {2020-10-20},
	journal = {arXiv:1908.07632 [stat]},
	author = {Moran, Kelly R. and Turner, Elizabeth L. and Dunson, David and Herring, Amy H.},
	month = oct,
	year = {2020},
	note = {arXiv: 1908.07632},
	keywords = {Statistics - Applications},
	file = {Moran et al. - 2020 - Bayesian Hierarchical Factor Regression Models to .pdf:/Users/magnusmunch/Zotero/storage/4SD36ITP/Moran et al. - 2020 - Bayesian Hierarchical Factor Regression Models to .pdf:application/pdf}
}

@article{ogasawara_negative_1999,
	title = {Negative {Binomial} {Factor} {Analysis}},
	volume = {26},
	issn = {0385-7417, 1349-6964},
	url = {http://link.springer.com/10.2333/bhmk.26.235},
	doi = {10.2333/bhmk.26.235},
	language = {en},
	number = {2},
	urldate = {2020-10-20},
	journal = {Behaviormetrika},
	author = {Ogasawara, Haruhiko},
	month = jul,
	year = {1999},
	pages = {235--250},
	file = {Ogasawara - 1999 - Negative Binomial Factor Analysis.pdf:/Users/magnusmunch/Zotero/storage/UJSLYQ7E/Ogasawara - 1999 - Negative Binomial Factor Analysis.pdf:application/pdf}
}

@article{van_deun_obtaining_2018,
	title = {Obtaining insights from high-dimensional data: sparse principal covariates regression},
	volume = {19},
	issn = {1471-2105},
	shorttitle = {Obtaining insights from high-dimensional data},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-018-2114-5},
	doi = {10.1186/s12859-018-2114-5},
	abstract = {Background: Data analysis methods are usually subdivided in two distinct classes: There are methods for prediction and there are methods for exploration. In practice, however, there often is a need to learn from the data in both ways. For example, when predicting the antibody titers a few weeks after vaccination on the basis of genomewide mRNA transcription rates, also mechanistic insights about the effect of vaccinations on the immune system are sought. Principal covariates regression (PCovR) is a method that combines both purposes. Yet, it misses insightful representations of the data as these include all the variables.
Results: Here, we propose a sparse extension of principal covariates regression such that the resulting solutions are based on an automatically selected subset of the variables. Our method is shown to outperform competing methods like sparse principal components regression and sparse partial least squares in a simulation study. Furthermore good performance of the method is illustrated on publicly available data including antibody titers and genomewide transcription rates for subjects vaccinated against the flu: the selected genes by sparse PCovR are higly enriched for immune related terms and the method predicts the titers for an independent test sample well. In comparison, no significantly enriched terms were found for the genes selected by sparse partial least squares and out-of-sample prediction was worse.
Conclusions: Sparse principal covariates regression is a promising and competitive tool for obtaining insights from high-dimensional data. Availability: The source code implementing our proposed method is available from GitHub, together with all scripts used to extract, pre-process, analyze, and post-process the data: https://github.com/katrijnvandeun/SPCovR.},
	language = {en},
	number = {1},
	urldate = {2020-11-04},
	journal = {BMC Bioinformatics},
	author = {Van Deun, Katrijn and Crompvoets, Elise A. V. and Ceulemans, Eva},
	month = dec,
	year = {2018},
	pages = {Article 104},
	file = {Van Deun et al. - 2018 - Obtaining insights from high-dimensional data spa.pdf:/Users/magnusmunch/Zotero/storage/KDEHEJVH/Van Deun et al. - 2018 - Obtaining insights from high-dimensional data spa.pdf:application/pdf}
}

@article{perrakis_scalable_2020,
	title = {Scalable {Bayesian} {Regression} in {High} {Dimensions} {With} {Multiple} {Data} {Sources}},
	volume = {29},
	issn = {1061-8600, 1537-2715},
	url = {https://www.tandfonline.com/doi/full/10.1080/10618600.2019.1624294},
	doi = {10.1080/10618600.2019.1624294},
	abstract = {Applications of high-dimensional regression often involve multiple sources or types of covariates. We propose methodology for this setting, emphasizing the “wide data” regime with large total dimensionality p and sample size n p. We focus on a flexible ridge-type prior with shrinkage levels that are specific to each data type or source and that are set automatically by empirical Bayes. All estimation, including setting of shrinkage levels, is formulated mainly in terms of inner product matrices of size n × n. This renders computation efficient in the wide data regime and allows scaling to problems with millions of features. Furthermore, the proposed procedures are free of user-set tuning parameters. We show how sparsity can be achieved by post-processing of the Bayesian output via constrained minimization of a certain Kullback–Leibler divergence. This yields sparse solutions with adaptive, source-specific shrinkage, including a closedform variant that scales to very large p. We present empirical results from a simulation study based on real data and a case study in Alzheimer’s disease involving millions of features and multiple data sources. Supplementary materials for this article are available online.},
	language = {en},
	number = {1},
	urldate = {2020-11-06},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Perrakis, Konstantinos and Mukherjee, Sach and {The Alzheimer’s Disease Neuroimaging Initiative}},
	month = jan,
	year = {2020},
	pages = {28--39},
	file = {Perrakis et al. - 2020 - Scalable Bayesian Regression in High Dimensions Wi.pdf:/Users/magnusmunch/Zotero/storage/BVVQZXFA/Perrakis et al. - 2020 - Scalable Bayesian Regression in High Dimensions Wi.pdf:application/pdf}
}

@article{nakaya_systems_2011,
	title = {Systems biology of vaccination for seasonal influenza in humans},
	volume = {12},
	issn = {1529-2908, 1529-2916},
	url = {http://www.nature.com/articles/ni.2067},
	doi = {10.1038/ni.2067},
	language = {en},
	number = {8},
	urldate = {2020-11-16},
	journal = {Nature Immunology},
	author = {Nakaya, Helder I and Wrammert, Jens and Lee, Eva K and Racioppi, Luigi and Marie-Kunze, Stephanie and Haining, W Nicholas and Means, Anthony R and Kasturi, Sudhir P and Khan, Nooruddin and Li, Gui-Mei and McCausland, Megan and Kanchan, Vibhu and Kokko, Kenneth E and Li, Shuzhao and Elbein, Rivka and Mehta, Aneesh K and Aderem, Alan and Subbarao, Kanta and Ahmed, Rafi and Pulendran, Bali},
	month = aug,
	year = {2011},
	pages = {786--795},
	file = {Nakaya et al. - 2011 - Systems biology of vaccination for seasonal influe.pdf:/Users/magnusmunch/Zotero/storage/2BU6DXG5/Nakaya et al. - 2011 - Systems biology of vaccination for seasonal influe.pdf:application/pdf}
}

@article{barrett_ncbi_2012,
	title = {{NCBI} {GEO}: archive for functional genomics data sets—update},
	volume = {41},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{NCBI} {GEO}},
	url = {http://academic.oup.com/nar/article/41/D1/D991/1067995/NCBI-GEO-archive-for-functional-genomics-data},
	doi = {10.1093/nar/gks1193},
	abstract = {The Gene Expression Omnibus (GEO, http://www. ncbi.nlm.nih.gov/geo/) is an international public repository for high-throughput microarray and next-generation sequence functional genomic data sets submitted by the research community. The resource supports archiving of raw data, processed data and metadata which are indexed, cross-linked and searchable. All data are freely available for download in a variety of formats. GEO also provides several web-based tools and strategies to assist users to query, analyse and visualize data. This article reports current status and recent database developments, including the release of GEO2R, an R-based web application that helps users analyse GEO data.},
	language = {en},
	number = {D1},
	urldate = {2020-11-16},
	journal = {Nucleic Acids Research},
	author = {Barrett, Tanya and Wilhite, Stephen E. and Ledoux, Pierre and Evangelista, Carlos and Kim, Irene F. and Tomashevsky, Maxim and Marshall, Kimberly A. and Phillippy, Katherine H. and Sherman, Patti M. and Holko, Michelle and Yefanov, Andrey and Lee, Hyeseung and Zhang, Naigong and Robertson, Cynthia L. and Serova, Nadezhda and Davis, Sean and Soboleva, Alexandra},
	month = nov,
	year = {2012},
	pages = {D991--D995},
	file = {Barrett et al. - 2012 - NCBI GEO archive for functional genomics data set.pdf:/Users/magnusmunch/Zotero/storage/GYNUHBTJ/Barrett et al. - 2012 - NCBI GEO archive for functional genomics data set.pdf:application/pdf}
}

@article{irizarry_exploration_2003,
	title = {Exploration, normalization, and summaries of high density oligonucleotide array probe level data},
	volume = {4},
	issn = {14654644, 14684357},
	url = {https://academic.oup.com/biostatistics/article-lookup/doi/10.1093/biostatistics/4.2.249},
	doi = {10.1093/biostatistics/4.2.249},
	abstract = {In this paper we report exploratory analyses of high-density oligonucleotide array data from the Affymetrix GeneChip R system with the objective of improving upon currently used measures of gene expression. Our analyses make use of three data sets: a small experimental study consisting of ﬁve MGU74A mouse GeneChip R arrays, part of the data from an extensive spike-in study conducted by Gene Logic and Wyeth’s Genetics Institute involving 95 HG-U95A human GeneChip R arrays; and part of a dilution study conducted by Gene Logic involving 75 HG-U95A GeneChip R arrays. We display some familiar features of the perfect match and mismatch probe (P M and M M) values of these data, and examine the variance–mean relationship with probe-level data from probes believed to be defective, and so delivering noise only. We explain why we need to normalize the arrays to one another using probe level intensities. We then examine the behavior of the P M and M M using spike-in data and assess three commonly used summary measures: Affymetrix’s (i) average difference (AvDiff) and (ii) MAS 5.0 signal, and (iii) the Li and Wong multiplicative model-based expression index (MBEI). The exploratory data analyses of the probe level data motivate a new summary measure that is a robust multiarray average (RMA) of background-adjusted, normalized, and log-transformed P M values. We evaluate the four expression summary measures using the dilution study data, assessing their behavior in terms of bias, variance and (for MBEI and RMA) model ﬁt. Finally, we evaluate the algorithms in terms of their ability to detect known levels of differential expression using the spike-in data. We conclude that there is no obvious downside to using RMA and attaching a standard error (SE) to this quantity using a linear model which removes probe-speciﬁc afﬁnities.},
	language = {en},
	number = {2},
	urldate = {2020-11-16},
	journal = {Biostatistics},
	author = {Irizarry, R. A.},
	month = apr,
	year = {2003},
	pages = {249--264},
	file = {Irizarry - 2003 - Exploration, normalization, and summaries of high .pdf:/Users/magnusmunch/Zotero/storage/TZTUTV9Y/Irizarry - 2003 - Exploration, normalization, and summaries of high .pdf:application/pdf}
}

@article{hayashi_bayesian_2006,
	title = {Bayesian {Factor} {Analysis} {When} {Only} a {Sample} {Covariance} {Matrix} {Is} {Available}},
	volume = {66},
	issn = {0013-1644, 1552-3888},
	url = {http://journals.sagepub.com/doi/10.1177/0013164405278583},
	doi = {10.1177/0013164405278583},
	language = {en},
	number = {2},
	urldate = {2020-11-19},
	journal = {Educational and Psychological Measurement},
	author = {Hayashi, Kentaro and Arav, Marina},
	month = apr,
	year = {2006},
	pages = {272--284},
	file = {Hayashi and Arav - 2006 - Bayesian Factor Analysis When Only a Sample Covari.pdf:/Users/magnusmunch/Zotero/storage/TW5UR7AB/Hayashi and Arav - 2006 - Bayesian Factor Analysis When Only a Sample Covari.pdf:application/pdf}
}

@article{ansari_bayesian_2000,
	title = {Bayesian factor analysis for multilevel binary observations},
	volume = {65},
	issn = {0033-3123, 1860-0980},
	url = {http://link.springer.com/10.1007/BF02296339},
	doi = {10.1007/BF02296339},
	abstract = {Multilevel covariance structure models have become increasingly popular in the psychometric literature in the past few years to account for population heterogeneity and complex study designs. We develop practical simulation based procedures for Bayesian inference of multilevel binary factor analysis models. We illustrate how Markov Chain Monte Carlo procedures such as Gibbs sampling and MetropolisHastings methods can be used to perform Bayesian inference, model checking and model comparison without the need for multidimensional numerical integration. We illustrate the proposed estimation methods using three simulation studies and an application involving student's achievement results in different areas of mathematics.},
	language = {en},
	number = {4},
	urldate = {2020-11-24},
	journal = {Psychometrika},
	author = {Ansari, Asim and Jedidi, Kamel},
	month = dec,
	year = {2000},
	pages = {475--496},
	file = {Ansari and Jedidi - 2000 - Bayesian factor analysis for multilevel binary obs.pdf:/Users/magnusmunch/Zotero/storage/7ARVNPRM/Ansari and Jedidi - 2000 - Bayesian factor analysis for multilevel binary obs.pdf:application/pdf}
}

@article{arismendi_multivariate_2017,
	title = {Multivariate elliptical truncated moments},
	volume = {157},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X17301239},
	doi = {10.1016/j.jmva.2017.02.011},
	abstract = {In this study, we derived analytic expressions for the elliptical truncated moment generating function (MGF), the zeroth-, ﬁrst-, and second-order moments of quadratic forms of the multivariate normal, Student’s t, and generalised hyperbolic distributions. The resulting formulae were tested in a numerical application to calculate an analytic expression of the expected shortfall of quadratic portfolios with the beneﬁt that moment based sensitivity measures can be derived from the analytic expression. The convergence rate of the analytic expression is fast – one iteration – for small closed integration domains, and slower for open integration domains when compared to the Monte Carlo integration method. The analytic formulae provide a theoretical framework for calculations in robust estimation, robust regression, outlier detection, design of experiments, and stochastic extensions of deterministic elliptical curves results.},
	language = {en},
	urldate = {2020-11-30},
	journal = {Journal of Multivariate Analysis},
	author = {Arismendi, Juan C. and Broda, Simon},
	month = may,
	year = {2017},
	pages = {29--44},
	file = {Arismendi and Broda - 2017 - Multivariate elliptical truncated moments.pdf:/Users/magnusmunch/Zotero/storage/Q7QDYIQB/Arismendi and Broda - 2017 - Multivariate elliptical truncated moments.pdf:application/pdf}
}

@article{ruben_probability_1962,
	title = {Probability {Content} of {Regions} {Under} {Spherical} {Normal} {Distributions}, {IV}: {The} {Distribution} of {Homogeneous} and {Non}-{Homogeneous} {Quadratic} {Functions} of {Normal} {Variables}},
	volume = {33},
	number = {2},
	journal = {Annals of Mathematical Statistics},
	author = {Ruben, Harold},
	year = {1962},
	pages = {542--570},
	file = {euclid.aoms.1177704580.pdf:/Users/magnusmunch/Zotero/storage/T67U6EW4/euclid.aoms.1177704580.pdf:application/pdf}
}

@article{sheil_algorithm_1977,
	title = {Algorithm {AS} 106: {The} {Distribution} of {Non}-{Negative} {Quadratic} {Forms} in {Normal} {Variables}},
	volume = {26},
	issn = {00359254},
	shorttitle = {Algorithm {AS} 106},
	url = {https://www.jstor.org/stable/2346884?origin=crossref},
	doi = {10.2307/2346884},
	language = {en},
	number = {1},
	urldate = {2020-12-01},
	journal = {Applied Statistics},
	author = {Sheil, J. and O'Muircheartaigh, I.},
	year = {1977},
	pages = {92},
	file = {Sheil and O'Muircheartaigh - 1977 - Algorithm AS 106 The Distribution of Non-Negative.pdf:/Users/magnusmunch/Zotero/storage/CLCK3WNY/Sheil and O'Muircheartaigh - 1977 - Algorithm AS 106 The Distribution of Non-Negative.pdf:application/pdf}
}

@phdthesis{goeman_statistical_2006,
	address = {Leiden, The Netherlands},
	title = {Statistical methods for microarray data: pathway analysis, prediction methods and visualization tools},
	shorttitle = {Statistical methods for microarray data},
	language = {nl},
	school = {Leiden University},
	author = {Goeman, Jelle Jurjen},
	year = {2006},
	note = {ISBN: 9789090203720
OCLC: 71674633},
	file = {Goeman - 2006 - Statistical methods for microarray data pathway a.pdf:/Users/magnusmunch/Zotero/storage/N7YESVHW/Goeman - 2006 - Statistical methods for microarray data pathway a.pdf:application/pdf}
}

@article{zwick_comparison_1986,
	title = {Comparison of {Five} {Rules} for {Determining} the {Number} of {Components} to {Retain}},
	volume = {99},
	language = {en},
	number = {3},
	journal = {Psychological Bulletin},
	author = {Zwick, William R and Velicer, Wayne F},
	year = {1986},
	pages = {432--442},
	file = {Zwick and Velicer - Comparison of Five Rules for Determining the Numbe.pdf:/Users/magnusmunch/Zotero/storage/9RQZB45B/Zwick and Velicer - Comparison of Five Rules for Determining the Numbe.pdf:application/pdf}
}

@misc{peeters_fmradio_2019,
	title = {{FMradio}: {Factor} {Modelling} for {Radiomics} {Data}},
	url = {https://CRAN.R-project.org/package=FMradio},
	author = {Peeters, Carel F.W. and Übelhör, Caroline and Kunzmann, Kevin},
	year = {2019}
}

@article{auerswald_how_2019,
	title = {How to determine the number of factors to retain in exploratory factor analysis: {A} comparison of extraction methods under realistic conditions.},
	volume = {24},
	issn = {1939-1463, 1082-989X},
	shorttitle = {How to determine the number of factors to retain in exploratory factor analysis},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/met0000200},
	doi = {10.1037/met0000200},
	abstract = {Exploratory factor analyses are commonly used to determine the underlying factors of multiple observed variables. Many criteria have been suggested to determine how many factors should be retained. In this study, we present an extensive Monte Carlo simulation to investigate the performance of extraction criteria under varying sample sizes, numbers of indicators per factor, loading magnitudes, underlying multivariate distributions of observed variables, as well as how the performance of the extraction criteria are influenced by the presence of cross-loadings and minor factors for unidimensional, orthogonal, and correlated factor models. We compared several variants of traditional parallel analysis (PA), the Kaiser-Guttman Criterion, and sequential ␹2 model tests (SMT) with 4 recently suggested methods: revised PA, comparison data (CD), the Hull method, and the Empirical Kaiser Criterion (EKC). No single extraction criterion performed best for every factor model. In unidimensional and orthogonal models, traditional PA, EKC, and Hull consistently displayed high hit rates even in small samples. Models with correlated factors were more challenging, where CD and SMT outperformed other methods, especially for shorter scales. Whereas the presence of cross-loadings generally increased accuracy, non-normality had virtually no effect on most criteria. We suggest researchers use a combination of SMT and either Hull, the EKC, or traditional PA, because the number of factors was almost always correctly retrieved if those methods converged. When the results of this combination rule are inconclusive, traditional PA, CD, and the EKC performed comparatively well. However, disagreement also suggests that factors will be harder to detect, increasing sample size requirements to N Ն 500.},
	language = {en},
	number = {4},
	urldate = {2021-03-04},
	journal = {Psychological Methods},
	author = {Auerswald, Max and Moshagen, Morten},
	month = aug,
	year = {2019},
	pages = {468--491},
	file = {Auerswald and Moshagen - 2019 - How to determine the number of factors to retain i.pdf:/Users/magnusmunch/Zotero/storage/GJ93RQ9D/Auerswald and Moshagen - 2019 - How to determine the number of factors to retain i.pdf:application/pdf}
}

@article{mes_outcome_2020,
	title = {Outcome prediction of head and neck squamous cell carcinoma by {MRI} radiomic signatures},
	volume = {30},
	issn = {0938-7994, 1432-1084},
	url = {http://link.springer.com/10.1007/s00330-020-06962-y},
	doi = {10.1007/s00330-020-06962-y},
	abstract = {Objectives Head and neck squamous cell carcinoma (HNSCC) shows a remarkable heterogeneity between tumors, which may be captured by a variety of quantitative features extracted from diagnostic images, termed radiomics. The aim of this study was to develop and validate MRI-based radiomic prognostic models in oral and oropharyngeal cancer. Materials and Methods Native T1-weighted images of four independent, retrospective (2005–2013), patient cohorts (n = 102, n = 76, n = 89, and n = 56) were used to delineate primary tumors, and to extract 545 quantitative features from. Subsequently, redundancy filtering and factor analysis were performed to handle collinearity in the data. Next, radiomic prognostic models were trained and validated to predict overall survival (OS) and relapse-free survival (RFS). Radiomic features were compared to and combined with prognostic models based on standard clinical parameters. Performance was assessed by integrated area under the curve (iAUC).
Results In oral cancer, the radiomic model showed an iAUC of 0.69 (OS) and 0.70 (RFS) in the validation cohort, whereas the iAUC in the oropharyngeal cancer validation cohort was 0.71 (OS) and 0.74 (RFS). By integration of radiomic and clinical variables, the most accurate models were defined (iAUC oral cavity, 0.72 (OS) and 0.74 (RFS); iAUC oropharynx, 0.81 (OS) and 0.78 (RFS)), and these combined models outperformed prognostic models based on standard clinical variables only (p {\textless} 0.001).
Conclusions MRI radiomics is feasible in HNSCC despite the known variability in MRI vendors and acquisition protocols, and radiomic features added information to prognostic models based on clinical parameters.},
	language = {en},
	number = {11},
	urldate = {2021-03-05},
	journal = {European Radiology},
	author = {Mes, Steven W. and van Velden, Floris H. P. and Peltenburg, Boris and Peeters, Carel F. W. and te Beest, Dennis E. and van de Wiel, Mark A. and Mekke, Joost and Mulder, Doriene C. and Martens, Roland M. and Castelijns, Jonas A. and Pameijer, Frank A. and de Bree, Remco and Boellaard, Ronald and Leemans, C. René and Brakenhoff, Ruud H. and de Graaf, Pim},
	month = nov,
	year = {2020},
	pages = {6311--6321},
	file = {Mes et al. - 2020 - Outcome prediction of head and neck squamous cell .pdf:/Users/magnusmunch/Zotero/storage/M3GV54F9/Mes et al. - 2020 - Outcome prediction of head and neck squamous cell .pdf:application/pdf}
}
