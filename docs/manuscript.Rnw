% set options
<<settings, include=FALSE, echo=FALSE>>=
opts_knit$set(root.dir="..", base.dir="../figs")
opts_chunk$set(fig.align='center', fig.path="../figs/", 
               echo=FALSE, cache=FALSE, message=FALSE, fig.pos='!ht')
knit_hooks$set(document=function(x) {
  sub('\\usepackage[]{color}', '\\usepackage{xcolor}', x, fixed=TRUE)})
@

% read figure code
<<figures, include=FALSE>>=
read_chunk("code/figures.R")
@

\documentclass[a4paper,hidelinks]{article}
% \usepackage{url,bm,algorithm,algpseudocode,mathtools,bbm,pgfplotstable,bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,bbm,algorithm,algpseudocode,pgfplotstable,threeparttable, booktabs,mathtools,dsfont}
\usepackage{bm,amsmath,amssymb,amsthm,amsfonts,graphics,graphicx,epsfig,
rotating,caption,subcaption,natbib,appendix,titlesec,multicol,hyperref,verbatim,
bbm,algorithm,algpseudocode,pgfplotstable,threeparttable,booktabs,mathtools,
dsfont,parskip,xr,enumerate,multirow,tikz,arydshln}
\usetikzlibrary{matrix, fit, backgrounds}
\externaldocument[sm-]{supplement}

% vectors and matrices
\newcommand{\bbeta}{\bm{\beta}}
\newcommand{\btau}{\bm{\tau}}
\newcommand{\bsigma}{\bm{\sigma}}
\newcommand{\balpha}{\bm{\alpha}}
\newcommand{\bomega}{\bm{\omega}}
\newcommand{\bOmega}{\bm{\Omega}}
\newcommand{\bSigma}{\bm{\Sigma}}
\newcommand{\bkappa}{\bm{\kappa}}
\newcommand{\btheta}{\bm{\theta}}
\newcommand{\bmu}{\bm{\mu}}
\newcommand{\bpsi}{\bm{\psi}}
\newcommand{\bchi}{\bm{\chi}}
\newcommand{\bgamma}{\bm{\gamma}}
\newcommand{\bGamma}{\bm{\Gamma}}
\newcommand{\blambda}{\bm{\lambda}}
\newcommand{\bLambda}{\bm{\Lambda}}
\newcommand{\bPsi}{\bm{\Psi}}
\newcommand{\bphi}{\bm{\phi}}
\newcommand{\Y}{\mathbf{Y}}
\newcommand{\y}{\mathbf{y}}
\newcommand{\X}{\mathbf{X}}
\newcommand{\x}{\mathbf{x}}
\newcommand{\I}{\mathbf{I}}
\newcommand{\T}{\mathbf{T}}
\newcommand{\Z}{\mathbf{Z}}
\newcommand{\W}{\mathbf{W}}
\renewcommand{\O}{\mathbf{O}}
\newcommand{\B}{\mathbf{B}}
\renewcommand{\H}{\mathbf{H}}
\newcommand{\U}{\mathbf{U}}
\newcommand{\D}{\mathbf{D}}
\newcommand{\Vm}{\mathbf{V}}
\newcommand{\rv}{\mathbf{r}}
\newcommand{\A}{\mathbf{A}}
\newcommand{\Q}{\mathbf{Q}}
\newcommand{\Sm}{\mathbf{S}}
\newcommand{\0}{\bm{0}}
\newcommand{\tx}{\tilde{\mathbf{x}}}
\newcommand{\hy}{\hat{y}}
\newcommand{\tm}{\tilde{m}}
\newcommand{\tkappa}{\tilde{\kappa}}
\newcommand{\m}{\mathbf{m}}
\newcommand{\C}{\mathbf{C}}
\renewcommand{\v}{\mathbf{v}}
\newcommand{\g}{\mathbf{g}}
\newcommand{\s}{\mathbf{s}}
\renewcommand{\c}{\mathbf{c}}
\newcommand{\ones}{\mathbf{1}}
\newcommand{\R}{\mathbf{R}}
\newcommand{\rvec}{\mathbf{r}}
\newcommand{\Lm}{\mathbf{L}}

% functions and operators
\renewcommand{\L}{\mathcal{L}}
\newcommand{\G}{\mathcal{G}}
\renewcommand{\P}{\mathcal{P}}
\newcommand{\argmax}{\text{argmax} \,}
\newcommand{\expit}{\text{expit}}
\newcommand{\erfc}{\text{erfc}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Cov}{\mathbb{C}\text{ov}}
\newcommand{\tr}{^{\text{T}}}
\newcommand{\diag}{\text{diag}}
\newcommand{\KL}{D_{\text{KL}}}
\newcommand{\trace}{\text{tr}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\appropto}{\mathrel{\vcenter{
  \offinterlineskip\halign{\hfil$##$\cr
    \propto\cr\noalign{\kern2pt}\sim\cr\noalign{\kern-2pt}}}}}
\newcommand{\card}[1]{\text{card} \left( #1 \right)}
\makeatletter
\newcommand*{\defeq}{\mathrel{\rlap{%
			\raisebox{0.3ex}{$\m@th\cdot$}}%
		\raisebox{-0.3ex}{$\m@th\cdot$}}%
	=}
\makeatother

% distributions
\newcommand{\N}{\text{N}}
\newcommand{\TG}{\text{TG}}
\newcommand{\Bi}{\text{Binom}}
\newcommand{\PG}{\text{PG}}
\newcommand{\Mu}{\text{Multi}}
\newcommand{\GIG}{\text{GIG}}
\newcommand{\IGauss}{\text{IGauss}}
\newcommand{\Un}{\text{U}}

% syntax and readability
\renewcommand{\(}{\left(}
\renewcommand{\)}{\right)}
\renewcommand{\[}{\left[}
\renewcommand{\]}{\right]}
\renewcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\makeatletter
\newcommand{\vast}{\bBigg@{3}}
\newcommand{\vastt}{\bBigg@{4}}
\newcommand{\Vast}{\bBigg@{5}}
\makeatother
\newcommand{\hl}[2]{
\begin{scope}[on background layer]
    \node [fit={#1}, fill=#2,inner sep=-1pt] {};
\end{scope}}

% settings
\pgfplotsset{compat=1.16}
\graphicspath{{../figs/}}
\setlength{\evensidemargin}{.5cm} \setlength{\oddsidemargin}{.5cm}
\setlength{\textwidth}{15cm}
\title{Semi-supervised empirical Bayes group-regularized factor regression}
\date{\today}
\author{Magnus M. M\"unch$^{1,2}$\footnote{Correspondence to: 
\href{mailto:m.munch@amsterdamumc.nl}{m.munch@amsterdamumc.nl}}, Mark A. van de 
Wiel$^{1,3}$, \\ Aad W. van der Vaart$^{2}$, and Carel F.W. Peeters$^{1,4}$}

\begin{document}

	\maketitle
	
	\noindent
	1. Department of Epidemiology \& Data Science, Amsterdam UMC, VU University, 
	PO Box 7057, 1007 MB Amsterdam, The Netherlands \\
	2. Mathematical Institute, Leiden University, Leiden, The Netherlands \\
	3. MRC Biostatistics Unit, Cambridge Institute of Public Health, Cambridge,
	United Kingdom \\
	4. Division of Mathematical \& Statistical Methods - Biometris, 
	Wageningen University \& Research, Wageningen, The Netherlands
	
	\begin{abstract}
		{
		The features in high dimensional biomedical prediction problems are often
		well described with lower dimensional manifolds. An example is
		genes that are organised in smaller functional networks. The outcome can 
		then be
		described with the factor regression model. A
		benefit of the factor model is that is allows for straightforward inclusion
		of unlabeled observations
		in the estimation of the model, i.e., semi-supervised learning.
		In addition, the high dimensional features in biomedical prediction problems
		are often well characterised. Examples are genes, for which annotation is
		available, and metabolites with $p$-values from a previous study available.
		In this paper, the extra information on the features is included in the 
		prior model for the features. 
		The extra information is weighted and included in the estimation
		through empirical Bayes, with Variational approximations to speed up the
		computation.
		The method is demonstrated in simulations and two applications.
		One application considers influenza vaccine 
		efficacy prediction based on microarray data. The second application
		predictions oral cancer metastatsis from RNAseq data.
		}
	\end{abstract}
	
	\noindent\textbf{Keywords}: Empirical Bayes; Factor regression; 
	High-dimensional data; Semi-supervised learning  
	
	\noindent\textbf{Software available from}: 
	\url{https://github.com/magnusmunch/bayesfactanal}
	
	\section{Introduction}
	In modern biomedical research, there is an interest in 
	prediction models based on large sets of omics features. Common 
	outcomes are, for example, categorical disease status, 
	time-to-event, or continuous anthropomorphic measures.
	
	In many omics studies, the number of omics features considered is large
	and may run in the tens of thousands (in, e.g., genomics). At the same time,
	the number of samples is generally low, commonly due to 
	high measurement costs, logistics, or the availability of subjects. 
	The high-dimensionality of 
	data (i.e., $p > n$) complicates model estimation.
	On the other hand, extra unlabeled omics data is often available.
	Here, `unlabeled' refers to data for which the predictor features are
	available, but not the study response/outcome.
	Unlabeled data may, for example, come from online repositories or 
	previous studies with the same set of features, but a different response.
	Inclusion of unlabeled data in prediction problems, termed semi-supervised
	learning in the machine learning community, has received plenty of attention
	\cite[see][for an introduction]{zhu_introduction_2009}.
	
	Several authors have argued that the high-dimensional feature space in omics 
	data arises from noisy observations on a lower dimensional latent space. 
	\cite{bernardo_bayesian_2003} show that gene 
	expression data from breast cancer patients are indeed well described with a
	lower dimensional (linear) latent space.
	Moreover, \cite{carvalho_high-dimensional_2008} improve prediction of 
	mutant p53 gene versus wild type in breast cancer patients with the lower
	dimensional structure of the gene expression data. 
	\cite{bernardo_bayesian_2003} and \cite{carvalho_high-dimensional_2008} use
	a Bayesian linear factor (regression) model approach to describe the latent 
	space. \cite{mes_outcome_2020} is an example of a frequentist
	latent space approach (technically a hybrid between Bayes and frequentist) to
	prediction from radiomics features.
	
	Inclusion of unlabeled data into the estimation of
	linear factor models may benefit estimation 
	\cite[]{liu_maximum_1998,banbura_maximum_2014}. Here, we extend the estimation
	of a Bayesian factor regression model to include unlabeled data to improve 
	prediction from a high-dimensional feature space. We treat the
	unlabeled data as data with a missing response and consider the full 
	likelihood approach, together with a Bayesian prior. 
	
	In addition, extra information on the features is often available. The extra
	information, termed co-data, may be a partitioning of the features, such as 
	pathway membership of the genes, or continuous information, such as $p$-values
	from a previous study. Recently, several methods have been introduced that
	use the co-data to improve prediction 
	\cite[see, e.g.,][]{van_nee_flexible_2020,munch_adaptive_2019,
	te_beest_improved_2017,van_de_wiel_better_2016}. 
	
	In the current paper, we apply the co-data approach
	\cite[more specifically, a group-adaptive empirical Bayes approach akin to 
	that in][]{munch_adaptive_2019}, together with the
	inclusion of the unlabeled data, to the
	Bayesian factor regression model. We present an extension 
	of the method to a mixed mode factor analysis, the outcome is
	binary instead of continuous. Simulations show that the approach is
	competitive or even outperforms classical approaches in some settings. 
	Applications to influenza vaccine efficacy 
	prediction and oral cancer lymph node metastasis prediction show that the
	approach has the potential to enhance predictive performance compared to
	existing methods.
	
	The rest of the paper is organised as follows: Sections \ref{sec:model} 
	and \ref{sec:estimation} describe the model and its estimation in detail. 
	The approach is demonstrated in a simulated setting in Section
	\ref{sec:simulations} and two
	real data settings in Section \ref{sec:applications}. We conclude with
	a short discussion on the pros and cons of the method in Section 
	\ref{sec:discussion}.
	
	\section{Model}\label{sec:model}
	\subsection{Observational model}
	We assume our observed $p$-dimensional feature vectors $\mathbf{x}_i$ and 
	outcomes $y_i$, $i=1, \dots n$, are standardised, such that 
	$\forall j: \sum_{i=1}^n x_{ij}=0$, 
	$\forall j: \sum_{i=1}^n x^2_{ij}=n$,
	$\sum_{i=1}^n y_{i}=0$, $\sum_{i=1}^n y_{i}^2=n$, and 
	follow the factor regression model \cite[]{liang_use_2007}:
	\begin{subequations}\label{eq:model}
	  \begin{align}
	    y | \bm{\lambda} & \sim \mathcal{N}(\bm{\beta}^{\text{T}} \bm{\lambda},
	      \sigma^2), \label{eq:linearoutcome}\\
	    \mathbf{x} | \bm{\lambda} & \sim \mathcal{N}_p(\mathbf{B}^{\text{T}} 
	      \bm{\lambda}, \bm{\Psi}) \label{eq:linearfeatures}\\
	    \bm{\lambda} & \sim \mathcal{N}_d(\mathbf{0}, \mathbf{I}_d),
	      \label{eq:linearfactors}
	  \end{align}
	\end{subequations}
	where $\bm{\lambda}$ consists of the latent factors, 
	$\bm{\Psi}=\text{diag}(\psi_j)$, $j=1\dots, p$, are the uniquenesses,
	$\sigma^2$ is the error variance, and $\mathbf{B}$ and $\bm{\beta}$ are 
	the factor loadings. The latent factor dimension $d$ is 
	assumed fixed and known. The factor model comes with some issues
	(namely, rotational invariance and factor indeterminancy). These do
	not play a major role in prediction problems, so we do not address them here.
	SM Section \ref{sm-sec:modelunidentifiability} provides some pointers into
	these issues. Model (\ref{eq:model}) implies a joint multivariate Gaussian 
	distribution for $\begin{bmatrix} \mathbf{x}^{\text{T}} & y 
	\end{bmatrix}^{\text{T}}$ (unconditional on $\bm{\lambda}$), 
	so a prediction $\tilde{y}$ from observed features
	$\tilde{\mathbf{x}}$ is obtained by taking the expectation of the
	conditional distribution of $\tilde{y}$ given $\tilde{\mathbf{x}}$ from
	model (\ref{eq:model}): 
	\begin{equation}\label{eq:prediction}
	  \mathbb{E}(\tilde{y}|\tilde{\mathbf{x}})=
	  \tilde{\mathbf{x}}^{\text{T}}
	  (\mathbf{B}^{\text{T}}\mathbf{B} + \bm{\Psi})^{-1}\mathbf{B}^{\text{T}}
	  \bm{\beta}=:\tilde{\mathbf{x}}^{\text{T}}\tilde{\bm{\beta}}.
	\end{equation}
	
	In the following, it is convenient to write $\bar{p}=p +1$,
	$\bar{\mathbf{x}} = 
	\begin{bmatrix}\mathbf{x}^{\text{T}} & y \end{bmatrix}^{\text{T}}$,
	$\bar{\mathbf{B}}=\begin{bmatrix}\mathbf{B} & 
	\bm{\beta}\end{bmatrix}$,
	$$
	\bar{\bm{\Psi}}=\begin{bmatrix} 
	\bm{\Psi} & \mathbf{0}_{p \times 1} \\
	\mathbf{0}_{1 \times p} & \sigma^2,
	\end{bmatrix}
	$$ 
	and consider the equivalent form of (\ref{eq:model}):
	\begin{subequations}\label{eq:model2}
	  \begin{align}
	    \bar{\mathbf{x}} | \bm{\lambda} & \sim \mathcal{N}_{\bar{p}}( 
	      \bar{\mathbf{B}}^{\text{T}} \bm{\lambda}, \bar{\bm{\Psi}}) \\
	    \bm{\lambda} & \sim \mathcal{N}_d(\mathbf{0}, \mathbf{I}_d).
	  \end{align}
	\end{subequations}
	
	If the outcomes $y_i$ are of sums of $N_i$ disjoint binary events with
	the shared probability of success, the linear outcome model 
	(\ref{eq:linearoutcome}) is replaced with its logistic counterpart:
	\begin{equation}\label{eq:logisticoutcome}
	y | \bm{\lambda}, \bm{\beta}, \beta_0 \sim 
	\mathcal{B}\left(N, \expit(\beta_0 + \bm{\beta}^{\text{T}}
	\bm{\lambda})\right),
	\end{equation}
	where $\mathcal{B}\left(N, \pi\right)$ denotes the binomial distribution with
	number of trials $N$ and success probability $\pi$. Note that the logistic
	model includes an intercept $\beta_0$ to accommodate unbalanced data, 
	whereas the linear model simply considers standardised data.
	Feature and factor models (\ref{eq:linearfeatures}) and 
	(\ref{eq:linearfactors}), in combination with outcome model
	(\ref{eq:logisticoutcome}) result in a mixed-mode factor model, with Gaussian 
	and binomially distributed features and outcome, respectively.
	This mixed-mode extension is detailed in Section 
	\ref{sm-sec:logistic} of the Supplementary Material (SM).
	
	\subsection{Bayesian prior model}
	In the Bayesian version of the model, the parameters $\theta := 
	\{ \bar{\mathbf{B}},\bar{\psi}_1,\dots,
	\bar{\psi}_{p+1}\}$ are endowed with conditionally conjugate prior 
	distributions:
	\begin{subequations}\label{eq:prior}
  	\begin{align}
  	  \bar{\mathbf{B}} | \bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}} & \sim 
  	    \prod_{\bar{j}=1}^{\bar{p}} \mathcal{N}_{d}(\mathbf{0}_{d}, 
  	  \bar{\psi}_{\bar{j}} \gamma_{\bar{j}} \mathbf{I}_d), \\
  	  \bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}} & \sim \prod_{\bar{j}=1}^{\bar{p}} 
  	    \Gamma^{-1}(\kappa_{\bar{j}}, \nu_{\bar{j}}),
  	\end{align}
  \end{subequations}
  where $\Gamma^{-1}(\kappa,\nu)$ denotes the inverse 
  Gamma distribution with shape $\kappa$ and scale $\nu$. 
  Note that index $\bar{j}$ and dimension $\bar{p}$  
  indicate the use of the equivalent model formulation (\ref{eq:model2}).
  In addition, we write $\bar{\psi}_{\bar{p}}:=\sigma^2$ for notational 
  convenience. Here, the variance of $\bar{\mathbf{b}}_j$ 
  (column $\bar{j}$ of $\bar{\mathbf{B}}$) scales with 
  error variance/uniquenesses
  $\bar{\psi}_{\bar{j}}$ as is common in Bayesian
  (univariate) linear models. This is mostly for computational reasons, but is
  often justified as a solution to scaling problems in multivariate 
  regression problems \cite[]{leday_gene_2017}. 
  
  In the Bayesian model a prediction $\tilde{y}$ from features 
  $\tilde{\mathbf{x}}$ is obtained by averaging over the posterior: 
	\begin{equation}\label{eq:bayesianprediction}
	  \mathbb{E}^*(\tilde{y}|\tilde{\mathbf{x}})=
	  \tilde{\mathbf{x}}^{\text{T}}
	  \mathbb{E}_{\bar{\mathbf{B}}, \bar{\bm{\Psi}} | \bar{\mathbf{x}}}
	  \[(\mathbf{B}^{\text{T}}\mathbf{B} + 
	  \bm{\Psi})^{-1} \mathbf{B}^{\text{T}}
	  \bm{\beta}\]=:\tilde{\mathbf{x}}^{\text{T}}\bm{\beta}^*.
	\end{equation}
	In practice, this expectation is hard to compute. 
	Here, we use a combination of variational Bayes for posterior computation and 
	Monte Carlo simulation for approximation of (\ref{eq:bayesianprediction}). 
	An alternative to Monte Carlo simulation is Taylor approximation, as explained 
	in Section \ref{sm-sec:posteriorexpectation} of the SM.
	
	\subsection{Additional feature structure}
	In some applications, the features naturally come partitioned into groups
	$\mathcal{G}_1, \dots, \mathcal{G}_G$. Examples of such partitions are
	distinct functional networks of genes, features with significant versus 
	features with non-significant association to the outcome in a previous study, 
	and feature groups based on prior expert knowledge of feature importance
	\cite[see, e.g.,][]{munch_adaptive_2019}.
	Figure \ref{fig:model} displays model (\ref{eq:model}) with partitioned 
	features as a Bayesian network.
	\begin{figure}[h!]
	  \centering
    \includegraphics[width=0.8\linewidth]{factor_network.pdf}
    \caption{Model (\ref{eq:model}) with paritioned features as a Bayesian 
    network, where the vertical dotted lines denote a partitioning of features 
    $X_1, \dots, X_p$ into groups $g=1, \dots, G$. Green and blue circles denote
    latent and observed variables, respectively. Note that the $\delta_j$ and
    $\epsilon$ are implicit in model (\ref{eq:model}) and omitted there for 
    brevity. Here they denote the Gaussian, centred errors. That is, we have
    $y = \bm{\beta}^{\text{T}} \bm{\lambda} + \epsilon$, 
    $\mathbf{x} = \mathbf{B}^{\text{T}} \bm{\lambda} + \bm{\delta}$,
    with $\epsilon \sim \mathcal{N}(0,\sigma^2)$ and
    $\bm{\delta} \sim \mathcal{N}_p(0,\bm{\Psi})$.}
    \label{fig:model}
  \end{figure}
  
	Straightforward inclusion of the partitioning is possible through
	(i) a factor loadings partitioning, i.e., 
	$\mathbf{B} = \begin{bmatrix}\begin{array}{c|c|c}\mathbf{B}_1 & \cdots & 
	\mathbf{B}_G\end{array}\end{bmatrix}$, or (ii) a uniquenessess partitioning,
	i.e., 
	$$
	\bm{\Psi} = \begin{bmatrix}
	  \bm{\Psi}_1 &  & \\
	  & \ddots & \\
	  & & \bm{\Psi}_G
	\end{bmatrix}.
	$$
	Prediction (\ref{eq:prediction}) shows that the effect of
	inflation of diagonal element $\psi_{j}$ or shrinkage of column 
	$\mathbf{b}_{j}$ on the induced regression coefficients 
	$\tilde{\bm{\beta}}$ and $\bm{\beta}^*$, is similar. 
	With an appropriate choice of priors for $\mathbf{B}$ and 
	$\psi_j, \dots, \psi_p$, the partitioning of the 
	features $\mathcal{G}_1, \dots, \mathcal{G}_G$ is included in the prior model.
	Here, We pursue option (i) and model the feature structure through the 
	$\mathbf{B}$ prior, by considering groupwise constant 
	(up to a uniquenessess scaling) prior variances, i.e.,
	$\forall j \in \mathcal{G}_g: \gamma_j=\gamma_g$. The groupwise constant prior
	variance shrinks feature effects in the same group similarly. A small
	group-specific prior variance results in more shrinkage of feature effects,
	compared to a group with larger group-specific prior variance. By setting the
	group variances, the prior expected relevance of the group's features is
	encoded in the model. However, determining the prior variances is not 
	straighforward in most applications. Section \ref{sec:hyperparameters} 
	proposes an empirical Bayes approach to estimate these variances.
	
	\section{Estimation}\label{sec:estimation}
	Maximum likelihood estimation of model (\ref{eq:model2}) is straightforward
	when $n < \bar{p}$ and many algorithms are available in literature. 
	In the $\bar{p} > n$ domain, estimation is possible through penalized 
	likelihood maximisation. In the current paper, focus is on the Bayesian model,
	so we refer the reader to Sections \ref{sm-sec:maximumlikelihood} and
	\ref{sm-sec:penalizedmaximumlikelihood} of the SM for details on 
	maximum (penalized) likelihood estimation of (\ref{eq:model2}). 
	
	Bayesian posteriors are commonly approximated through MCMC sampling. 
	Sampling from the posterior of model 
	(\ref{eq:model2}) and (\ref{eq:prior}) is relatively straightforward 
	(see SM Section \ref{sm-sec:gibbssampling} for a Gibbs sampler).
	However, due to high-dimensionality of the parameters, sampling is relatively 
	slow. In addition, the MCMC chain shows poor mixing in all investigated 
	applications and simulations. Poorly mixing MCMC chains require 
	a prohibitive number of samples to properly explore the posterior. Here, we
	avoid computationally expensive MCMC sampling by a mean-field variational 
	Bayes approximation of the posterior.
	
	Mean-field variational Bayes methods minimise the Kullback-Leibler divergence
	of the posterior from the (approximate) variational posterior. With observed 
	variables $\mathbf{X}$, some partitioning of 
  unobserved variables $\bm{\theta} = \{ \theta_1, \dots, \theta_K \}$, and
  an factorised posterior assumption $p(\bm{\theta} | \mathbf{X}) \approx
  \prod_{k=1}^K q(\theta_k)$, this results in marginal posteriors
  $q(\theta_k) \propto
  \exp \{ \mathbb{E}_{\bm{\theta}_{-k} | \theta_k, \mathbf{X}} 
  \[ \log p( \theta_k | \bm{\theta}_{-k}, \mathbf{X}) \] \}$. Note that we 
  slightly abuse notation and let $q(\cdot)$ denote distinct densities based 
  on the corresponding argument.
  For $p( \theta_k | \bm{\theta}_{-k}, \mathbf{X})$ in 
  the exponential family, $q(\theta_k)$ is in the same exponential
  family with natural parameter $\mathbb{E}_{\bm{\theta}_{-k} | \theta_k, 
  \mathbf{X}} \[ \eta (\bm{\theta}_{-k},\mathbf{X}) \]$, where 
  $\eta (\bm{\theta}_{-k},\mathbf{X})$ is the natural parameter of the full
  conditional distribution \cite[]{blei_variational_2017}.
  
  Here, let $\bm{\Lambda} = \begin{bmatrix} \bm{\lambda}_1 & \cdots 
  \bm{\lambda}_n \end{bmatrix}^{\text{T}}$ and the approximate posterior 
  factorise as
  \begin{equation}\label{eq:variationalposterior}
    p(\bm{\Lambda},\bar{\mathbf{B}},\bar{\psi}_1, \dots,
    \bar{\psi}_{\bar{p}} | \bar{\mathbf{X}}) \approx q(\bm{\Lambda})
    q(\bar{\mathbf{B}})q(\bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}}),
  \end{equation}
  so that the approximate posterior that minimises the Kullback-Leibler 
  divergence of posterior to approximation is
  \begin{subequations}\label{eq:variationaldistributions}
    \begin{align}
      q(\bm{\Lambda}) & \overset{D}{=} \prod_{i=1}^{n} \mathcal{N}_d
      (\bm{\phi}_i, \bm{\Xi}), \\
      q(\bar{\mathbf{B}}) & \overset{D}{=} \prod_{\bar{j}=1}^{\bar{p}}
      \mathcal{N}_d (\bm{\mu}_{\bar{j}}, \bm{\Omega}_{\bar{j}}),\\
      q(\bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}}) & \overset{D}{=}
      \prod_{\bar{j}=1}^{\bar{p}} \Gamma^{-1}
      (n/2 + d/2 + \kappa_{\bar{j}}, \zeta_{\bar{j}}).
    \end{align}
  \end{subequations}
  The so-called variational parameters are
  \begin{subequations}\label{eq:variationalparameters}
    \begin{align}
      \bm{\phi}_i & = \left\{ \sum_{{\bar{j}}=1}^{\bar{p}} \mathbb{E}
        (\bar{\psi}_{\bar{j}}^{-1})
        \[\mathbb{V}(\bar{\mathbf{b}}_{\bar{j}}) + 
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}})
        \mathbb{E}(\bar{\mathbf{b}}^{\text{T}}_{\bar{j}})\] + 
        \mathbf{I}_d \right\}^{-1}
        \mathbb{E}(\bar{\mathbf{B}}) \mathbb{E}(\bar{\bm{\Psi}}^{-1})
        \bar{\mathbf{x}}_i, \, i=1, \dots, n,\\
      \bm{\Xi} & = \left\{ \sum_{{\bar{j}}=1}^{\bar{p}} 
        \mathbb{E}(\bar{\psi}_{\bar{j}}^{-1})
        \[\mathbb{V}(\bar{\mathbf{b}}_{\bar{j}}) + 
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}})
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}}^{\text{T}})\] + \mathbf{I}_d 
        \right\}^{-1},\\
      \bm{\mu}_{\bar{j}} &= \[\mathbb{E}(\bm{\Lambda}^{\text{T}})
        \mathbb{E}(\bm{\Lambda}) + n \mathbb{V}(\mathbf{\bm{\lambda}}_i) +
        \gamma_{{\bar{j}}}^{-1} \mathbf{I}_d\]^{-1}
        \mathbb{E}(\bm{\Lambda}^{\text{T}})\bar{\mathbf{x}}_{\bar{j}}, \, 
        {\bar{j}}=1,\dots, \bar{p},\\
      \bm{\Omega}_{\bar{j}} & = \mathbb{E}(\psi_{\bar{j}}^{-1})^{-1}
        \[\mathbb{E}(\bm{\Lambda}^{\text{T}})
        \mathbb{E}(\bm{\Lambda}) + n \mathbb{V}(\mathbf{\bm{\lambda}}_i) +
        \gamma_{{\bar{j}}}^{-1} \mathbf{I}_d\]^{-1}, \, {\bar{j}}=1,\dots 
        {\bar{p}}, \\
      \zeta_{\bar{j}} & = \bar{\mathbf{x}}_{\bar{j}}^{\text{T}} 
        \bar{\mathbf{x}}_{\bar{j}}/2 -
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}}^{\text{T}})\mathbb{E}
        (\bm{\Lambda}^{\text{T}})
        \bar{\mathbf{x}}_{\bar{j}} + 
        \text{tr}\[\mathbb{E}(\bm{\Lambda}^{\text{T}})
        \mathbb{E}(\bm{\Lambda})\mathbb{V}(\bar{\mathbf{b}}_{\bar{j}})\]/2 +
        n\text{tr}\[\mathbb{V}(\bm{\lambda}_i)
        \mathbb{V}(\bar{\mathbf{b}}_{\bar{j}})\]/2 \nonumber \\
      & \,\,\,\,\,\, \,\,\, + \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}}^{\text{T}})
        \mathbb{E}(\bm{\Lambda}^{\text{T}}) \mathbb{E}(\bm{\Lambda})
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}})/2 + 
        n\mathbb{E}(\bar{\mathbf{b}}_{\bar{j}}^{\text{T}}) 
        \mathbb{V}(\bm{\lambda}_i)
        \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}})/2 + 
        \gamma_{\bar{j}}^{-1}\mathbb{E}
        (\mathbf{b}_{\bar{j}}^{\text{T}})\mathbb{E}(\mathbf{b}_{\bar{j}})/2 
        \label{eq:zeta} \\
      & \,\,\,\,\,\, \,\,\, + \gamma_{\bar{j}}^{-1}
        \text{tr}\[\mathbb{V}(\mathbf{b}_{\bar{j}})\]/2 + \nu_{\bar{j}}, 
        \, {\bar{j}}=1,\dots,{\bar{p}}, \nonumber
    \end{align}
  \end{subequations}
  where we slightly abuse notation and let 
  $\bar{\mathbf{x}}_i$ and
  $\bar{\mathbf{x}}_{\bar{j}}$ denote the $i$th row and $\bar{j}$th column of 
  $\bar{\mathbf{X}}$, respectively.
  The expectations and variances are
  \begin{align*}
    \mathbb{E}(\bar{\psi}_{\bar{j}}^{-1}) & = (n/2 + d/2 + \kappa_{\bar{j}})/
      \zeta_{\bar{j}}, \, {\bar{j}}=1,\dots,{\bar{p}},\\
    \mathbb{E}(\bar{\mathbf{b}}_{\bar{j}}) & = \bm{\mu}_{\bar{j}}, 
      \, {\bar{j}}=1,\dots,{\bar{p}},\\
    \mathbb{V}(\bar{\mathbf{b}}_{\bar{j}}) & = \bm{\Omega}_{\bar{j}},
      \, {\bar{j}}=1,\dots,{\bar{p}},\\
    \mathbb{E}(\bm{\Lambda}) & = \bm{\Phi}, \\
    \mathbb{V}(\bm{\lambda}_i) & = \bm{\Xi}, \, i=1,\dots,n,
  \end{align*}
  with $\bm{\Phi} = \begin{bmatrix} \bm{\phi}_1 & \cdots \bm{\phi}_n
  \end{bmatrix}^{\text{T}}$. 
  The parameters contain cyclic dependencies and are
  updated until convergence.
  
	Model (\ref{eq:model}) describes a general covariance matrix. However, 
	a correlation matrix better describes the standardised data. In 
	the frequentist setting the general covariance model is straightforward to 
	extend to the correlation model by restriction of the likelihood to the space 
	of correlation matrices. Moreover, this
	is the default setting in the \texttt{R} package \texttt{factanal}.
	In the Bayesian setting, this requires either more intricate prior modelling 
	or post hoc corrections of the posterior. Here, we consider a post hoc
	correction that ensures that the posterior expectation of the covariance of
	$\bar{\mathbf{X}}$ desribes a correlation matrix: 
	$$
	\forall \bar{j}: 
	\E_{\bar{\mathbf{B}},\bar{\psi}_1 ,\dots, \bar{\psi}_{\bar{p}} | 
  \bar{\mathbf{X}}} \(\bar{\mathbf{b}}_{\bar{j}}^{\text{T}} 
  \bar{\mathbf{b}}_{\bar{j}} + {\bar{\psi}}_{\bar{j}}\) = 1.
  $$
	SM Section 
	\ref{sm-sec:correlation} contains more details on 
	this posterior correction and a possible future direct correlation modelling
	approach. In the following, the post hoc correction approach is 
	applied.
	
	\subsection{Unlabeled observations}
	Inspection of (\ref{eq:prediction}) learns that the predictions 
	$\mathbb{E}(\tilde{y}|\tilde{\mathbf{x}})$
	depend on the observational model for $\mathbf{x}$ through $\mathbf{B}$ and
	$\bm{\Psi}$. As detailed in \cite{liang_use_2007}, this implies that 
	estimation benefits from additional unlabeled features $\mathbf{x}_i$,
	$i=n+1, \dots, n + m$, with the corresponding unobserved outcomes
	$z_i$, $i=n+1, \dots n + m$. A straightforward method of including the 
	unlabeled observations is to consider the full data likelihood
	$p(\mathbf{X}, \mathbf{z}, \mathbf{y} | \bar{\mathbf{B}}, \bar{\bm{\Psi}})$,
	with $\mathbf{z} = \begin{bmatrix} z_{n+1} & \cdots & z_{n+m} 
	\end{bmatrix}^{\text{T}}$
	\cite[]{banbura_maximum_2014,liu_maximum_1998}.
	Maximum likelihood estimation then requires marginalisation over unobserved
	outcomes $z_i$. Section \ref{sm-sec:unlabeled} in the
	SM describes an EM algorithm for (penalized) maximum likelihood estimation 
	with missing data. 
	
	In the Bayesian model, the unobserved outcomes are now included in the 
	posterior distributions. The variational Bayes posterior 
	(\ref{eq:variationalposterior}) is augmented as
	$$
  p(\bm{\Lambda},\bar{\mathbf{B}},\bar{\psi}_1, \dots,
  \bar{\psi}_{\bar{p}}, \mathbf{z}| \bar{\mathbf{X}}) \approx q(\bm{\Lambda})
  q(\bar{\mathbf{B}})q(\bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}})
  q(\mathbf{z}),
  $$
	where
	\begin{align*}
	  q(\mathbf{z}) & \overset{D}{=} \prod_{i={n+1}}^{n+m} \mathcal{N}
	    (\upsilon_i, \chi) \text{, with}\\
	  \upsilon_i & = \mathbb{E}(\bar{\mathbf{b}}_{\bar{p}}^{\text{T}} )
	    \mathbb{E}(\bm{\lambda}_i), \\
	  \chi & = \mathbb{E}(\bar{\psi}_{\bar{p}}^{-1})^{-1}.
	\end{align*}
	In addition, the term $\mathbbm{1}_{\bar{j}=\bar{p}}m\mathbb{V}(z_i)/2$ is 
	added to (\ref{eq:zeta}) and all occurences of $\bar{\mathbf{x}}_i$ and 
	$\bar{\mathbf{x}}_{\bar{j}}$ in (\ref{eq:variationalparameters}) are replaced
	with $\tilde{\mathbf{x}}_i$ and $\tilde{\mathbf{x}}_{\bar{j}}$, where
	$$
	\tilde{\mathbf{X}} = \begin{bmatrix}
    \multicolumn{1}{c}{\multirow{2}{*}{$\mathbf{X}$}} & \mathbf{y} \\
    \multicolumn{1}{c}{} & \mathbb{E}(\mathbf{z})
  \end{bmatrix},
	$$
	and
	\begin{align*}
	  \mathbb{E}(z_i) & = \upsilon_i, \\
	  \mathbb{V}(z_i) & = \chi.
	\end{align*}
	SM Section \ref{sm-sec:gibbssampling} 
  contains more details on the inclusion of
  unlabeled observations in the (approximate) Bayesian posterior computations
  through MCMC. Although not shown here due to brevity, the 
  unobserved outcome approach is straightforward to extend to an unobserved
  features approach.  
	
	\subsection{Latent dimension}
	Although we initially assumed $d$ to be the true latent dimension, in general
	it is unkown and needs to be estimated. Methods for dimension estimation are 
	plentiful in the literature \cite[see, e.g., ][]{zwick_comparison_1986}. 
	Our modest aim of accurate prediction does not require correct estimation of 
	the latent dimension, as even picking the true latent dimension does not 
	always lead to optimal predictions \cite[]{goeman_statistical_2006}.
	Without this requirement of correct latent dimension estimation, we resort to
	the simple and fast Kaiser criterion. The Kaiser criterion picks $d$ 
	that retains
	dimensions with variance contribution larger than that of the average feature 
	$\mathbf{x}$. This amounts to choosing $d=\sum_{j=1}^p\mathbbm{1}
	\{v_j > 1\}$, with $v_j$, $j=1, \dots, p$, the eigenvalues of the correlation 
	matrix. That is, we set $d$ to the number of eigenvalues of the 
	correlation matrix of $\mathbf{X}$ larger than one.
	
	% In fact, 
	% \cite{goeman_statistical_2006} show the balance between bias and variance
	% that leads to optimal prediction error is for $d$ 
	% somewhere between 0 and the true latent dimension.
	% Here, we consider a few estimation 
	% methods:
	% \begin{enumerate}
	%   \item Pick $d$ such that a 
	%     pre-specified proportion of the variance of
	%     $\mathbf{X}$ is explained with the latent factors from a simple
	%     factor analysis. \cite{ferrari_bayesian_2020} propose the proportion of
	%     explained variance 0.9. In practice this amounts to choosing $d$, such 
	%     that $\sum_{j=1}^d v_j/p > 0.9$, where $v_j$ is the $j$th largest eigen
	%     value of the correlation matrix of $\mathbf{X}$.
	%   \item The Kaiser criterion picks $d$ such that we retain dimensions with
	%     variance contribution larger than that of the average feature 
	%     $\mathbf{x}$. This amounts to choosing $d=\sum_{j=1}^p\mathbbm{1}
	%     \{v_j > 1\}$, i.e., set $d$ to the number of eigen values of the 
	%     correlation matrix of $\mathbf{X}$ larger than one.
	%   \item The Marchenku-Pastur law based rule picks $d$ such that all 
	%     dimensions contribute more to the variance of $\mathbf{X}$ than
	%     than of the average feature in the asymptotic limit 
	%     ($n,p \rightarrow \infty$) of a random matrix
	%     $\mathbf{X}_{n \times p}$. In practice, this means choosing 
	%     $d=\sum_{j=1}^p \mathbbm{1} \{ v_j > (1 + \sqrt{p/n})^2 \}$.
	% \end{enumerate}
	% 
	% - more explanation needed
	
	\subsection{Hyperparameters}\label{sec:hyperparameters}
	The Bayesian model requires a choice of
	hyperparameters $\gamma_g$, $\kappa_j$ and $\nu_j$. 
	Choosing the $\gamma_g$ by hand requires intricate prior expert
	knowledge, which might not be available. An alternative is to estimate them
	from the data using empirical Bayes. Or, if we do know the overall scale of
	the $\gamma_g$, but not the group-specific deviations, we may reparametrise as
	$\gamma_g = \gamma \gamma_g'$, fix the overall scale $\gamma$ and estimate the
	group-specific multipliers $\gamma_g'$.
	
	In both empirical Bayes settings we maximise the marginal 
	likelihood (constrained maximisation for the second approach). Direct 
	marginal likelihood maximisation requires calculation of a $p$-dimensional 
	integral for which no closed form is available. With $p$ large (i.e., the high
	dimensional setup considered here), an EM algorithm with iterations
	$$
  \bm{\gamma}^{(k + 1)} = \underset{\bm{\gamma}}{\argmax} 
  \mathbb{E}_{ \theta | \mathbf{y}} \[\log p(\bar{\mathbf{B}} | 
  \bar{\psi}_1, \dots, \bar{\psi}_{\bar{p}} ) | 
  \bm{\gamma}^{(k)} \],
  $$
	where $\bm{\gamma} = \begin{bmatrix} \gamma_1 & \cdots \gamma_G 
	\end{bmatrix}^{\text{T}}$, is computationally much more feasible.
	With a variational Bayes approximation of the difficult expectation, this
  results in
  $$
  \bm{\gamma}^{(k + 1)} = \underset{\bm{\gamma}}{\argmax}\left\{
  - \frac{1}{2} \sum_{g=1}^G \gamma_g^{-1} 
  \sum_{j \in \mathcal{G}_g} \mathbb{E}(\bar{\psi}_j^{-1}) 
  \left\{ \text{tr} \[ 
  \mathbb{V}(\bar{\mathbf{b}}_j)\] + \mathbb{E}(\bar{\mathbf{b}}_j^{\text{T}})
  \mathbb{E}(\bar{\mathbf{b}}_j) \right\} - \frac{d}{2} 
  \sum_{g=1}^{G} |\mathcal{G}_g| \log \gamma_{g} \right\}.
  $$
  Finally, this gives empirical Bayes updates:
  $$
  \gamma_g^{(k + 1)} = \frac{\sum_{j \in \mathcal{G}_g} 
  \mathbb{E}(\bar{\psi}_j^{-1}) \left\{ \text{tr} \[ 
  \mathbb{V}(\bar{\mathbf{b}}_j)\] + \mathbb{E}(\bar{\mathbf{b}}_j^{\text{T}})
  \mathbb{E}(\bar{\mathbf{b}}_j) \right\}}{|\mathcal{G}_g| d}.
  $$
  For the $\gamma_g = \gamma \gamma_g'$ parametrisation, the updates
  \begin{align*}
  \bm{\gamma}'^{(k + 1)} & = \underset{\bm{\gamma}'}{\argmax}\left\{ 
  - \frac{1}{\gamma} \sum_{g=1}^G \gamma_g'^{-1} 
  \sum_{j \in \mathcal{G}_g} \mathbb{E}(\bar{\psi}_j^{-1}) 
  \left\{ \text{tr} \[ 
  \mathbb{V}(\bar{\mathbf{b}}_j)\] + \mathbb{E}(\bar{\mathbf{b}}_j^{\text{T}})
  \mathbb{E}(\bar{\mathbf{b}}_j) \right\} - \frac{d}{2} 
  \sum_{g=1}^{G} |\mathcal{G}_g| \log \gamma_{g}' \right\}, \\
  & \text{subject to } \prod_{g=1}^G \gamma_g'^{|\mathcal{G}_g|}=1,
  \end{align*}
  are not available
  in closed form, but still convex and easy to compute with standard numerical
  optimisation tools.
	Empirical Bayes estimation of the $\gamma_g$ or $\gamma_g'$ is data-dependent 
	and does not rely on subjective arguments. In addition, empirical Bayes 
	estimation avoids (possibly complicated) hyperpriors on the $\gamma_g$
	and $\gamma_g'$. A 
	drawback is that we lose the uncertainty propagation property of the full 
	Bayesian approach.
	
	Prior error variance/uniquenesses shapes $\kappa_{\bar{j}}$ and scale 
	$\nu_{\bar{j}}$, and overall prior variance $\gamma$ are set to default values
	to reflect a lack of prior 
	knowledge. Our default choice of hyperparameters should take the
	standardisation
	of the data into account. Three postulates are used to select the
	hyperparameters:
	(i) we ensure that the prior expectation describes
	a correlation matrix model, i.e., 
	$\forall j: \E_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}} (\bar{\mathbf{b}}_{\bar{j}}^{\text{T}} 
  \bar{\mathbf{b}}_{\bar{j}} + {\bar{\psi}}_{\bar{j}}) = 1$. 
  Furthermore, (ii) the prior contributions of the error and the latent 
  structure to the data are assumed equal, i.e., 
  $\forall j: \E_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}} (\bar{\mathbf{b}}_{\bar{j}}^{\text{T}} 
  \bar{\mathbf{b}}_{\bar{j}}) = \E_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}} ({\bar{\psi}}_{\bar{j}}) = 1/2$.
  Lastly, (iii) the prior uniqueness variance is set to 
  $\mathbb{V}_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}} ({\bar{\psi}}_{\bar{j}}) = 1$.
	These three postulates together result in $\gamma = 1/d$, 
	$\forall \bar{j}: \kappa_{\bar{j}}=9$, and 
	$\forall \bar{j}: \nu_{\bar{j}}=4$. As a result,
	$\forall {\bar{j}}: \mathbb{V}_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}}(\mathbf{b}_j^{\text{T}}\mathbf{b}_j) = 1 + 
	5/(2d)$. For $d$ large compared to $5/2$ (as one expects in high-dimensional
	settings), we have $\mathbb{V}_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}}(\mathbf{b}_j^{\text{T}}\mathbf{b}_j) \approx
	1 = \mathbb{V}_{\bar{\mathbf{B}},\bar{\psi}_1
	,\dots, \bar{\psi}_{\bar{p}}} ({\bar{\psi}}_{\bar{j}})$, so that the 
	contributions to 
	the prior variance of latent structure and error are approximately equal.
	The methods described in this Section are implemented in the \texttt{R}
	package \texttt{bayesfactanal} available from 
	\url{https://github.com/magnusmunch/bayesfactanal}.
	
	\section{Simulations}\label{sec:simulations}
	To assess the potential benefit of the proposed models in prediction of
	outcome $y$ from features $\mathbf{x}$, 
	a simulation is set up. The simulation setting is 
	meant to demonstrate the potential benefit of (i) the Bayesian factor 
	regression model in general, (ii) the inclusion of the feature structure 
	through the empirical Bayes estimation of the $\gamma_g'$ as explained in 
	Section \ref{sec:hyperparameters}, and (iii) the use of unlabeled features in
	the estimation.
	
	To that end, $n=50$ labeled, 
	and $m \in \{0, 50, 100, 200, 500 \}$ unlabeled observations are drawn from 
	model (\ref{eq:model}) and standardised after simulation. 
	Error variance and uniquenesses are set to 
	$\sigma^2=1$ and $\forall j: \psi_j=1$. The number of features is fixed to
	$p=100$. Two scenarios for the model parameters $b_{hj}$ and $\beta_h$ are 
	considered:
	\begin{enumerate}
	  \item the number of factors are fixed to $d=10$. The $b_{hj}$ are set so 
	    that each feature loads on two factors and factor is a part of 20
	    features (see (\ref{eq:simulation}), where each $b$ denotes
	    10 values and the empty cells are set to zero). $\beta_h$ is set so that 
	    the outcome loads on all factors. 
	    The features are divided in two groups 
	    $\mathcal{G}_1 = \{1, \dots, 50 \}$ and 
	    $\mathcal{G}_2 = \{51, \dots, 100 \}$.
	    The non-zero $b_{hj}$ are 
	    drawn from independent univariate centered Gaussian distributions. The 
	    variances are $\mathbb{V}(b_{hj})=0.1$ for $j=1, \dots 50$, and
	    $\mathbb{V}(b_{hj})=1$ for $j=51, \dots, 100$ The $\beta_h$ are 
	    independent and drawn from the standard Gaussian distribution. 
	  \item The second scenario fixes $d=40$. The model parameters $b_{hj}$ are 
	    drawn from independent univariate centered Gaussian distributions. The 
	    variances are $\mathbb{V}(b_{hj})=0.1$ for $j=1, \dots 50$, and
	    $\mathbb{V}(b_{hj})=10$ for $j=51, \dots, 100$, i.e., the features are
	    structured in two groups: $\mathcal{G}_1 = \{1, \dots, 50 \}$ and
	    $\mathcal{G}_2 = \{51, \dots, 100 \}$. To ensure that
	    the proportion of variance in $y$ explained with the factors is
	    0.7, $\beta_h$ is set $\beta_h=0.483$.
	\end{enumerate}
	\begin{equation}\label{eq:simulation}
	  \mathbf{B}=
	  \left[
      \begin{array}{ c c c c c:c c c c c}
        b & &  &  &  &  &  &  &  & b\\ 
        b & b &  &  &  &  &  &  &  & \\
        & b & b &  &  &  &  &  &  & \\
        &  & b & b & &  &  &  &  & \\
        &  &  & b & b & &  &  &  & \\
        &  &  &  & b & b &  &  &  & \\
        &  &  &  &  & b & b &  &  & \\
        &  &  &  &  &  & b & b & & \\
        &  &  &  &  &  &  & b & b & \\
        &  &  &  &  &  &  &  & b & b 
      \end{array} 
    \right]
  \end{equation}
	The first scenario models a situation where the features load on two factors 
	only in such a way that the marginal correlation between features is weak. 
	This might occur, for example, if genes are 
	organised in
	nearly disjoint functional networks, but the outcome is related to all the
	networks. Ridge regression is expected to 
	perform well here. With such a sparse loadings matrix, 
	$(\mathbf{B}^{\text{T}}\mathbf{B} + \bm{\Psi})^{-1} \approx \mathbf{I}_p$.
	That is, the information in $\mathbf{X}$ contributes little to the induced
	regression coefficients $\tilde{\bm{\beta}}$. In addition, the induced
	regression coefficients become
	$\tilde{\bm{\beta}} \approx \mathbf{B}^{\text{T}} \bm{\beta} = 
	\mathbb{C}\text{ov}(y, \mathbf{x})$, a (rescaled version of the) quantity 
	that standard linear regression methods aim to estimate.
	
	The second scenario models a setting where all features load on all factors,
	but the strength of the loading depends on the feature group. This might 
	occur, for example, if genes are organised in several interconnected 
	functional networks, but some network have weak connections. The outcome is 
	again related to the all functional networks. In this setting, the
	factor regression methods are expected to perform well. In contrast to the
	first simulation, 
	$(\mathbf{B}^{\text{T}}\mathbf{B} + \bm{\Psi})^{-1} \neq \mathbf{I}_p$,
	so information on the induced regression coefficients $\tilde{\bm{\beta}}$ is
	contained in $\mathbf{X}$. This results in increased effiency due to the 
	inclusion of data. Also, $\tilde{\bm{\beta}}$, is a weighted version of
	$\mathbb{C}\text{ov}(y, \mathbf{x})$ that is not straightforward to estimate
	with standard linear regression methods.
	
	Six models are compared:
	\begin{enumerate}
	  \item Ridge regression with cross validated penalty parameter with the
	    \texttt{R} package \texttt{glmnet} \cite[]{friedman_regularization_2010};
	  \item Lasso regression with cross validated penalty parameter with the
	    \texttt{R} \texttt{glmnet} package \cite[]{friedman_regularization_2010};
	  \item a two-step factor regression method: (i) a penalized factor 
	    model is estimated from the feature correlation matrix, with cross 
	    validated penalty parameter.
	    Next, (ii) outcomes are regressed on the feature factor scores 
	    $\hat{\mathbb{E}}(\bm{\lambda}_i | \mathbf{x}_i)$ to obtain the prediction 
	    rule. This approach was shown to work in \cite{peeters_stable_2019} and
	    is implemented in the \texttt{R} \texttt{FMradio} package 
	    \cite[]{peeters_fmradio_2019};
	  \item a penalized factor regression model that includes unlabeled 
	    observations, with cross validated penalty parameter, and estimated as in
	    SM Section (\ref{sm-sec:mlestimation});
	  \item the proposed Bayesian factor regression model (\ref{eq:prior}), 
	    approximated with variational Bayes as in Section \ref{sec:estimation}. 
	    The fixed hyperparameters are described in Section 
	    \ref{sec:hyperparameters}. Note that this model does not include 
	    external feature structure and therefore does not estimate the 
	    $\gamma_g'$;
	  \item the proposed empirical Bayesian factor regression model 
	    (\ref{eq:prior}), approximated with variational Bayes as in Section 
	    \ref{sec:estimation}. The hyperparameters are described in Section 
	    \ref{sec:hyperparameters}, where we include the grouping of the features
	    and estimate group-specific $\gamma_g'$ by empirical Bayes.
	\end{enumerate}
	For all models, the data are standardised before estimation, as is common
	in most real data applications.
	Models 3-6 allow for the inclusion of unlabeled features and are estimated
	for a range of number of unlabeled features.
	In addition, we 
	% consider the true data-generating model $\tilde{\beta}$ and
	fitted an intercept-only null model. We calculate estimation mean squared 
	error
	(EMSE) of $\tilde{\bm{\beta}}$, prediction mean squared error (PMSE),
	and correlation between predictions and observations 
	($\mathbb{C}\text{or}(y,\hat{y})$) on test data of size 
	$n_{\text{test}}=1000$. Lower PMSE and EMSE indicate better performance, while
	higher $\mathbb{C}\text{or}(y,\hat{y})$ indicates better performance.
	The results, with the median taken
	over 50 simulation replications, are displayed in Figures
	\ref{fig:simulation1} and Figures 
	\ref{fig:simulation2}, for scenarios 1 and 2, respectively.
	
<<simulation1, fig.cap="Simulation results for scenario 1 with median (a) EMSE and (b) PMSE, respectively.", out.width="100%", fig.asp=1/2>>=
load(file="results/simulations_bayesfactreg_res3.Rdata")
library(RColorBrewer)
source(file="code/figures.R")
simfig1(res19, methods=c("ridge", "lasso", "FMradio", "emfactanal", "vbfactanal", 
                        "ebfactanal", "null"), m=c(0, 50, 100, 200, 500), 
        measure="median", col=brewer.pal(n=11, name="Set3"), lwd=2, 
        lty=c(rep(1, 3), NA, rep(1, 3)),
        pch=c(rep(NA, 3), 4, rep(NA, 3)), legend=TRUE,
        label=c("ridge", "lasso", "FMradio", "penalized", "Bayes",
                "EBayes", "null"))
@
<<simulation2, fig.cap="Simulation results for scenario 2 with median (a) EMSE and (b) PMSE, respectively.", out.width="100%", fig.asp=1/2>>=
load(file="results/simulations_bayesfactreg_res3.Rdata")
library(RColorBrewer)
source(file="code/figures.R")
simfig1(res9, methods=c("ridge", "lasso", "FMradio", "emfactanal", "vbfactanal", 
                        "ebfactanal", "null"), m=c(0, 50, 100, 200, 500), 
        measure="median", col=brewer.pal(n=11, name="Set3"), lwd=2, 
        pch=rep(4, 7), legend=TRUE,
        label=c("ridge", "lasso", "FMradio", "penalized", "Bayes",
                "EBayes", "null"))
@

  In both scenarios, the penalized factor regression model was not estimable
  with unlabeled data, due to non-convergence. The two-step FMradio approach
  also suffers from non-convergence with more unlabeled data, but was still 
  estimable in some simulations, so it is included in the Figures.

  In both scenarios estimation (i.e., EMSE) and prediction calibration 
  (i.e., PMSE) of the Bayesian methods
  initially improves with more unlabeled data. However, in scenario 1 it starts
  deteriorating again after about $m=100$. Surprisingly, the opposite holds for 
  FMradio. In scenario 2, where the performance continues to improve with 
  more unlabeled data, the rate of improvement decreases with the number of
  unlabeled observations. This is unsurprising, as estimators generally converge
  at a similarly-shaped $\sqrt{n}$ rate. In both scenarios, discrimination 
  (i.e., $\mathbb{C}\text{or}(y,\hat{y})$) keeps improving with the addition
  of unlabeled features. For scenario 1, this is surprising, considering the
  eventual deterioration in calibration and estimation.
  
  In scenario 1, the Bayesian methods outperform the frequentist methods for
  almost all $m$ in terms of estimation and discrimination. Calibration is
  worse for the Bayesian methods for small and large $m$, but better for medium
  $m$. The two-step factor regression model FMradio, performs worse than the
  Bayesian factor regression methods and ridge, only outperforming lasso.
  In scenario 2, the frequentist methods outperform the Bayesian method for 
  small $m$ in terms of estimation and calibration.
  For medium $m$, the Bayesian methods outperform ridge, and eventally,
  for large $m$, also lasso. FMradio outperforms all other methods in 
  estimation, calibration, and discrimination. Scenario 2 simulates strong 
  factors, that explain much of the data. Extraction of these factors 
  in step one of the FMradio approach is therefore relatively easy. Estimation
  of the prediction rule based on these strong factors in step two of FMradio
  then results in a strong predictor.
  
  A comparison of full Bayes and empirical Bayes shows that the inclusion of 
  the feature groupings helps in both estimation and prediction. In scenario 1,
  empirical Bayes estimation and calibration is slightly better than full 
  Bayes. Discrimination is about equal. In scenario 2 empirical clearly
  outperforms full Bayes in all three performance measures.
  Figures
  \ref{fig:gamma1} and \ref{fig:gamma2} display the estimated 
  $\log \hat{\gamma}'_g$ for the empirical Bayes model in scenarios 1 and 2,
  respectively.
  Both Figures show a clear influence of the feature grouping
  on estimation, as the prior variances of the groups show a clear difference.
  Furthermore, the influence of the feature grouping grows with the number
  of unlabeled observations, as the diverging lines indicate.
<<gamma1, fig.cap="Simulation results for scenario 1 with median $\\log \\hat{\\gamma}_g'$ estimated with empirical Bayes.", out.width="40%">>=
load(file="results/simulations_bayesfactreg_res2.Rdata")
library(RColorBrewer)
source(file="code/figures.R")
simfig2(res9, methods=c("ebfactanal"),
        m=c(0, 50, 100, 200, 500), measure="median", lwd=2, 
        cex.lab=2, cex.axis=2, cex=2,
        legend=TRUE, labels=c("EBayes", "group 1", "group 2"),
        legend.pos="right")
@
<<gamma2, fig.cap="Simulation results for scenario 2 with median $\\log \\hat{\\gamma}_g'$ estimated with empirical Bayes.", out.width="40%">>=
load(file="results/simulations_bayesfactreg_res2.Rdata")
library(RColorBrewer)
source(file="code/figures.R")
simfig2(res19, methods=c("ebfactanal"),
        m=c(0, 50, 100, 200, 500), measure="median", lwd=2, 
        cex.lab=2, cex.axis=2, cex=2,
        legend=TRUE, labels=c("EBayes", "group 1", "group 2"),
        legend.pos="right")
@

	\section{Applications}\label{sec:applications}
	\subsection{Influenza vaccine}
	The data described in this Section are from \cite{nakaya_systems_2011} and 
	made publicly available through the NCBI GEO archive 
	\cite[]{barrett_ncbi_2012} with accession numbers GSE29614 and GSE29617. 
	The analysis mostly follows \cite{van_deun_obtaining_2018}, 
	where a main aim was to predict vaccine efficacy with microarray gene
	expression data. Here follows a short description of the data; 
	for more details we refer the reader to
	\cite{van_deun_obtaining_2018}.
	
	The data are from 9 and 26 subjects, observed in the 2007 and 2008 flu 
	seasons, respectively. For all subjects there are three efficacy measures
	from just before and 28 after vaccination available in the form of three
	different plasma hemagglutination inhibition (HAI) antibody titers. The
	antibody titers were combined by first taking the maximum of the three 
	log-transformed titers and then substracting the measurements just before and
	three days after vaccination. The scores were
	standardised to mean zero and variance one.
	In addition to the vaccine efficacy measures, there are 54,675 microarray gene 
	expression measurements available from just before and three days after 
	vaccination. The Robust Multichip Average (RMA) algorithm 
	\cite[]{irizarry_exploration_2003} 
	was used to pre-process the microarrays. After pre-processing, a change 
	score was calculated by substracting the measurements just before and three
	days after vaccination from each other. These scores were standardised to
	mean zero and variance one. Before the analysis, a pre-selection
	of 416 genes with highest coefficient of variation is made. 
	The choice of 416 genes follows the analysis results of 
	\cite{van_deun_obtaining_2018}. Here, we consider the 2007 data as unlabeled
	and the 2008 data as labeled.  
	
	The application is an example of a difficult high-dimensional prediction 
	problem, with little data available: a situation that regularly arises in 
	practice. Here, the available unlabeled data potentially 
	increases predictive performance signficicantly. Additionally, genes are often
	considered to be organised in functional networks, so the factor model
	is an appropriate choice and we expect the factor regression methods to
	outperform classical linear regression methods.
	
	We estimate the same models as in Section \ref{sec:simulations}, 
	with the exception of the empirical Bayes model, because there is no
	grouping of the features available. To assess performance we
	calculated cross-validated PMSE and $\mathbb{C}\text{or}(y,\hat{y})$ 
	and display them in Table
	\ref{tab:application1}, where null refers to the intercept only model.
	The penalized factor regression model did not converge, so is not included 
	in the results.
<<application1>>=
# load(file="results/analysis_expression_influenza_res1.Rdata")
load(file="results/analysis_expression_influenza_filt416_res1.Rdata")
time.influenza <- time[-c(3, 5)]
tab <- cbind(pmse[!is.na(pmse)], pcor[!is.na(pmse)])[-3, ]
tab <- round(tab, 3)
tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1] <-
  paste0("\\textbf{", tab[which(tab[, 1]==min(tab[, 1], na.rm=TRUE)), 1], "}")
tab[which(tab[, 2]==max(tab[, 2], na.rm=TRUE)), 2] <-
  paste0("\\textbf{", tab[which(tab[, 2]==max(tab[, 2], na.rm=TRUE)), 2], "}")
colnames(tab) <- c("PMSE", "$\\mathbb{C}\\text{or}(y,\\hat{y})$")       
rownames(tab) <- c("ridge", "lasso", "FMradio", "VBayes", "null")
kableExtra::kable_styling(knitr::kable(
  tab, align="r", digits=3,
  caption="Cross-validated PMSE and $\\mathbb{C}\\text{or}(y,\\hat{y})$ (best performing in bold) calculated on the influenza vaccine data.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
	Table \ref{tab:application1} shows that the variational Bayesian factor 
	regression that includes the unlabeled data outperforms the other methods in 
	terms of calibration (i.e., PMSE) and discrimination 
	(i.e., $\mathbb{C}\text{or}(y,\hat{y})$), according to expectation. The other
	methods perform similarly in terms of PMSE, while lasso performance approaches 
	the Bayesian factor regression in terms of $\mathbb{C}\text{or}(y,\hat{y})$.
	
	\subsection{Oral cancer lymph node metastasis}
	In this Section, oral cancer lymph node metastasis is predicted with gene
	expression data. RNAseqs, taken from TCGA 
	\cite[]{the_cancer_genome_atlas_network_comprehensive_2015}, are measured on 
	133 HPV-negative oral tumours taken from 76 and 57 oral cancer patients, with
	and without lymph node metastasis, respectively. 
	For more details on these data, see \cite{te_beest_improved_2017}.
	Additional gene expressions are available from an independent 
	microarray study on 97 oral cancer patients in
	\cite{mes_prognostic_2017}. 
	These microarrays are normalised to the same scale as
  the RNAseqs and included in the analysis as unlabeled data.
	A set of 871 genes with $p \leq 0.01$ in the microarray data is 
	pre-selected.
  To investigate the empirical Bayes estimation of the $\gamma_g$, the 
	genes are divided in three groups, based on the cis-correlation between
	between the RNAseq data and TCGA DNA copy numbers on the same patients, 
	quantified by Kendall's $\tau$. 
	% The procedure to group the features is
	% described in \cite{munch_adaptive_2019}. 
	
	This Section investigates an example of a high-dimensional classification
	problem in for which both unlabeled data and external feature
	information is available. As before, genes are assumed to be organised in
	functional networks, so we expect the factor regression methods to fit the
	data well. We expect features with a large
	positive correlation between RNAseqs and DNA copy number, as quantified by
	Kendall's $\tau$, to be more important for metastasis prediction. We
	therefore expect to estimate larger $\gamma_g'$ for the groups with
	higher Kendall's $\tau$.
	
	We estimate the logistic extensions of the models estimated in Section 
	\ref{sec:simulations}. To assess performance we
	calculated a calibration measure Brier skill score (BSS) and discrimination 
	measure area under the receiver operator curve (AUC)
	on the unlabeled data and display them in Table
	\ref{tab:application2}.
	The penalized factor regression model did not converge, so is not included 
	in the results.
<<application2>>=
load(file="results/rnaseq_oral_cancer_metastasis_res4.Rdata")
time.oral <- time[-c(3, 6)]
# time.oral <- time
tab <- cbind(briers, auc)[-c(3, 6), ]
tab <- round(tab, 3)
tab[which(tab[, 1]==max(tab[, 1], na.rm=TRUE)), 1] <-
  paste0("\\textbf{", tab[which(tab[, 1]==max(tab[, 1], na.rm=TRUE)), 1], "}")
tab[which(tab[, 2]==max(tab[, 2], na.rm=TRUE)), 2] <-
  paste0("\\textbf{", tab[which(tab[, 2]==max(tab[, 2], na.rm=TRUE)), 2], "}")
colnames(tab) <- c("BSS", "AUC")       
rownames(tab) <- c("ridge", "lasso", "FMradio", "VBayes", "EBayes")
kableExtra::kable_styling(knitr::kable(
  tab, align="r", digits=3,
  caption="BSS and AUC (best performing in bold) calculated on the oral cancer lymph node metastasis data.",
  format="latex", booktabs=TRUE, escape=FALSE),
  latex_options=c("HOLD_position"))
@
  Surprisingly, the best performing model in terms of calibration (i.e., BSS) 
  is the lasso. The Bayesian factor regression methods outperform the
  other methods in terms of discrimination (i.e., AUC).
  The estimated $\gamma_g'$ are $0.97$, $0.98$, and $1.01$ for the low-, 
  medium-, and high cis-correlation groups, respectively. This small difference
  in shrinkage leads to a marginal increase in predictive performance of the
  empirical Bayes method compared to the full Bayes version.
	% \subsection{Alzheimers disease}
	% - check Perrakis and Mukherjee (2019) data
	% - try metabolomics data
	
	\section{Discussion}\label{sec:discussion}
	This paper investigates a Bayesian factor regression model for high 
	dimensional prediction and classification problems. It allows for the 
	inclusion of unlabeled data and feature groupings to improve predictive
	performance. Estimation is through a combination of variational and
	empirical Bayes techniques. The approach is competitive with classical
	ridge and lasso regression, as well as with more elaborate frequentist
	factor modelling 
	approaches such as penalized factor regression and the two-step factor 
	\texttt{FMradio}. Simulations show that
	the method is especially useful if the features are generated in 
	dense, correlated networks. Two applications show that the method predicts
	just as well, or better, than existing methods in real data settings.
	
	A technical advantage of the pursued factor modelling approach is the 
	straightforward inclusion of unlabeled observations through the full 
	likelihood approach. However, some caution regarding this approach is 
	advised. For the full likelihood approach to return unbiased estimates,
	the missing data mechanism is assumed to be at most missing at random (MAR). 
	That is, the missingness possibly depends on the observed features, but not
	on unobserved features. In the current setting, MAR implies that unobserved 
	labels are not missing due to the value of the labels. We argue that in most 
	applications, this is a reasonable assumption. In the examples above,
	observations are unlabeled because they come from independent studies. 
	Due to the independence, it is reasonable to assume that no relation
	exists between not observing labels and the actual labels.
	Another technical advantage of Bayesian modelling is the occurence of
	convergence issues in frequentist models. Sections \ref{sec:simulations} and 
	\ref{sec:applications} 
	show that the frequentist factor models suffer from convergence 
  issues if the number of labeled and/or unlabeled samples becomes large. 
  More investigation is required to determine when and why these convergence
  issues occur.
  An inherent benefit of Bayesian modelling is the
  uncertainty quantification that automatically comes with the Bayesian 
  posterior. This allows for straightforward calculation of prediction 
  intervals. We note that the uncertainty quantification in the current setting
  requires a more thorough investigation.
	
	More elaborate prior modelling of the factor 
  loadings is possible through the $\gamma_j$. For example, a more sparse 
  lasso model for the factor loadings introduces the hyperpriors:
  $\gamma_j \sim \text{Exp}(\lambda_j)$. Feature grouping is then included by
  parametrising $\forall j \in \mathcal{G}_g: \lambda_j = \lambda_g$, and
  estimating the $\lambda_g$ with empirical Bayes. In general, 
  such Gaussian scale mixture extensions of
  the $\bar{\mathbf{B}}$ prior require the addition of one or more extra 
  layers to the prior and one or more extra variational parameters to update 
  during estimation. Some existing examples of sparse Bayesian
  factor models are \cite{ferrari_bayesian_2020} and
  \cite{carvalho_high-dimensional_2008}. Sparse factor models often simplify
  the latent dimension estimation. In any case, latent dimension estimation is 
  a topic that deserves more 
  attention. Here, estimation is via a simple Kaiser criterion. More elaborate
  methods are available in literature \cite[see, e.g.,][]{auerswald_how_2019}.
	
	Lastly, we give some indication of computational times.
	The proposed factor regression approaches are slower to estimate compared to
	the other methods. Model estimation times for the influenza application are: 
	\Sexpr{round(time.influenza, 2)[-4]}, and 
	\Sexpr{round(time.influenza, 2)[4]} seconds, for the ridge, lasso,
	FMradio, and Bayesian factor regression models, respectively. 
	For the oral cancer metastasis appication we have
	\Sexpr{round(time.oral, 2)[-c(3, 4, 5)]} and \Sexpr{round(time.oral, 2)[3]} 
	seconds for the ridge, lasso and FMradio, and
	\Sexpr{round(time.oral/60, 2)[4]} and \Sexpr{round(time.oral/60, 2)[5]} 
	minutes for the variational and empirical Bayesian models.
	Especially in the second application, the estimation is considerably slower.
	However, we argue that these times are still manageable and much faster
	than traditional MCMC estimation times.
	
	\bibliographystyle{author_short3} 
	\bibliography{refs}
	
	\section*{Session info}

<<r session-info, eval=TRUE, echo=TRUE, tidy=TRUE>>=
devtools::session_info()
@

\end{document}